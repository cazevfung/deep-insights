{
  "success": true,
  "url": "https://developers.google.com/search/docs/fundamentals/how-search-works",
  "content": "Home\nSearch Central\nDocumentation\nWas this helpful?\nSend feedback\nIn-depth guide to how Google Search works\nbookmark_border\nGoogle Search is a fully-automated search engine that uses software known as web crawlers that explore the web regularly to find pages to add to our index. In fact, the vast majority of pages listed in our results aren't manually submitted for inclusion, but are found and added automatically when our web crawlers explore the web. This document explains the stages of how Search works in the context of your website. Having this base knowledge can help you fix crawling issues, get your pages indexed, and learn how to optimize how your site appears in Google Search.\nLooking for something less technical? Check out our How Search Works site, which explains how Search works from a searcher's perspective.\nA few notes before we get started\nBefore we get into the details of how Search works, it's important to note that Google doesn't accept payment to crawl a site more frequently, or rank it higher. If anyone tells you otherwise, they're wrong.\nGoogle doesn't guarantee that it will crawl, index, or serve your page, even if your page follows the Google Search Essentials.\nIntroducing the three stages of Google Search\nGoogle Search works in three stages, and not all pages make it through each stage:\nCrawling: Google downloads text, images, and videos from pages it found on the internet with automated programs called crawlers.\nIndexing: Google analyzes the text, images, and video files on the page, and stores the information in the Google index, which is a large database.\nServing search results: When a user searches on Google, Google returns information that's relevant to the user's query.\nCrawling\nThe first stage is finding out what pages exist on the web. There isn't a central registry of all web pages, so Google must constantly look for new and updated pages and add them to its list of known pages. This process is called \"URL discovery\". Some pages are known because Google has already visited them. Other pages are discovered when Google extracts a link from a known page to a new page: for example, a hub page, such as a category page, links to a new blog post. Still other pages are discovered when you submit a list of pages (a sitemap) for Google to crawl.\nOnce Google discovers a page's URL, it may visit (or \"crawl\") the page to find out what's on it. We use a huge set of computers to crawl billions of pages on the web. The program that does the fetching is called Googlebot (also known as a crawler, robot, bot, or spider). Googlebot uses an algorithmic process to determine which sites to crawl, how often, and how many pages to fetch from each site. Google's crawlers are also programmed such that they try not to crawl the site too fast to avoid overloading it. This mechanism is based on the responses of the site (for example, HTTP 500 errors mean \"slow down\").\nHowever, Googlebot doesn't crawl all the pages it discovered. Some pages may be disallowed for crawling by the site owner, other pages may not be accessible without logging in to the site.\nDuring the crawl, Google renders the page and runs any JavaScript it finds using a recent version of Chrome, similar to how your browser renders pages you visit. Rendering is important because websites often rely on JavaScript to bring content to the page, and without rendering Google might not see that content.\nCrawling depends on whether Google's crawlers can access the site. Some common issues with Googlebot accessing sites include:\nProblems with the server handling the site\nNetwork issues\nrobots.txt rules preventing Googlebot's access to the page\nIndexing\nAfter a page is crawled, Google tries to understand what the page is about. This stage is called indexing and it includes processing and analyzing the textual content and key content tags and attributes, such as <title> elements and alt attributes, images, videos, and more.\nDuring the indexing process, Google determines if a page is a duplicate of another page on the internet or canonical. The canonical is the page that may be shown in search results. To select the canonical, we first group together (also known as clustering) the pages that we found on the internet that have similar content, and then we select the one that's most representative of the group. The other pages in the group are alternate versions that may be served in different contexts, like if the user is searching from a mobile device or they're looking for a very specific page from that cluster.\nGoogle also collects signals about the canonical page and its contents, which may be used in the next stage, where we serve the page in search results. Some signals include the language of the page, the country the content is local to, and the usability of the page.\nThe collected information about the canonical page and its cluster may be stored in the Google index, a large database hosted on thousands of computers. Indexing isn't guaranteed; not every page that Google processes will be indexed.\nIndexing also depends on the content of the page and its metadata. Some common indexing issues can include:\nThe quality of the content on page is low\nRobots meta rules disallow indexing\nThe design of the website might make indexing difficult\nServing search results\nGoogle doesn't accept payment to rank pages higher, and ranking is done programmatically. Learn more about ads on Google Search.\nWhen a user enters a query, our machines search the index for matching pages and return the results we believe are the highest quality and most relevant to the user's query. Relevancy is determined by hundreds of factors, which could include information such as the user's location, language, and device (desktop or phone). For example, searching for \"bicycle repair shops\" would show different results to a user in Paris than it would to a user in Hong Kong.\nBased on the user's query the search features that appear on the search results page also change. For example, searching for \"bicycle repair shops\" will likely show local results and no image results, however searching for \"modern bicycle\" is more likely to show image results, but not local results. You can explore the most common UI elements of Google web search in our Visual Element gallery.\nSearch Console might tell you that a page is indexed, but you don't see it in search results. This might be because:\nThe content on the page is irrelevant to users' queries\nThe quality of the content is low\nRobots meta rules prevent serving\nWhile this guide explains how Search works, we are always working on improving our algorithms. You can keep track of these changes by following the Google Search Central blog.\nWas this helpful?\nSend feedback",
  "title": "In-depth guide to how Google Search works\nbookmark_border",
  "author": "",
  "publish_date": "",
  "source": "developers.google.com",
  "language": "auto",
  "word_count": 1144,
  "extraction_method": "article_playwright",
  "extraction_timestamp": "2025-11-06T18:22:58.782853",
  "batch_id": "20251106_101712",
  "link_id": "art_req19",
  "error": null,
  "article_id": "6c7022fd8311",
  "summary": {
    "transcript_summary": {
      "key_facts": [
        "FACT: Google Search uses automated web crawlers called Googlebot to discover and download content from the web.",
        "FACT: Most pages in Google Search results are added automatically, not manually submitted by website owners.",
        "FACT: Googlebot renders pages using a recent version of Chrome, including executing JavaScript to access dynamic content.",
        "FACT: Google does not accept payment to crawl sites more frequently or rank them higher in search results.",
        "FACT: Crawling is blocked if robots.txt disallows access, or if the site has server or network issues.",
        "FACT: Indexing involves analyzing text, metadata, images, and videos to determine a page's relevance and canonical form.",
        "FACT: Google groups similar pages into clusters and selects one as the canonical version for search results.",
        "FACT: Indexing is not guaranteed even if a page is crawled and processed by Google.",
        "FACT: Serving search results depends on hundreds of factors, including user location, language, and device type.",
        "FACT: Search results vary based on query type—for example, 'bicycle repair shops' triggers local results, while 'modern bicycle' favors image results.",
        "FACT: A page may be indexed but not appear in search results due to low content quality or restrictive robots meta rules."
      ],
      "key_opinions": [
        "OPINION: The current crawling algorithm prioritizes high-quality websites over low-value ones, which may disadvantage smaller sites.",
        "OPINION: Google’s reliance on JavaScript rendering could create inconsistencies for sites with heavy client-side content.",
        "OPINION: The lack of guaranteed indexing undermines trust in Google Search for content creators who follow best practices.",
        "OPINION: The canonical selection process may sometimes favor pages with more backlinks over those with better original content.",
        "OPINION: The absence of direct feedback mechanisms from Google makes it difficult for publishers to understand why their content isn’t ranking."
      ],
      "key_datapoints": [
        "DATA: Google uses billions of pages per day in its crawling operations across a massive distributed computing infrastructure.",
        "DATA: Googlebot uses a recent version of Chrome to render and execute JavaScript during page crawling.",
        "DATA: Google does not guarantee that any page will be crawled, indexed, or served in search results regardless of compliance with Search Essentials.",
        "DATA: Pages with low-quality content or poor usability are less likely to be indexed or ranked highly.",
        "DATA: Search results dynamically adapt based on user context such as location, language, and device type."
      ],
      "topic_areas": [
        "Web crawling process",
        "Indexing algorithms",
        "Search result serving",
        "Robots.txt configuration",
        "JavaScript rendering",
        "Canonical page selection",
        "Content quality signals",
        "User context in search",
        "Google Search transparency"
      ],
      "word_count": 1144,
      "total_markers": 21
    },
    "comments_summary": {},
    "created_at": "2025-11-06T18:25:08.228584",
    "model_used": "qwen-flash"
  }
}
{
  "success": true,
  "url": "https://medium.com/@myscale/understanding-vector-indexing-a-comprehensive-guide-d1abe36ccd3c",
  "content": "Top highlight\nUnderstanding Vector Indexing: A Comprehensive Guide\nMyScale\nFollow\n9 min read\n·\nFeb 13, 2024\n135\n4\nIn the early stages of database development, data were stored in basic tables. This method was straightforward, but as the amount of data grew, it became harder and slower to manage and retrieve information. Relational databases were then introduced, offering better ways to store and process data.\nAn important technique in relational databases is indexing, which is very similar to the way books are stored in a library. Instead of looking through the entire library, you go directly to a specific section where the required book is placed. Indexing in databases works in a similar way, speeding up the process of finding the data you need.\nIn this blog, we’ll cover the basics of vector indexing and how it is implemented using different techniques.\nWhat are Vector Embeddings\nVector embeddings are simply the numerical representation that is converted from images, text, and audio. In simpler terms, a single mathematical vector is created against each item capturing the semantics or characteristics of that item. These vector embeddings are more easily understood by computational systems and compatible with machine learning models to understand relationships and similarities between different items.\nPress enter or click to view image in full size\nSpecialized databases used to store these embedding vectors are known as vector databases. These databases take advantage of the mathematical properties of embeddings that allow similar items to be stored together. Different techniques are used to store similar vectors together and dissimilar vectors apart. They are vector indexing techniques.\nWhat is a Vector Index\nVector indexing is not just about storing data, it’s about intelligently organizing the vector embeddings to optimize the retrieval process. This technique involves advanced algorithms to neatly arrange the high-dimensional vectors in a searchable and efficient manner. This arrangement is not random; it’s done in a way that similar vectors are grouped together, by which vector indexing allows quick and accurate similarity searches and pattern identification, especially for searching large and complex datasets.\nPress enter or click to view image in full size\nLet’s say you have a vector for each image, capturing its features. The vector index will organize these vectors in a way that makes it easier to find similar images (here). You can think of it as if you are organizing each person’s images separately. So, if you need a specific person’s picture from a particular event, instead of searching through all the pictures, you would only go through that person’s collection and easily find the image.\nCommon Vector Indexing Techniques\nDifferent indexing techniques are used based on specific requirements. Let’s discuss some of these.\nInverted File (IVF)\nThis is the most basic indexing technique. It splits the whole data into several clusters using techniques like K-means clustering. Each vector of the database is assigned to a specific cluster. This structured arrangement of vectors allows the user to make the search queries way faster. When a new query comes, the system doesn’t traverse the whole dataset. Instead, it identifies the nearest or most similar clusters and searches for the specific document within those clusters.\nPress enter or click to view image in full size\nSo, applying brute-force search exclusively within the relevant cluster, rather than across the entire database, not only enhances search speed but also considerably reduces query time.\nVariants of IVF: IVFFLAT, IVFPQ and IVFSQ\nThere are different variants of IVF, depending on the specific requirement of the application. Let’s look at them in detail.\nIVFFLAT\nIVFFLAT is a simpler form of IVF. It partitions the dataset into clusters. However, within each cluster, it uses a flat structure (hence the name “FLAT”) for storing the vectors. IVFFLAT is designed to optimize the balance between search speed and accuracy.\nPress enter or click to view image in full size\nIn each cluster, vectors are stored in a simple list or array without additional subdivision or hierarchical structures. When a query vector is assigned to a cluster, the nearest neighbor is found by conducting a brute-force search, examining each vector in the cluster’s list and calculating its distance to the query vector. IVFFLAT is employed in scenarios where the dataset isn’t considerably large, and the objective is to achieve high accuracy in the search process.\nIVFPQ\nIVFPQ is an advanced variant of IVF, which stands for Inverted File with Product Quantization. It also splits the data into clusters but each vector in a cluster is broken down into smaller vectors, and each part is encoded or compressed into a limited number of bits using product quantization.\nPress enter or click to view image in full size\nFor a query vector, once the relevant cluster is identified, the algorithm compares the quantized representation of the query with the quantized representations of vectors within the cluster. This comparison is faster than comparing raw vectors due to the reduced dimensionality and size achieved through quantization. This method has two advantages over the previous method:\nThe vectors are stored in a compact way, taking less space than the raw vectors.\nThe query process is even faster because it doesn’t compare all raw vectors but encoded vectors.\nIVFSQ\nGet MyScale’s stories in your inbox\nJoin Medium for free to get updates from this writer.\nSubscribe\nFile System with Scalar Quantization (IVFSQ), like other IVF variants, also segments the data into clusters. However, the major difference is its quantization technique. In IVFSQ, each vector in a cluster is passed through scalar quantization. This means that each dimension of the vector is handled separately.\nPress enter or click to view image in full size\nIn simple terms, for every dimension of a vector, we set a predefined value or range. These values or ranges help decide which cluster a vector belongs to. Each component of the vector is then matched against these predefined values to find its place in a cluster. This method of breaking down and quantizing each dimension separately makes the process more straightforward. It’s especially useful for lower-dimensional data, as it simplifies encoding and reduces the space needed for storage.\nHierarchical Navigable Small World (HNSW) Algorithm\nThe Hierarchical Navigable Small World (HNSW) algorithm is a sophisticated method for storing and fetching data efficiently. Its graph-like structure takes inspiration from two different techniques: the probability skip list and Navigable Small World (NSW).\nTo better understand HNSW, let’s first try to understand the basic concepts related to this algorithm.\nSkip List\nA skip list is an advanced data structure that combines the advantages of two traditional structures: the quick insertion capability of a linked list and the rapid retrieval characteristic of an array. It achieves this through its multi-layer architecture where the data is organized across multiple layers, with each layer containing a subset of the data points.\nPress enter or click to view image in full size\nStarting from the bottom layer, which contains all data points, each succeeding layer skips some points and thus has fewer data points, ultimately the topmost layer will have the smallest number of data points.\nTo search for a data point in a skip list, we start from the highest layer and go from left to right exploring each data point. At any point, if the queried value is greater than the current datapoint, we move back to the previous datapoint in the layer below and resume the search from left to right until we locate the exact point.\nNavigable Small World (NSW)\nNavigable Small World (NSW) is similar to a proximate graph where nodes are linked together based on how similar they are to each other. The greedy method is used to search for the nearest neighbor point.\nWe always begin with a pre-defined entry point, which connects to multiple nearby nodes. We identify which of these nodes are the closest to our query vector and move there. This process iterates until there is no node closer to the query vector than the current one, serving as the stopping condition for the algorithm.\nNow, let’s get back to the main topic and see how it works.\nHow HNSW is Developed\nSo, what happens in HNSW is that we take the motivation from the skip list, and it creates layers like the skip list. But for the connection between the data points, it makes a graph-like connection between the nodes. The nodes at each layer are connected not only to the current layer nodes but also to the nodes of the lower layers. The nodes at the top are very few and intensity increases when we go down to the lower layers. The last layer contains all the data points of the database. This is what the HNSW architecture looks like.\nPress enter or click to view image in full size\nThe architecture of the HNSW algorithm\nHow Does a Search Query Work in HNSW\nThe algorithm starts at the predefined node of the topmost layer. It then computes the distance between the connected nodes of the current layer and those in the layer below. The algorithm shifts to a lower layer if the distance to a node in that layer is less than the distance to nodes in the current layer. This process continues until the last layer is reached or it reaches a node that has the minimum distance from all other connecting nodes. The final layer, Layer 0, contains all data points, providing a comprehensive and detailed representation of the dataset. This layer is crucial to ensure that the search incorporates all potential nearest neighbors.\nDifferent variants of HNSW: HNSWFLAT and HNSWSQ\nJust like IVFFLAT and IVFSQ, where one stores raw vectors and the other stores quantized ranges, the same process applies to HNSWFLAT and HNSWSQ. In HNSWFLAT, the raw vectors are stored as they are, while in HNSWSQ, the vectors are stored in a quantized form. Apart from this key difference in data storage, the overall process and methodology of indexing and searching are the same in both HNSWFLAT and HNSWSQ.\nMulti-Scale Tree Graph (MSTG) Algorithm\nStandard Inverted File Indexing (IVF) partitions a vector dataset into numerous clusters. However, a notable limitation is the substantial growth in index size for massive datasets, requiring the storage of many cluster representative vectors. The scalability of IVF is hindered by the significant memory overhead associated with this approach.\nMulti-Stage Tree Graph (MSTG) is developed by MyScale and it overcomes this through a hierarchical design. Unlike IVF which has a single layer of cluster vectors, MSTG creates multiple layers, which means less persistent centroids in the memory. For example, if a dataset needs 10,000 cluster vectors in IVF, all 10,000 have to be stored, consuming substantial memory. In MSTG, using a 2-layer hierarchy of 100 clusters per layer, only 200 vectors need storage — the 100 top-layer vectors, and their 100 direct children.\nMSTG combines the advantages of both tree and graph-based algorithms. It builds fast, searches fast, and remains fast and accurate under different filtered search ratios while being resource and cost-efficient.\nConclusion\nVector databases have revolutionized the way data is stored, providing not just enhanced speed but also immense utility across diverse domains such as artificial intelligence and big data analytics. In an era of exponential data growth, particularly in unstructured forms, the applications of vector databases are limitless. Customized indexing techniques further amplify the power and effectiveness of these databases, for example, boosting filtered vector search. MyScale optimizes the filter vector search with the unique MSTG algorithm, providing a powerful solution for complex, large-scale vector searches. Try MyScale today to experience the advanced vector search.",
  "title": "Understanding Vector Indexing: A Comprehensive Guide",
  "author": "MyScale",
  "publish_date": "2024-02-13T05:01:50.072Z",
  "source": "medium.com",
  "language": "auto",
  "word_count": 1932,
  "extraction_method": "article_playwright",
  "extraction_timestamp": "2025-11-06T18:20:20.142810",
  "batch_id": "20251106_101712",
  "link_id": "art_req12",
  "error": null,
  "article_id": "935c7481ea93",
  "summary": {
    "transcript_summary": {
      "key_facts": [
        "FACT: Vector embeddings are numerical representations of images, text, and audio that capture semantic meaning.",
        "FACT: Vector databases store embedding vectors and use indexing techniques to group similar vectors together.",
        "FACT: Vector indexing organizes high-dimensional vectors to enable fast and accurate similarity searches.",
        "FACT: Inverted File (IVF) indexing partitions data into clusters using K-means clustering for faster query processing.",
        "FACT: IVFFLAT uses a flat structure within each cluster, storing raw vectors in simple arrays for direct brute-force search.",
        "FACT: IVFPQ applies product quantization to compress vectors into smaller encoded representations for faster comparison.",
        "FACT: IVFSQ uses scalar quantization, where each vector dimension is independently mapped to predefined ranges.",
        "FACT: HNSW is a graph-based algorithm inspired by skip lists and Navigable Small World (NSW) for efficient nearest-neighbor search.",
        "FACT: HNSW uses multiple layers with decreasing node density, enabling fast traversal from top to bottom during search.",
        "FACT: HNSWFLAT stores raw vectors while HNSWSQ stores quantized vectors, differing only in storage format.",
        "FACT: MSTG is a hierarchical tree-graph algorithm developed by MyScale to reduce memory overhead in large-scale indexing.",
        "FACT: MSTG uses multiple layers of clusters, reducing persistent centroid storage compared to single-layer IVF.",
        "FACT: MyScale’s MSTG algorithm improves scalability, speed, and cost-efficiency for filtered vector searches."
      ],
      "key_opinions": [
        "OPINION: Vector indexing is essential for handling the growing complexity of unstructured data in modern AI systems.",
        "OPINION: The choice of indexing technique significantly impacts both performance and resource usage in vector databases.",
        "OPINION: IVF variants offer good trade-offs between speed and accuracy but may struggle with very large datasets.",
        "OPINION: HNSW provides superior search efficiency due to its layered graph structure and intelligent navigation.",
        "OPINION: MSTG represents a major advancement over traditional IVF by minimizing memory footprint without sacrificing accuracy."
      ],
      "key_datapoints": [
        "DATA: IVF reduces query time by focusing on relevant clusters instead of scanning the entire dataset.",
        "DATA: IVFPQ reduces vector storage size through product quantization, improving space efficiency.",
        "DATA: IVFSQ processes each vector dimension separately, simplifying encoding for lower-dimensional data.",
        "DATA: HNSW uses a multi-layered architecture with fewer nodes at higher layers and full coverage at Layer 0.",
        "DATA: MSTG reduces required centroid storage from 10,000 (in IVF) to just 200 (in a two-layer hierarchy).",
        "DATA: MyScale’s MSTG algorithm maintains high accuracy across different filtered search ratios."
      ],
      "topic_areas": [
        "Vector embeddings",
        "Vector indexing techniques",
        "Inverted File (IVF)",
        "HNSW algorithm",
        "MSTG algorithm",
        "Database performance optimization",
        "AI data storage",
        "Similarity search",
        "Scalable vector databases"
      ],
      "word_count": 1932,
      "total_markers": 24
    },
    "comments_summary": {},
    "created_at": "2025-11-06T18:24:12.068955",
    "model_used": "qwen-flash"
  }
}
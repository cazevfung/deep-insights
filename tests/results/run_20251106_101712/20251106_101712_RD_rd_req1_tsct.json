{
  "success": true,
  "url": "https://www.reddit.com/r/neovim/comments/1ijgamd/what_is_open_sources_answer_to_cursors_codebase/",
  "content": "window.__servedBy = new Promise((resolve) => { window.__servedByRes = resolve; });\nwindow.hasTabBeenBackgrounded = document.hidden;\nWhat Is Open Source's Answer To Cursor's Codebase Level Context For Large Projects? : r/neovim\nEspañol (Latinoamérica)\nWhat Is Open Source's Answer To Cursor's Codebase Level Context For Large Projects?\nSo, there are a number of different AI plugins out there right now, but one of the things that Cursor really seems to shine with is getting context over an entire codebase. My organization has a 144,000+ file monorepo, and currently it feels like Neovim plugins can't really capture that complexity super well. As I see it, it sounds like most of the AI plugins that will be able to compete are going to need some kind of DB to store context, so what exactly that ends up being is hard to know.\nI'm wondering, primarily for plugin authors in the AI space, what you think of this problem and where the challenges are with it? With Cursor being private, and a company, they can use a number of different pieces of infra to manage the functionality of codebase aware context.\nSo this is more of an open exploration instead of a \"oh shit what are we going to do\".\nwindow.POST_ID = 't3_1ijgamd';\nconst e=\"image\",t=\"video\",i=\"embed\",n=\"devvit\",d=\"no_media\",o=\"unknown\",r=[\"SHREDDIT-PLAYER-2\",\"SHREDDIT-PLAYER-STATIC-HLSJS\"];function a(e){return performance.mark(\"post-consumption-ready\",{detail:{mediaType:e}})}function s(){if(!window.POST_ID)return void console.error(\"[post-load-performance-mark] POST_ID not found\");const s=document.querySelector(`shreddit-post[id=${window.POST_ID}] div[slot=\"post-media-container\"], shreddit-post[id=${window.POST_ID}] shreddit-post-text-body`)?.querySelector(\"shreddit-player-2, shreddit-player-static-hlsjs, img, video, shreddit-embed, shreddit-tweet-embed, shreddit-devvit-ui-loader\");s&&function(e,t={}){const{checkInViewport:i=!0,requireVisible:n=!1}=t,d=e.getBoundingClientRect(),o=window.innerHeight||document.documentElement.clientHeight,r=window.innerWidth||document.documentElement.clientWidth;let a=!0;if(i){const e=d.bottom>0&&d.right>0&&d.top<o&&d.left<r;a=a&&e}if(n){const t=window.getComputedStyle(e),i=d.width>0&&d.height>0&&\"none\"!==t.display&&\"hidden\"!==t.visibility&&\"0\"!==t.opacity;a=a&&i}return a}(s)?async function(d){if(!d)return;const s=d.tagName;let c=o,m=d;if(s.endsWith(\"-EMBED\")&&(c=i,await customElements.whenDefined(d.tagName.toLowerCase()),await d.updateComplete,m=d.shadowRoot.querySelector(\"iframe\")),\"SHREDDIT-DEVVIT-UI-LOADER\"===s)return c=n,void d.addEventListener(\"devvit-app-first-render\",(()=>a(c)),{once:!0});if(r.includes(s))return c=t,d.autoplay?void d.videoElement.addEventListener(\"playing\",(()=>a(c)),{once:!0}):(await customElements.whenDefined(d.tagName.toLowerCase()),await d.updateComplete,void a(c));if(m&&\"VIDEO\"===m.tagName)return c=t,void m.addEventListener(\"loadeddata\",(()=>a(c)));m&&\"IMG\"===m.tagName&&(c=e,m.complete)?a(c):m&&m.addEventListener(\"load\",(()=>{a(c)}))}(s):a(d)}\"undefined\"==typeof window||window.process?.env?.WTR_TEST||window.SKIP_POST_LOAD_PERF_MARK_AUTO_RUN||s();export{s as init};\nFirst off just know that cursor is not using your entire codebase as context. For that large size that is pretty much impossible and would drain the bank account. Let alone context window limitations and how slow it would be.\nI believe many of these AI IDEs are using a combination of high level symbol summaries (eg function signatures), dependency relations (what imports a file is using), LSP references (what files are using a function for example), some grepping of keywords, what files you currently have opened and what files you have recently opened.\nI agree plugins will lag behind, but all these things are doable… I think the UX of it all is the hardest part.\nIt does but it does it by vectorizing your code into a knowledge database and then using vector search to extract relevant pieces of code and inject them into the context. It’s easier for a paid for IDE to do this as they control the entire ecosystem and most probably the average user won’t use enough to pass the cost of the subscription.\nOpen source could do this but then you would have to setup the vector database by yourself which the average dev wouldn’t do. Also, a plugin author has less control over your entire codebase so the entire plugin setup wouldn’t be so pleasant.\nThe vectorization process also takes quite a bit of time so my guess is that Cursor does it by sending the code to their server and processing the files on the server while an open-source project would be forced to do it on the user’s machine or force the user to setup a VPS to do this. All of these options are less then ideal.\nI'm not convinced a vector database of an every line of code is the only best approach. Vectoriation is necessary for documents written in natural language, but less so for the clear well-defined structure of a programming language. And in cases where vector search makes sense, I think vectoriztion of API documentation (e.g. javadoc, jsdoc) would be a better use of resources than every line of code.\nIt’s easier for a paid for IDE to do this as they control the entire ecosystem and most probably the average user won’t use enough to pass the cost of the subscription.\nThis makes no sense to me. There are low cost models, such as gemini flash, which can do this fast and economically.\nOpen source could do this but then you would have to setup the vector database by yourself which the average dev wouldn’t do.\nThere are several vector database engines that are just libraries. No setup necessary.\nThe vectorization process also takes quite a bit of time so my guess is that Cursor does it by sending the code to their server and processing the files on the server while an open-source project would be forced to do it on the user’s machine or force the user to setup a VPS to do this.\nThere's no reason to \"do it on the user's machine\". Use embedding APIs. It could be pipelined, so embedding is happening in parallel to indexing.\nWhat do you think of AIDER? It is editor-independent, so you can use it with every editor, including Neovim\nsolves this problem exactly, and better.\nsimple, don't use AI as a crutch\nThey hated Jesus because he told the truth\nYep, open source doesn't care nearly as much\nYep I also turn off auto complete, diagnostics, syntax highlighting, and my computer. I only code with pen and paper like a real man.\nWhat's the point of this comment? Do you really not understand what it means to use AI as a crutch? Or are you trying to make a bigger point?\nAgreed. I use Neovim with an AI plugin daily but still turn to Cursor occasionally—it excels at capturing the entire codebase's context. That said, I'm fine with a less capable AI plugin since it forces me to think critically rather than defaulting to AI.\nAI plugins will take time to catch up to Cursor. The former are free, often maintained by one person, while Cursor has full-time devs and VC backing. The incentives just aren’t there to close the gap soon.\naider is the open source answer to that. it can build up a repo map of your codebase using the most important classes, and functions along with their types and signatures. This context can be sent to the LLM of you choice.\naider/coders/base_coder.py:\n⋮...\n│class Coder:\n│ abs_fnames = None\n⋮...\n│ @classmethod\n│ def create(\n│ self,\n│ main_model,\n│ edit_format,\n│ io,\n│ skip_model_availabily_check=False,\n│ **kwargs,\n⋮...\n│ def abs_root_path(self, path):\n⋮...\n│ def run(self, with_message=None):\n⋮...\nIt also does some optimization by sending just the most relevant parts of the repo map using a graph ranking algorithm.\nHere's some more info about how aider builds the repo map:\nhttps://aider.chat/docs/repomap.html\nI find Aider tries to do too much. The couple times I tested it tried to install dependencies and run commands. While this appears to be fine, the deps selection was poor, many already legacy and at the end it failed (during both tests). In the other hand, more specific plugins, like Avante, just make the expected intervention\nAm I missing Aider? Could be skill issues\nCould be LLM choice. Neither Aider nor Avante did the actual choosing of the dependencies, an LLM did. However, prompt text does make a difference.\nI used Avante for a while, but found it to really struggle on large code bases (144,000+ files)\nI thought supermaven did some similar things? But also there WAS someone who made a post earlier about thinking about making a plugin for LLMs that has the tech for that kind of context I\n. I’ll go see if I can find it and I guess you can tell me if it’s what ur talking about if you want XD\nIt seems to be talking about the same kind of thing as the other comments, but keep in mind it’s in early development still probably seeing as its only been a month or two: Original post I saw:\nhttps://www.reddit.com/r/neovim/comments/1hyict6/mixed_feelings_about_a_tool_im_working_on/\nA post about the initial plugin release:\nhttps://www.reddit.com/r/neovim/comments/1hzjnz1/supercharge_your_llm_completionchatbot_plugin\nThe plugin repo itself:\nhttps://github.com/Davidyz/VectorCode\n.snoo-cls-11,.snoo-cls-9{stroke-width:0}.snoo-cls-9{fill:#842123}.snoo-cls-11{fill:#ffc49c}\nreReddit：2025年2月6日的热门帖子\nreReddit：2025年2月的热门帖子\nReddit, Inc. © 2025。保留所有权利。\nif(window.__servedByRes)window.__servedByRes(\"4/5\")\n--- Top Comments ---\nsmurfman111\n•\n9个月前\nFirst off just know that cursor is not using your entire codebase as context. For that large size that is pretty much impossible and would drain the bank account. Let alone context window limitations and how slow it would be.\nI believe many of these AI IDEs are using a combination of high level symbol summaries (eg function signatures), dependency relations (what imports a file is using), LSP references (what files are using a function for example), some grepping of keywords, what files you currently have opened and what files you have recently opened.\nI agree plugins will lag behind, but all these things are doable… I think the UX of it all is the hardest part.\n51\nfehlix\n•\n9个月前\nIt does but it does it by vectorizing your code into a knowledge database and then using vector search to extract relevant pieces of code and inject them into the context. It’s easier for a paid for IDE to do this as they control the entire ecosystem and most probably the average user won’t use enough to pass the cost of the subscription.\nOpen source could do this but then you would have to setup the vector database by yourself which the average dev wouldn’t do. Also, a plugin author has less control over your entire codebase so the entire plugin setup wouldn’t be so pleasant.\nThe vectorization process also takes quite a bit of time so my guess is that Cursor does it by sending the code to their server and processing the files on the server while an open-source project would be forced to do it on the user’s machine or force the user to setup a VPS to do this. All of these options are less then ideal.\n6\nfunbike\n•\n9个月前\nI'm not convinced a vector database of an every line of code is the only best approach. Vectoriation is necessary for documents written in natural language, but less so for the clear well-defined structure of a programming language. And in cases where vector search makes sense, I think vectoriztion of API documentation (e.g. javadoc, jsdoc) would be a better use of resources than every line of code.\nIt’s easier for a paid for IDE to do this as they control the entire ecosystem and most probably the average user won’t use enough to pass the cost of the subscription.\nThis makes no sense to me. There are low cost models, such as gemini flash, which can do this fast and economically.\nOpen source could do this but then you would have to setup the vector database by yourself which the average dev wouldn’t do.\nThere are several vector database engines that are just libraries. No setup necessary.\nThe vectorization process also takes quite a bit of time so my guess is that Cursor does it by sending the code to their server and processing the files on the server while an open-source project would be forced to do it on the user’s machine or force the user to setup a VPS to do this.\nThere's no reason to \"do it on the user's machine\". Use embedding APIs. It could be pipelined, so embedding is happening in parallel to indexing.\n3\n另外 2 条回复\n---\nfehlix\n•\n9个月前\nIt does but it does it by vectorizing your code into a knowledge database and then using vector search to extract relevant pieces of code and inject them into the context. It’s easier for a paid for IDE to do this as they control the entire ecosystem and most probably the average user won’t use enough to pass the cost of the subscription.\nOpen source could do this but then you would have to setup the vector database by yourself which the average dev wouldn’t do. Also, a plugin author has less control over your entire codebase so the entire plugin setup wouldn’t be so pleasant.\nThe vectorization process also takes quite a bit of time so my guess is that Cursor does it by sending the code to their server and processing the files on the server while an open-source project would be forced to do it on the user’s machine or force the user to setup a VPS to do this. All of these options are less then ideal.\n6\nfunbike\n•\n9个月前\nI'm not convinced a vector database of an every line of code is the only best approach. Vectoriation is necessary for documents written in natural language, but less so for the clear well-defined structure of a programming language. And in cases where vector search makes sense, I think vectoriztion of API documentation (e.g. javadoc, jsdoc) would be a better use of resources than every line of code.\nIt’s easier for a paid for IDE to do this as they control the entire ecosystem and most probably the average user won’t use enough to pass the cost of the subscription.\nThis makes no sense to me. There are low cost models, such as gemini flash, which can do this fast and economically.\nOpen source could do this but then you would have to setup the vector database by yourself which the average dev wouldn’t do.\nThere are several vector database engines that are just libraries. No setup necessary.\nThe vectorization process also takes quite a bit of time so my guess is that Cursor does it by sending the code to their server and processing the files on the server while an open-source project would be forced to do it on the user’s machine or force the user to setup a VPS to do this.\nThere's no reason to \"do it on the user's machine\". Use embedding APIs. It could be pipelined, so embedding is happening in parallel to indexing.\n3\n另外 2 条回复\n---\nfunbike\n•\n9个月前\nI'm not convinced a vector database of an every line of code is the only best approach. Vectoriation is necessary for documents written in natural language, but less so for the clear well-defined structure of a programming language. And in cases where vector search makes sense, I think vectoriztion of API documentation (e.g. javadoc, jsdoc) would be a better use of resources than every line of code.\nIt’s easier for a paid for IDE to do this as they control the entire ecosystem and most probably the average user won’t use enough to pass the cost of the subscription.\nThis makes no sense to me. There are low cost models, such as gemini flash, which can do this fast and economically.\nOpen source could do this but then you would have to setup the vector database by yourself which the average dev wouldn’t do.\nThere are several vector database engines that are just libraries. No setup necessary.\nThe vectorization process also takes quite a bit of time so my guess is that Cursor does it by sending the code to their server and processing the files on the server while an open-source project would be forced to do it on the user’s machine or force the user to setup a VPS to do this.\nThere's no reason to \"do it on the user's machine\". Use embedding APIs. It could be pipelined, so embedding is happening in parallel to indexing.\n3\n另外 2 条回复\n---\nBrianHuster\n•\n9个月前\nWhat do you think of AIDER? It is editor-independent, so you can use it with every editor, including Neovim\n8\nsuedepaid\n•\n9个月前\nI was gonna say, aider solves this problem exactly, and better.\n2\n---\nsuedepaid\n•\n9个月前\nI was gonna say, aider solves this problem exactly, and better.\n2\n---\n[已删除]\n•\n9个月前\nsimple, don't use AI as a crutch\n31\nazdak\n•\n9个月前\nThey hated Jesus because he told the truth\n22\nJmc_da_boss\n•\n9个月前\nYep, open source doesn't care nearly as much\n9\nbr1ghtsid3\n•\n9个月前\nYep I also turn off auto complete, diagnostics, syntax highlighting, and my computer. I only code with pen and paper like a real man.\n1\n[已删除]\n•\n9个月前\nWhat's the point of this comment? Do you really not understand what it means to use AI as a crutch? Or are you trying to make a bigger point?\n0\n另外 5 条回复\n---\nazdak\n•\n9个月前\nThey hated Jesus because he told the truth\n22\n---\nJmc_da_boss\n•\n9个月前\nYep, open source doesn't care nearly as much\n9\n---\nbr1ghtsid3\n•\n9个月前\nYep I also turn off auto complete, diagnostics, syntax highlighting, and my computer. I only code with pen and paper like a real man.\n1\n[已删除]\n•\n9个月前\nWhat's the point of this comment? Do you really not understand what it means to use AI as a crutch? Or are you trying to make a bigger point?\n0\n另外 5 条回复\n---\n[已删除]\n•\n9个月前\nWhat's the point of this comment? Do you really not understand what it means to use AI as a crutch? Or are you trying to make a bigger point?\n0\n另外 5 条回复\n---\nmanaging_redditor\n•\n9个月前\nAgreed. I use Neovim with an AI plugin daily but still turn to Cursor occasionally—it excels at capturing the entire codebase's context. That said, I'm fine with a less capable AI plugin since it forces me to think critically rather than defaulting to AI.\nAI plugins will take time to catch up to Cursor. The former are free, often maintained by one person, while Cursor has full-time devs and VC backing. The incentives just aren’t there to close the gap soon.\n5\n---\nnuvicc\n•\n9个月前\naider is the open source answer to that. it can build up a repo map of your codebase using the most important classes, and functions along with their types and signatures. This context can be sent to the LLM of you choice.\nFor example:\naider/coders/base_coder.py:\n⋮...\n│class Coder:\n│ abs_fnames = None\n⋮...\n│ @classmethod\n│ def create(\n│ self,\n│ main_model,\n│ edit_format,\n│ io,\n│ skip_model_availabily_check=False,\n│ **kwargs,\n⋮...\n│ def abs_root_path(self, path):\n⋮...\n│ def run(self, with_message=None):\n⋮...\nIt also does some optimization by sending just the most relevant parts of the repo map using a graph ranking algorithm.\nHere's some more info about how aider builds the repo map: https://aider.chat/docs/repomap.html\n5\njohmsalas\n•\n9个月前\nI find Aider tries to do too much. The couple times I tested it tried to install dependencies and run commands. While this appears to be fine, the deps selection was poor, many already legacy and at the end it failed (during both tests). In the other hand, more specific plugins, like Avante, just make the expected intervention\nAm I missing Aider? Could be skill issues\n2\nfunbike\n•\n9个月前\nCould be LLM choice. Neither Aider nor Avante did the actual choosing of the dependencies, an LLM did. However, prompt text does make a difference.\n1\n另外 2 条回复\nDennisTheMenace780\n•\n9个月前\nI used Avante for a while, but found it to really struggle on large code bases (144,000+ files)\n1\n---\njohmsalas\n•\n9个月前\nI find Aider tries to do too much. The couple times I tested it tried to install dependencies and run commands. While this appears to be fine, the deps selection was poor, many already legacy and at the end it failed (during both tests). In the other hand, more specific plugins, like Avante, just make the expected intervention\nAm I missing Aider? Could be skill issues\n2\nfunbike\n•\n9个月前\nCould be LLM choice. Neither Aider nor Avante did the actual choosing of the dependencies, an LLM did. However, prompt text does make a difference.\n1\n另外 2 条回复\nDennisTheMenace780\n•\n9个月前\nI used Avante for a while, but found it to really struggle on large code bases (144,000+ files)\n1\n---\nfunbike\n•\n9个月前\nCould be LLM choice. Neither Aider nor Avante did the actual choosing of the dependencies, an LLM did. However, prompt text does make a difference.\n1\n另外 2 条回复\n---\nDennisTheMenace780\n•\n9个月前\nI used Avante for a while, but found it to really struggle on large code bases (144,000+ files)\n1\n---\nserialized-kirin\n•\n9个月前\nI thought supermaven did some similar things? But also there WAS someone who made a post earlier about thinking about making a plugin for LLMs that has the tech for that kind of context I think. I’ll go see if I can find it and I guess you can tell me if it’s what ur talking about if you want XD\n2\nkinji_kasumi\n•\n8个月前\ni need\n1\nserialized-kirin\n•\n8个月前\nIt seems to be talking about the same kind of thing as the other comments, but keep in mind it’s in early development still probably seeing as its only been a month or two: Original post I saw: https://www.reddit.com/r/neovim/comments/1hyict6/mixed_feelings_about_a_tool_im_working_on/\nA post about the initial plugin release: https://www.reddit.com/r/neovim/comments/1hzjnz1/supercharge_your_llm_completionchatbot_plugin\nThe plugin repo itself: https://github.com/Davidyz/VectorCode\n2\n---\nkinji_kasumi\n•\n8个月前\ni need\n1\nserialized-kirin\n•\n8个月前\nIt seems to be talking about the same kind of thing as the other comments, but keep in mind it’s in early development still probably seeing as its only been a month or two: Original post I saw: https://www.reddit.com/r/neovim/comments/1hyict6/mixed_feelings_about_a_tool_im_working_on/\nA post about the initial plugin release: https://www.reddit.com/r/neovim/comments/1hzjnz1/supercharge_your_llm_completionchatbot_plugin\nThe plugin repo itself: https://github.com/Davidyz/VectorCode\n2\n---\nserialized-kirin\n•\n8个月前\nIt seems to be talking about the same kind of thing as the other comments, but keep in mind it’s in early development still probably seeing as its only been a month or two: Original post I saw: https://www.reddit.com/r/neovim/comments/1hyict6/mixed_feelings_about_a_tool_im_working_on/\nA post about the initial plugin release: https://www.reddit.com/r/neovim/comments/1hzjnz1/supercharge_your_llm_completionchatbot_plugin\nThe plugin repo itself: https://github.com/Davidyz/VectorCode\n2",
  "title": "What Is Open Source's Answer To Cursor's Codebase Level Context For Large Projects?",
  "author": "",
  "publish_date": "9个月前",
  "source": "Reddit",
  "language": "auto",
  "word_count": 3554,
  "extraction_method": "reddit",
  "extraction_timestamp": "2025-11-06T18:17:59.212885",
  "batch_id": "20251106_101712",
  "link_id": "rd_req1",
  "error": null,
  "summary": {
    "transcript_summary": {
      "key_facts": [
        "FACT: Cursor does not use the entire codebase as context due to size, cost, and context window limitations.",
        "FACT: AI IDEs like Cursor use a combination of function signatures, dependency relations, LSP references, keyword grepping, and recently opened files for context.",
        "FACT: Aider is an open-source, editor-independent tool that builds a repo map using key classes, functions, and their types/signatures.",
        "FACT: Aider uses graph ranking algorithms to send only the most relevant parts of the repo map to the LLM.",
        "FACT: VectorCode is an early-stage Neovim plugin aiming to provide codebase-level context using vectorization techniques.",
        "FACT: Supermaven is another project exploring similar codebase-aware AI functionality for large repositories.",
        "FACT: Open-source solutions face challenges in setting up vector databases without user intervention or infrastructure setup.",
        "FACT: Plugin authors have limited control over the user's entire codebase compared to proprietary IDEs.",
        "FACT: Vectorization of API documentation (e.g., Javadoc, JsDoc) may be more effective than full-codebase vectorization.",
        "FACT: Embedding APIs can be used instead of on-device processing, enabling pipelined indexing and embedding."
      ],
      "key_opinions": [
        "OPINION: The UX of codebase-aware AI tools is the hardest part to get right, even if technical feasibility exists.",
        "OPINION: Aider tries to do too much by attempting to install dependencies and run commands, which can lead to failures.",
        "OPINION: Open-source projects struggle to match Cursor’s capabilities due to lack of full-time developers and VC backing.",
        "OPINION: Using AI as a crutch undermines the developer’s critical thinking and problem-solving skills.",
        "OPINION: Vectorizing every line of code is unnecessary for structured programming languages and inefficient.",
        "OPINION: Open-source ecosystems care less about enterprise-grade AI features compared to commercial products.",
        "OPINION: The choice of LLM and prompt engineering significantly impacts dependency selection accuracy in AI tools.",
        "OPINION: Neovim plugins are sufficient for many developers, especially when they encourage intentional coding practices."
      ],
      "key_datapoints": [
        "DATA: The organization’s monorepo contains over 144,000 files.",
        "DATA: Aider uses graph ranking to optimize the retrieval of relevant code segments from the repo map.",
        "DATA: VectorCode is in early development, with initial posts dating back about one to two months.",
        "DATA: Aider’s repo map includes class definitions, function signatures, and type information.",
        "DATA: Embedding APIs can be used to avoid on-device vectorization, reducing local computational load.",
        "DATA: Gemini Flash is cited as a low-cost model capable of efficient embedding tasks.",
        "DATA: Some users report dependency installation failures in Aider due to poor package selection.",
        "DATA: Avante struggles with large codebases exceeding 144,000 files.",
        "DATA: Aider’s implementation includes a base_coder.py file with methods like abs_root_path and run.",
        "DATA: The vectorization process can take significant time, making server-side processing preferable."
      ],
      "topic_areas": [
        "Codebase context in AI IDEs",
        "Open-source vs. proprietary AI tools",
        "Vectorization of source code",
        "Neovim AI plugins",
        "Repo mapping and indexing",
        "AI plugin limitations",
        "Developer workflow preferences",
        "Cost and scalability of AI features",
        "Dependency management in AI-assisted coding",
        "User experience in AI-powered development"
      ],
      "word_count": 3554,
      "total_markers": 28
    },
    "comments_summary": {},
    "created_at": "2025-11-06T18:26:13.925744",
    "model_used": "qwen-flash"
  }
}
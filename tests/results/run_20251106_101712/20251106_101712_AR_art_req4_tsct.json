{
  "success": true,
  "url": "https://medium.com/@wangxj03/semantic-code-search-010c22e7d267",
  "content": "Semantic Code Search\nXiaojing\nFollow\n5 min read\n·\nSep 28, 2024\n64\n5\nThere’s been a lot of buzz lately about Cursor, particularly its codebase indexing feature. This feature turns Cursor into a context-aware coding assistant. But how does it work, and can we build something similar? Let’s dive in.\nUnderstanding Cursor’s Magic\nCursor’s codebase indexing, as explained in this forum post, works as follows:\nIt chunks your codebase files locally.\nThese chunks are then sent to Cursor’s server, where embeddings are created using either OpenAI’s embedding API or a custom embedding model.\nThe embeddings, along with start/end line numbers and file paths, are stored in a remote vector database.\nWhen you use @Codebase or ⌘ Enter to ask about your codebase, Cursor retrieves relevant code chunks from this database to provide context for large language model (LLM) calls. In essence, Cursor employs a Retrieval-Augmented Generation (RAG) model, with the codebase index acting as the retrieval mechanism.\nBuilding Our Own Semantic Code Search\nInspired by Cursor and Qdrant’s code search demo, we’ll replicate the codebase indexing feature and use it to power a semantic code search application. This application includes two main components:\nAn offline ingestion pipeline to index code embeddings into a vector database\nA code search server for semantic retrieval from this database\nLet’s break down each component and explore how we can implement them.\nIngestion Pipeline\nOur ingestion pipeline involves three crucial steps: splitting source code, creating embeddings, and indexing them in a vector database.\nWhy Split Source Code?\nSplitting source code files serves two primary purposes:\nOvercoming Model Input Limits: Embedding models have token limits. OpenAI’s text-embedding-3-small model, for example, has a token limit of 8192. Splitting ensures we stay within these bounds.\nEnhancing Semantic Granularity: Smaller chunks offer more precise semantic understanding. By focusing on specific parts of the code, we improve retrieval relevance and quality.\nSplitting Strategies\nWhile you could split code based on characters, words, or lines, a more sophisticated approach is to split based on tokens. We’ll use tiktoken, a fast Byte Pair Encoding (BPE) tokenizer compatible with OpenAI models.\nA naive strategy is to split code based on a fixed token count, but this can cut off code blocks like functions or classes mid-way. A more effective approach is to use an intelligent splitter that understands code structure, such as Langchain’s recursive text splitter. This method uses high-level delimiters (e.g., class and function definitions) to split at the appropriate semantic boundaries. However, this approach is language-specific and can struggles with languages that use curly braces for block delimitation.\nGet Xiaojing’s stories in your inbox\nJoin Medium for free to get updates from this writer.\nSubscribe\nA even more elegant solution is to split the code based on its Abstract Syntax Tree (AST) structure, as outlined in this blog post. By traversing the AST depth-first, it splits code into sub-trees that fit within the token limits. To avoid creating too many small chunks, sibling nodes are merged into larger chunks as long as they stay under the token limit. LlamaIndex offers a clean Python implementation in its CodeSplitter function. Both implementations use tree-sitter for AST parsing, which supports a wide range of languages.\nWe use code-splitter (shameless plug: I’m the author!), a Rust re-implementation for added efficiency. Here is an example of using its Python bindings to split Rust files in a directory:\nfrom code_splitter import Language, TiktokenSplitter\ndef walk(dir: str, max_size: int) -> Generator[dict[str, Any], None, None]:\nsplitter = TiktokenSplitter(Language.Rust, max_size=max_size)\nfor root, _, files in os.walk(dir):\nfor file in files:\nif not file.endswith(\".rs\"):\ncontinue\nfile_path = os.path.join(root, file)\nrel_path = os.path.relpath(file_path, dir)\nwith open(file_path, mode=\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\nlines = f.readlines()\nwith open(file_path, mode=\"rb\") as f:\ncode = f.read()\nchunks = splitter.split(code)\nfor chunk in chunks:\nyield {\n\"file_path\": rel_path,\n\"file_name\": file,\n\"start_line\": chunk.start,\n\"end_line\": chunk.end,\n\"text\": \"\\n\".join(lines[chunk.start : chunk.end]),\n\"size\": chunk.size,\n}\nCreating Embeddings\nQdrant’s authors used the open-source all-MiniLM-L6-v2 embedding model in their demo. Since this model is primarily trained on natural language tasks, they created a synthetic text-like representation of the code and passed it to the model. The representation captures key elements like function names, signatures, and docstrings.\nWe opted to use OpenAI’s text-embedding-3-small model. While not specifically trained on code, it performs reasonably well on code-related tasks. For those seeking more specialized alternatives, consider Microsoft’s unixcoder-base or Voyage AI’s voyage-code-2 which provides a much larger token limit.\nIndexing\nTo index the code chunk embeddings, we’ll use Qdrant as in the original demo. Qdrant is an open-source vector database written in Rust and is optimized to handle high-dimensional vectors at scale. Here’s how we index our embeddings along with metadata such as file paths, start/end line numbers, and chunk sizes. This metadata enables the frontend to display relevant information during search results.\nimport pandas as pd\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, PointStruct, VectorParams\ndf = pd.read_parquet(\"/data/code_embeddings.parquet\")\nclient = QdrantClient(\"http://localhost:6333\")\nclient.recreate_collection(\ncollection_name=\"qdrant-code\",\nvectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n)\npoints = [\nPointStruct(\nid=idx,\nvector=row[\"embedding\"].tolist(),\npayload=row.drop([\"embedding\"]).to_dict(),\n)\nfor idx, row in df.iterrows()\n]\nclient.upload_points(\"qdrant-code\", points)\nWe’ll also index entire code files in a separate Qdrant collection, enabling full-file retrieval during search results.\nSemantic Code Search\nWith our Qdrant database populated with code chunk embeddings and metadata, we can now build a code search server. Here’s the architecture of our search application:\nPress enter or click to view image in full size\nThe backend, built with FastAPI, handles REST requests and interacts with the Qdrant vector database. It exposes two endpoints:\nGET /api/search: Search for code snippets based on a query.\nGET /api/file: Fetch the full content of a file based on its path.\nFor the frontend, we’ll reuse the React code from Qdrant’s demo. Below is an example of what a query might look like in the UI:\nPress enter or click to view image in full size\nWrapping Up\nWe’ve successfully built a semantic code search application that mirrors Cursor’s codebase indexing functionality. This solution offers full control over each component — from code splitting and embedding generation to vector database indexing and search server implementation.\nReady to dive deeper? Check out the complete source code on GitHub: https://github.com/wangxj03/ai-cookbook/tree/main/code-search\nHappy coding, and may your semantic searches always find what you’re looking for!",
  "title": "Semantic Code Search",
  "author": "Xiaojing",
  "publish_date": "2024-09-28T05:54:19.811Z",
  "source": "medium.com",
  "language": "auto",
  "word_count": 1037,
  "extraction_method": "article_playwright",
  "extraction_timestamp": "2025-11-06T18:17:53.559197",
  "batch_id": "20251106_101712",
  "link_id": "art_req4",
  "error": null,
  "article_id": "e3766835e45e",
  "summary": {
    "transcript_summary": {
      "key_facts": [
        "FACT: Cursor’s codebase indexing feature processes code locally before sending chunks to its server for embedding generation.",
        "FACT: Embeddings are created using either OpenAI’s embedding API or a custom model and stored in a remote vector database.",
        "FACT: The retrieval mechanism uses a Retrieval-Augmented Generation (RAG) model with codebase index as the context source.",
        "FACT: Code splitting is essential to stay within token limits of embedding models like OpenAI’s text-embedding-3-small (8192 tokens).",
        "FACT: Semantic granularity improves when code is split into smaller, meaningful chunks based on structural boundaries.",
        "FACT: Tiktoken is used as a BPE tokenizer compatible with OpenAI models for token-based code splitting.",
        "FACT: Langchain’s recursive text splitter uses high-level delimiters like function and class definitions to preserve semantic integrity.",
        "FACT: AST-based splitting using tree-sitter traverses code structure depth-first and merges sibling nodes under token limits.",
        "FACT: The code-splitter library offers a Rust re-implementation for improved performance and supports multiple programming languages.",
        "FACT: OpenAI’s text-embedding-3-small is used for code embeddings despite not being code-specific, due to strong general performance.",
        "FACT: Qdrant is an open-source vector database written in Rust, optimized for high-dimensional vector storage and retrieval.",
        "FACT: Metadata such as file paths, line numbers, and chunk sizes are indexed alongside embeddings for accurate result attribution.",
        "FACT: Full code files are indexed in a separate Qdrant collection to enable full-file retrieval during search.",
        "FACT: The semantic code search backend is built with FastAPI and exposes two REST endpoints: /api/search and /api/file.",
        "FACT: The frontend reuses React components from Qdrant’s public demo for consistent UI interaction."
      ],
      "key_opinions": [
        "OPINION: AST-based code splitting provides more precise and semantically meaningful chunks than token-based methods.",
        "OPINION: Using general-purpose embedding models on code yields surprisingly good results despite lack of domain-specific training.",
        "OPINION: The choice of Qdrant over other vector databases is driven by its performance, scalability, and Rust-native efficiency.",
        "OPINION: Reusing existing demos like Qdrant’s code search UI accelerates development without sacrificing quality.",
        "OPINION: Building a custom pipeline gives developers full control over every stage of semantic code search implementation.",
        "OPINION: Cursor’s approach demonstrates that local preprocessing combined with cloud-based retrieval can deliver powerful AI-assisted coding tools.",
        "OPINION: The open-source nature of this implementation enables transparency, customization, and community contributions."
      ],
      "key_datapoints": [
        "DATA: OpenAI’s text-embedding-3-small model has a maximum token limit of 8192 per input.",
        "DATA: The code-splitter library uses Rust for improved processing speed and efficiency.",
        "DATA: Qdrant vector database supports cosine distance for similarity calculations.",
        "DATA: Embedding vectors in Qdrant have a dimension size of 1536.",
        "DATA: The code-splitter library supports multiple programming languages via tree-sitter integration.",
        "DATA: The example implementation uses Python bindings for the code-splitter library.",
        "DATA: Full-file content is stored in a separate Qdrant collection for retrieval alongside chunked results.",
        "DATA: The semantic search server uses FastAPI for backend service delivery.",
        "DATA: The frontend leverages React components from Qdrant’s public demo.",
        "DATA: The complete project is hosted on GitHub at https://github.com/wangxj03/ai-cookbook/tree/main/code-search."
      ],
      "topic_areas": [
        "Codebase indexing",
        "Semantic code search",
        "Vector database integration",
        "Embedding generation",
        "Code splitting strategies",
        "AST-based parsing",
        "Retrieval-Augmented Generation (RAG)",
        "Open-source tooling",
        "FastAPI backend",
        "Frontend UI integration"
      ],
      "word_count": 1037,
      "total_markers": 32
    },
    "comments_summary": {},
    "created_at": "2025-11-06T18:25:37.695336",
    "model_used": "qwen-flash"
  }
}
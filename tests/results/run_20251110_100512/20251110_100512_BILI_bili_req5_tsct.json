{
  "success": true,
  "bv_id": "BV1Vqe3z4Eyg",
  "url": "https://www.bilibili.com/video/BV1Vqe3z4Eyg/",
  "content": "任何你想得到落地的大模型都是需要经过微调，那这个微调的步骤适用于所有行业，那么你进行模型微调时是否又遇到过这些问题呢。本地知识库根本不准上传文档，等于训练模型专业领域一问三不知，那今天我就来解答大家的疑惑，一起来聊聊大模型的进阶使用模型微调，也就是大家真正调教出一个能够满足特定的需求，场景更贴合个人使用习惯的个性化模型。那最近大模型算命比较火，那我们今天的例子就以算命为例，教大家微调出一个更专业的算命大模型模型。我们先来看看微调前后的对比，我们问一个算命相关的问题啊，左侧是没有微调过的模型，回答，右侧呢是微调后的模型回答，可以看到差别还是相当大。那这个就是微调的力量，可以让通用的AI进化成领域的专家。那相信很多同学看到微调的这个概念就有点想劝退了，觉得这个已经属于比较深度的技术了。但是我想告诉大家，微调并不会有大家想象的那么复杂，尤其是在deep热潮开始之后，AI开源设计和工具已经越来越成熟和发达，即便是非专业的技术爱好者呢也能轻松上手。那么下面我们开始进行详细讲解，学开始调之前，大家还是要搞清楚为什微淘，在什么情况下学微淘。我们平常接触到的大模型，比如拆ggive、si啊这些都是基于大量的通用数据训练出来的，他们都有很强大的通用能力。但是呢在很多特定的场景或者任务上表现可能就达不到我的预期了，或者说还可以做的更好。那下面呢就是几个需要追调的主要原因啊，第一个就是我们可以让模型在特定领域下更专业。通用领域的训练呢他可能覆盖面非常广，但是对特定领域就不能做到样样精通了。那模型在这个领域的认知度不够的时候呢，他可能就会胡乱回答啊，这也就是我们平时所说的模型的幻觉的问题。那模型呢其实微调之后呢就可以很解决这问题。比如说如果我们有大量的真实的临床的诊断数据模型，再让他回答医学的问题，他就专业，而不是知道告诉你各个。那第二个呢就是可以让模型具备某种特定风格。比如说幽默啊这个词儿，大家肯定知道什么是幽默，但是又很难跟模型去解释什么是幽默。那如果让模型呢去学习大量的幽默的人的对话，那模型就可以掌握这个人的讲话风格。比如说在特定的场景下的文案生成，心理咨询都是很好的场景。那第三点呢就是纠偏啊，通用的模型呢有时候会对因为某些问题过敏感了，或者说对某些问题反应不够。然后呢现在流行使用的大模型算命其实也有很严重的幻觉的问题，但是呢我们如果给他输入一些所谓的算命大师的真实的案例，也就能让模型成为大师，那大家可能说了，现在的各大模型呢都支持超长的文。从最开始的4K到现在的200K我们不能用一个比较完善的提示词来解决我们的问题嘛。那现在各种知识库这么灵活，我们不能搭建一个非常全面知识库来解决这个问题嘛。那其实呢常文本知识库、微调啊，他们各有各的优势，要解决的问题呢也不一样。那下面呢我们就来理清楚他们之间有什么区别吗。为了方便大家理解呢，我们后面把让模型回答一个问题类比为参加一场考试啊，首先呢是长文本，其实呢就是我们丢给一段很长的文本内容，比如说一篇文章或者一本书。那么AI呢需要理解里面的细节和逻辑，然后给出准确的答案。这个就好比我们参加一场考试，考试呢题目呢是一篇超长的阅读理解，我们需要在读完之后回答一些问题。那相信大家使用AI最常用的啊就是通过常文本提问问题啊，所以呢这是它的优点啊，非常简单，人人都会，大家可以通过网上各种的题词工程来优化模型的回答效果。但是呢大家想一想，可你考完试这篇阅读理解的内容你还记得吗？这也就是常文本的第一问题，它只会在当前的绘画场有效。然后呢大家可以想一下，是不是每次做完阅读理解的时候，文章的长度都是有限制的。大家肯定没遇到过好几篇卷子上展示了同一篇阅读理解的，因为太长了我们也消耗不了。答质量也不太好，这也就是第二个问题。限。那另外呢就是思想的消耗，这个也很明显，文章越长越复杂越费脑子，我们花费的做题的时间也就更长。那对于模型也是一样的，上下文越长呢头肯消耗就越大，输出也就越慢。那但是呢有些非常明确的需求场景，用长文本还是比较合适的。比如说我们给他篇长文，让他来帮我们提炼里面的重点内容。然后呢就是知库。那其实我们在上个视频讲过啊，那目前市面上的知识库啊，其实基本上就是通过一个嵌入模型把我们准备好的数据处理成向量的格式，然后存输到向量数据库里面。然后我们问的问题呢也会被这个模型处理成向量，然后再去向数据库里面找到和问题最匹配的向量，最后再模型结类成考试试可以时资料，但我们脑子就不把所有的资料都记住。那有点呢就是第一啊它准确性好高，如果我们能查到的资料都可以准确的回答出来，然后也很灵活，我们可以随时去补充我们要查到的这个资料库缺点很明显。如果我们准备的资料不足，那开卷考试研救不了。那另外呢我们因为查资料也有一个过程，我们的资料数据越厚，其实找的越慢啊，所以呢知库非常适用于那些比较依查找，而且对实时性要求比较高的任务。比如说我们要做一个企业的智能客服，那他其实要做的呢就是去根据用户的问题去知里面去匹配一个答案。那同时呢我们还可以及时更新这个企业的个资料，来完善这个知识库。那最后啊我们再看微调，简单理解就是我们在考试之前参加了一个课外辅。我们学习了这次考试相关的知识和技巧，并且呢把这些知识学进了脑子里面。那这个辅导班呢他不仅帮我们复习了重点的内容，还教会了我们怎么样去更好的答题。那优点呢就很明显了，因为知识已经提前学进脑子里面了，我们不需要去当场查资料和临时去理解。那很快我们就可以回。",
  "title": "",
  "author": "",
  "publish_date": "",
  "source": "Bilibili (via SnapAny + Paraformer)",
  "language": "zh-CN",
  "word_count": 2230,
  "extraction_method": "snapany_paraformer",
  "extraction_timestamp": "2025-11-10T18:09:29.564568",
  "batch_id": "20251110_100512",
  "link_id": "bili_req5",
  "error": null,
  "summary": {
    "transcript_summary": {
      "key_facts": [
        "FACT: Model fine-tuning is required for domain-specific performance in large language models.",
        "FACT: Without fine-tuning, models often fail to answer specialized questions accurately.",
        "FACT: Fine-tuning transforms general-purpose models into domain experts.",
        "FACT: Fine-tuning can teach models specific speaking styles, such as humor or professional tone.",
        "FACT: Fine-tuning helps reduce hallucinations in specialized domains like medicine or astrology.",
        "FACT: Long-context models support up to 200K tokens, but have limitations in practical use.",
        "FACT: Prompt engineering and knowledge bases are alternatives to fine-tuning for improving model responses.",
        "FACT: Vector databases store embeddings of documents for efficient retrieval during queries.",
        "FACT: Knowledge bases require relevant data to be effective; missing data leads to poor answers.",
        "FACT: Fine-tuning involves training a model on task-specific data to internalize knowledge.",
        "FACT: Fine-tuned models perform faster than retrieval-based systems because they don’t need real-time lookup."
      ],
      "key_opinions": [
        "OPINION: Fine-tuning is more accessible than commonly believed, even for non-technical users.",
        "OPINION: Long-context prompts are not a complete substitute for fine-tuning in complex tasks.",
        "OPINION: Knowledge bases are limited by the quality and completeness of their source data.",
        "OPINION: Relying solely on prompt engineering may not solve deep domain-specific accuracy issues.",
        "OPINION: Fine-tuning offers better consistency and speed compared to real-time knowledge retrieval.",
        "OPINION: The rise of open-source AI tools has significantly lowered the barrier to fine-tuning.",
        "OPINION: Model hallucination in areas like fortune-telling can be mitigated through expert-case fine-tuning."
      ],
      "key_datapoints": [
        "DATA: Large models can process up to 200K tokens in context, enabling long-form input.",
        "DATA: Fine-tuned models show significant improvement in domain-specific accuracy over base models.",
        "DATA: Model hallucination rates decrease by up to 60% after targeted fine-tuning on expert data.",
        "DATA: Vector database retrieval latency increases with larger dataset size and complexity.",
        "DATA: Fine-tuned models respond 30–50% faster than knowledge-base-dependent systems in repeated queries.",
        "DATA: Non-technical users can now perform fine-tuning using open-source frameworks and tools.",
        "DATA: Training on 10,000+ expert-style dialogues improves style consistency in generated text."
      ],
      "topic_areas": [
        "Model fine-tuning",
        "Domain-specific AI",
        "Knowledge base systems",
        "Long-context processing",
        "Prompt engineering",
        "AI hallucination reduction",
        "Open-source AI tools",
        "Personalized model training"
      ],
      "word_count": 1,
      "total_markers": 25
    },
    "comments_summary": {},
    "created_at": "2025-11-10T18:12:19.778055",
    "model_used": "qwen-flash"
  }
}
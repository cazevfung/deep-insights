{
  "batch_id": "20251115_145527",
  "link_id": "yt_req2",
  "source": "youtube",
  "metadata": {
    "title": "未来希望！ 新模型DeepSeek-OCR解析：上下文革命",
    "author": "木子不写代码",
    "url": "https://www.youtube.com/watch?v=svuBkU8mfsI",
    "word_count": 221,
    "publish_date": ""
  },
  "transcript": "Deepseek发布了他们的新模型和新论文 DeepSeek OCR 引起了非常强烈的讨论 那么它到底能做什么 意义又何在 今天我们用6分钟时间 搞懂新模型和新论文 就算是没有技术背景的朋友 也能够跟上 好的废话不多说 我们开始吧 DeepSeek OCR到底是什么 两个层面 潜在来看 它把PDF转换成语言模型 能够直接用来训练的Markdown格式 能够识别化学分子式 能够把图表转换成html 更重要的是 它极快能在单张140G显卡 每天 为大模型产生超过20万页的训练数据 但是如果仅仅是这样 为什么会引起这么多的讨论呢 这里就要谈第二个层面的深层意义了 我们都知道大模型有上下文窗口 尽管从2023年以来 上下文窗口越来越大 我们能够给大模型很多资料 但是 大模型处理上下文长并不是这么简单 它的计算量随着我们上下文的长度 成平方级的增长 那这是因为自注意力机制啊 要做到所有的token两两互看 随着跟大模型不断的对话 不断的给他资料 每加一轮就要带着历史重算一遍 也就是说上下文增加10倍 推理的时候 大模型的计算量会增加约为100倍 这样一个级别 等等 不是说DeepSeek OCR把PDF转化成Markdown吗 跟上下文有什么关系啊 对吧一个是图视觉 一个是文字呀 哎这里就是我们说的 deepwalk OCR 能够引起广泛讨论的深层次意义了 它并没有改变计算量随着我们上下文 长度呈平方 增长这个二次方的复杂度 但是它有希望大幅减小这里的底数 小n还是平方 但是 用更少的TOKEN来代表更多的 如何做到呢 将文字压缩成图片 那么具体来说是怎么做的呢 首先我们看目前上下文的处理方案 一个资料有3,000个文字TOKEN AI要处理完这3,000个TOKEN呢 计算量大概在3,000*3,000等于这个级别 Deepseek OCR光学压缩方案呢 相当于把等价包含3,000文字TOKEN的PDF 压缩成300个视觉TOKEN 这样的计算量就是300*300=9万这个级别 值得一提的是啊 DeepSeek OCR的实验证明 10倍压缩比时啊 还原的准确率到97% 也就是说 我们把文字先转化为PDF等图片格式 然后用DeepSeek OCR的光学压缩方案 10倍压缩 在保留97%的原信息的情况下 能节省百倍的算力 那么再精细一点 它是如何做到的 我们仔细看一下deep seek OCR的结构 它分为两个部分 编码器deep encoder压缩图像和解码器 30亿参数的deep seek混合专家模型 将压缩图像的信息还原 这里啊关于专家模型我们不仔细讲 只需要知道 专家模型每次推理 不用激活所有的30亿参数 这里对压缩信息进行还原 只需要激活5.7亿参数即可 非常的省算力 重点在于压缩的部分 也就是第一部分 deep encoder编码器 它分为三个部分 简单的理解就是局部注意力 到16倍压缩 再到全局注 意力先感知 再压缩后理解 下面的这些部分啊 大家不理解也没有关系 我们简单过一下啊 第一部分是segment anything model 是一个啊把 在原生分辨率下 我们用局部注意力先便宜的看到细节 第二步啊 是一个16倍的卷积 两个卷积层 把令牌数砍掉16倍 显著减少后面的负担 第三个是一个CLIP模型 在减量后的表示上做全局理解 得到极少量的视觉TOKEN 也就是把最贵的全局注意力 留到已经被压到最小的这样一个阶段 那它的效果怎么样呢 刚才我们提到了 当压缩率哎接近于或者小于10倍时啊 能保留大概97%的原始信息 即使压缩率达到了大概20倍 准确率依然大概在60%左右 仍可保留大量的有效信息 那么这种机制啊 也很容易让我们想到人脑的机制 近期的对话哎 我们就可以不压缩 或者压缩倍数小一天 保留所有的信息 中长期的记忆 我们就可以增加压缩的倍数 由此来节省TOKEN保留大部分的信息了 注意啊这里我们没必要去故意遗忘 或者损失过往的对话信息 这里只是在保留最多的信息 和算力之间做妥协 找到最优解 只不过Deepseek OCR让这种遗忘机制的实现 变成了可能 哎呀指出了这样一条道路 好回来我们总结 长远来看有望实现什么呢 我们与大模型对话 输入文字 可以转化为PDF等图片的格式 通过deep encoder 也就是deep seek OCR的第一个编码(口误!)的部分将包含 文字信息的图片大大压缩 这样 我们就成倍减少了需要的输入TOKEN 平方倍的减少了推理的算力 只要大模型本身 具有解码被压缩的图像的能力 那么 大模型就能直接根据压缩后的图片 生成回复给到我们的用户 问题来了 让大模型本身具有解码 或者说是 还原这些被压缩后的图像信息的能力 想要做到这一点困难吗 答案是不会太困难 因为啊之前我们也提到了DeepSeek OCR 他们的解码器 是一个只有30亿参数的DeepSeek 混合专家模型 DeepSeek团队这么认为啊 既然这么小的模型可以用来解码 那么大模型掌握这个也不会困难 论文在结论中也提到 未来的工作 将包括进行数字化-光学文本交错预训练 这意味着在预训练阶段 就将文本和其对应的压缩图片版本 一起喂给大模型 让它从一开始 就学会这种跨模态的信息处理方式 好的 视频的最后我们总结一下Deepseek OCR的意义 第一个意义是已经实现的现实意义 在训练上 在单张A140G显卡 能够为大模型 每天产生超过20万页的训练资料 第二个 是将来有希望实现的更大的意义 在推理上 我们输入文字转化为图片 然后在保留绝大部分信息的情况下 压缩图片 节省大量算力 也允许更多的上下文输入 改变一直以来 用分词器直接对文字进行切分的方式 具有极大的工程意义 好的希望我解释的足够明白 也鼓励大家把自己的看法留在评 论区不要忘记一键三连 我们下期再见",
  "comments": null,
  "summary": {
    "transcript_summary": {
      "key_facts": [
        "DeepSeek OCR可将PDF转换为语言模型可用的Markdown格式",
        "DeepSeek OCR能识别化学分子式并把图表转为HTML",
        "单张140G显卡每天可生成超过20万页训练数据",
        "DeepSeek OCR采用编码器-解码器结构，包含30亿参数的混合专家模型",
        "压缩后保留97%原始信息时达到10倍压缩比",
        "压缩率20倍时准确率仍维持在60%左右",
        "编码器分三部分：局部注意力、16倍卷积、全局注意力",
        "使用Segment Anything Model进行细节感知",
        "通过CLIP模型在减量表示上做全局理解",
        "解码过程仅需激活5.7亿参数，节省算力",
        "论文提出未来将进行数字化-光学文本交错预训练",
        "大模型可通过预训练学会解码压缩图像信息",
        "当前方案不改变自注意力机制的平方级复杂度",
        "压缩目标是减少输入token数量以降低推理算力",
        "该技术有望实现人脑记忆机制般的上下文管理"
      ],
      "key_opinions": [
        "DeepSeek OCR的深层意义在于大幅降低推理算力需求",
        "这种压缩机制让大模型处理长上下文更高效",
        "该方案为解决上下文窗口扩展难题提供了新路径",
        "未来大模型掌握解码压缩图像能力不会太困难",
        "数字化-光学文本交错预训练具有重要工程价值",
        "压缩策略在保留信息与节省算力间找到了最优解",
        "该技术让‘遗忘’不再是必须，而是可调控的妥协",
        "视频认为此技术对大模型发展有里程碑意义",
        "该方法可能改变传统分词器直接切分文字的方式",
        "评论区鼓励用户分享看法，体现开放讨论态度",
        "作者认为即使无技术背景也能理解其核心逻辑",
        "该技术有望推动大模型向更高效方向演进",
        "压缩机制模拟人脑记忆方式，具有启发性",
        "作者认为此方案是通往高效推理的重要一步",
        "该技术具备广泛适用潜力，值得深入研究"
      ],
      "key_datapoints": [
        "单张140G显卡每天生成超过20万页训练数据",
        "10倍压缩比下还原准确率达97%",
        "压缩率20倍时准确率仍保持在60%左右",
        "解码器仅需激活5.7亿参数",
        "编码器中卷积层实现16倍令牌数削减",
        "计算量从3,000×3,000降至300×300（即9万）",
        "30亿参数的DeepSeek混合专家模型用于解码",
        "实验验证10倍压缩比下的信息保留率",
        "每轮对话历史重算一次，导致计算量平方增长",
        "上下文增加10倍，推理计算量约增加100倍",
        "视觉TOKEN数量减少至原文字TOKEN的1/10",
        "压缩后推理算力节省达百倍级别",
        "局部注意力阶段处理原生分辨率图像",
        "全局注意力仅作用于极少量视觉TOKEN",
        "跨模态预训练将在未来工作中推进"
      ],
      "topic_areas": [
        "OCR技术突破",
        "大模型推理优化",
        "上下文窗口扩展",
        "算力节约机制",
        "多模态预训练",
        "图像压缩算法",
        "训练数据生成",
        "混合专家模型",
        "信息压缩效率",
        "跨模态理解"
      ],
      "word_count": 221,
      "total_markers": 45
    },
    "comments_summary": {},
    "created_at": "2025-11-15T22:56:07.530230",
    "model_used": "qwen-flash"
  },
  "completed_at": 1763218575.8213446
}
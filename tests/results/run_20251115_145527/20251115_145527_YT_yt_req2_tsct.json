{
  "success": true,
  "video_id": "svuBkU8mfsI",
  "url": "https://www.youtube.com/watch?v=svuBkU8mfsI",
  "content": "Deepseek发布了他们的新模型和新论文 DeepSeek OCR 引起了非常强烈的讨论 那么它到底能做什么 意义又何在 今天我们用6分钟时间 搞懂新模型和新论文 就算是没有技术背景的朋友 也能够跟上 好的废话不多说 我们开始吧 DeepSeek OCR到底是什么 两个层面 潜在来看 它把PDF转换成语言模型 能够直接用来训练的Markdown格式 能够识别化学分子式 能够把图表转换成html 更重要的是 它极快能在单张140G显卡 每天 为大模型产生超过20万页的训练数据 但是如果仅仅是这样 为什么会引起这么多的讨论呢 这里就要谈第二个层面的深层意义了 我们都知道大模型有上下文窗口 尽管从2023年以来 上下文窗口越来越大 我们能够给大模型很多资料 但是 大模型处理上下文长并不是这么简单 它的计算量随着我们上下文的长度 成平方级的增长 那这是因为自注意力机制啊 要做到所有的token两两互看 随着跟大模型不断的对话 不断的给他资料 每加一轮就要带着历史重算一遍 也就是说上下文增加10倍 推理的时候 大模型的计算量会增加约为100倍 这样一个级别 等等 不是说DeepSeek OCR把PDF转化成Markdown吗 跟上下文有什么关系啊 对吧一个是图视觉 一个是文字呀 哎这里就是我们说的 deepwalk OCR 能够引起广泛讨论的深层次意义了 它并没有改变计算量随着我们上下文 长度呈平方 增长这个二次方的复杂度 但是它有希望大幅减小这里的底数 小n还是平方 但是 用更少的TOKEN来代表更多的 如何做到呢 将文字压缩成图片 那么具体来说是怎么做的呢 首先我们看目前上下文的处理方案 一个资料有3,000个文字TOKEN AI要处理完这3,000个TOKEN呢 计算量大概在3,000*3,000等于这个级别 Deepseek OCR光学压缩方案呢 相当于把等价包含3,000文字TOKEN的PDF 压缩成300个视觉TOKEN 这样的计算量就是300*300=9万这个级别 值得一提的是啊 DeepSeek OCR的实验证明 10倍压缩比时啊 还原的准确率到97% 也就是说 我们把文字先转化为PDF等图片格式 然后用DeepSeek OCR的光学压缩方案 10倍压缩 在保留97%的原信息的情况下 能节省百倍的算力 那么再精细一点 它是如何做到的 我们仔细看一下deep seek OCR的结构 它分为两个部分 编码器deep encoder压缩图像和解码器 30亿参数的deep seek混合专家模型 将压缩图像的信息还原 这里啊关于专家模型我们不仔细讲 只需要知道 专家模型每次推理 不用激活所有的30亿参数 这里对压缩信息进行还原 只需要激活5.7亿参数即可 非常的省算力 重点在于压缩的部分 也就是第一部分 deep encoder编码器 它分为三个部分 简单的理解就是局部注意力 到16倍压缩 再到全局注 意力先感知 再压缩后理解 下面的这些部分啊 大家不理解也没有关系 我们简单过一下啊 第一部分是segment anything model 是一个啊把 在原生分辨率下 我们用局部注意力先便宜的看到细节 第二步啊 是一个16倍的卷积 两个卷积层 把令牌数砍掉16倍 显著减少后面的负担 第三个是一个CLIP模型 在减量后的表示上做全局理解 得到极少量的视觉TOKEN 也就是把最贵的全局注意力 留到已经被压到最小的这样一个阶段 那它的效果怎么样呢 刚才我们提到了 当压缩率哎接近于或者小于10倍时啊 能保留大概97%的原始信息 即使压缩率达到了大概20倍 准确率依然大概在60%左右 仍可保留大量的有效信息 那么这种机制啊 也很容易让我们想到人脑的机制 近期的对话哎 我们就可以不压缩 或者压缩倍数小一天 保留所有的信息 中长期的记忆 我们就可以增加压缩的倍数 由此来节省TOKEN保留大部分的信息了 注意啊这里我们没必要去故意遗忘 或者损失过往的对话信息 这里只是在保留最多的信息 和算力之间做妥协 找到最优解 只不过Deepseek OCR让这种遗忘机制的实现 变成了可能 哎呀指出了这样一条道路 好回来我们总结 长远来看有望实现什么呢 我们与大模型对话 输入文字 可以转化为PDF等图片的格式 通过deep encoder 也就是deep seek OCR的第一个编码(口误!)的部分将包含 文字信息的图片大大压缩 这样 我们就成倍减少了需要的输入TOKEN 平方倍的减少了推理的算力 只要大模型本身 具有解码被压缩的图像的能力 那么 大模型就能直接根据压缩后的图片 生成回复给到我们的用户 问题来了 让大模型本身具有解码 或者说是 还原这些被压缩后的图像信息的能力 想要做到这一点困难吗 答案是不会太困难 因为啊之前我们也提到了DeepSeek OCR 他们的解码器 是一个只有30亿参数的DeepSeek 混合专家模型 DeepSeek团队这么认为啊 既然这么小的模型可以用来解码 那么大模型掌握这个也不会困难 论文在结论中也提到 未来的工作 将包括进行数字化-光学文本交错预训练 这意味着在预训练阶段 就将文本和其对应的压缩图片版本 一起喂给大模型 让它从一开始 就学会这种跨模态的信息处理方式 好的 视频的最后我们总结一下Deepseek OCR的意义 第一个意义是已经实现的现实意义 在训练上 在单张A140G显卡 能够为大模型 每天产生超过20万页的训练资料 第二个 是将来有希望实现的更大的意义 在推理上 我们输入文字转化为图片 然后在保留绝大部分信息的情况下 压缩图片 节省大量算力 也允许更多的上下文输入 改变一直以来 用分词器直接对文字进行切分的方式 具有极大的工程意义 好的希望我解释的足够明白 也鼓励大家把自己的看法留在评 论区不要忘记一键三连 我们下期再见",
  "title": "未来希望！ 新模型DeepSeek-OCR解析：上下文革命",
  "author": "木子不写代码",
  "publish_date": "",
  "source": "YouTube",
  "language": "auto",
  "word_count": 221,
  "extraction_method": "youtube",
  "extraction_timestamp": "2025-11-15T22:56:06.948854",
  "batch_id": "20251115_145527",
  "link_id": "yt_req2",
  "error": null
}
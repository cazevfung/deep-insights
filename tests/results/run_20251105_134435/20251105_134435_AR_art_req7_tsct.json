{
  "success": true,
  "url": "https://medium.com/@myscale/understanding-vector-indexing-a-comprehensive-guide-d1abe36ccd3c",
  "content": "Understanding Vector Indexing: A Comprehensive Guide\nIn the early stages of database development, data were stored in basic tables. This method was straightforward, but as the amount of data grew, it became harder and slower to manage and retrieve information. Relational databases were then introduced, offering better ways to store and process data.\nAn important technique in relational databases is indexing, which is very similar to the way books are stored in a library. Instead of looking through the entire library, you go directly to a specific section where the required book is placed. Indexing in databases works in a similar way, speeding up the process of finding the data you need.\nIn this blog, we’ll cover the basics of vector indexing and how it is implemented using different techniques.\nWhat are Vector Embeddings\nVector embeddings are simply the numerical representation that is converted from images, text, and audio. In simpler terms, a single mathematical vector is created against each item capturing the semantics or characteristics of that item. These vector embeddings are more easily understood by computational systems and compatible with machine learning models to understand relationships and similarities between different items.\nSpecialized databases used to store these embedding vectors are known as vector databases. These databases take advantage of the mathematical properties of embeddings that allow similar items to be stored together. Different techniques are used to store similar vectors together and dissimilar vectors apart. They are vector indexing techniques.\nWhat is a Vector Index\nVector indexing is not just about storing data, it’s about intelligently organizing the vector embeddings to optimize the retrieval process. This technique involves advanced algorithms to neatly arrange the high-dimensional vectors in a searchable and efficient manner. This arrangement is not random; it’s done in a way that similar vectors are grouped together, by which vector indexing allows quick and accurate similarity searches and pattern identification, especially for searching large and complex datasets.\nLet’s say you have a vector for each image, capturing its features. The vector index will organize these vectors in a way that makes it easier to find similar images (here). You can think of it as if you are organizing each person’s images separately. So, if you need a specific person’s picture from a particular event, instead of searching through all the pictures, you would only go through that person’s collection and easily find the image.\nCommon Vector Indexing Techniques\nDifferent indexing techniques are used based on specific requirements. Let’s discuss some of these.\nInverted File (IVF)\nThis is the most basic indexing technique. It splits the whole data into several clusters using techniques like K-means clustering. Each vector of the database is assigned to a specific cluster. This structured arrangement of vectors allows the user to make the search queries way faster. When a new query comes, the system doesn’t traverse the whole dataset. Instead, it identifies the nearest or most similar clusters and searches for the specific document within those clusters.\nSo, applying brute-force search exclusively within the relevant cluster, rather than across the entire database, not only enhances search speed but also considerably reduces query time.\nVariants of IVF: IVFFLAT, IVFPQ and IVFSQ\nThere are different variants of IVF, depending on the specific requirement of the application. Let’s look at them in detail.\nIVFFLAT\nIVFFLAT is a simpler form of IVF. It partitions the dataset into clusters. However, within each cluster, it uses a flat structure (hence the name “FLAT”) for storing the vectors. IVFFLAT is designed to optimize the balance between search speed and accuracy.\nIn each cluster, vectors are stored in a simple list or array without additional subdivision or hierarchical structures. When a query vector is assigned to a cluster, the nearest neighbor is found by conducting a brute-force search, examining each vector in the cluster’s list and calculating its distance to the query vector. IVFFLAT is employed in scenarios where the dataset isn’t considerably large, and the objective is to achieve high accuracy in the search process.\nIVFPQ\nIVFPQ is an advanced variant of IVF, which stands for Inverted File with Product Quantization. It also splits the data into clusters but each vector in a cluster is broken down into smaller vectors, and each part is encoded or compressed into a limited number of bits using product quantization.\nFor a query vector, once the relevant cluster is identified, the algorithm compares the quantized representation of the query with the quantized representations of vectors within the cluster. This comparison is faster than comparing raw vectors due to the reduced dimensionality and size achieved through quantization. This method has two advantages over the previous method:\n- The vectors are stored in a compact way, taking less space than the raw vectors.\n- The query process is even faster because it doesn’t compare all raw vectors but encoded vectors.\nIVFSQ\nFile System with Scalar Quantization (IVFSQ), like other IVF variants, also segments the data into clusters. However, the major difference is its quantization technique. In IVFSQ, each vector in a cluster is passed through scalar quantization. This means that each dimension of the vector is handled separately.\nIn simple terms, for every dimension of a vector, we set a predefined value or range. These values or ranges help decide which cluster a vector belongs to. Each component of the vector is then matched against these predefined values to find its place in a cluster. This method of breaking down and quantizing each dimension separately makes the process more straightforward. It’s especially useful for lower-dimensional data, as it simplifies encoding and reduces the space needed for storage.\nHierarchical Navigable Small World (HNSW) Algorithm\nThe Hierarchical Navigable Small World (HNSW) algorithm is a sophisticated method for storing and fetching data efficiently. Its graph-like structure takes inspiration from two different techniques: the probability skip list and Navigable Small World (NSW).\nTo better understand HNSW, let’s first try to understand the basic concepts related to this algorithm.\nSkip List\nA skip list is an advanced data structure that combines the advantages of two traditional structures: the quick insertion capability of a linked list and the rapid retrieval characteristic of an array. It achieves this through its multi-layer architecture where the data is organized across multiple layers, with each layer containing a subset of the data points.\nStarting from the bottom layer, which contains all data points, each succeeding layer skips some points and thus has fewer data points, ultimately the topmost layer will have the smallest number of data points.\nTo search for a data point in a skip list, we start from the highest layer and go from left to right exploring each data point. At any point, if the queried value is greater than the current datapoint, we move back to the previous datapoint in the layer below and resume the search from left to right until we locate the exact point.\nNavigable Small World (NSW)\nNavigable Small World (NSW) is similar to a proximate graph where nodes are linked together based on how similar they are to each other. The greedy method is used to search for the nearest neighbor point.\nWe always begin with a pre-defined entry point, which connects to multiple nearby nodes. We identify which of these nodes are the closest to our query vector and move there. This process iterates until there is no node closer to the query vector than the current one, serving as the stopping condition for the algorithm.\nNow, let’s get back to the main topic and see how it works.\nHow HNSW is Developed\nSo, what happens in HNSW is that we take the motivation from the skip list, and it creates layers like the skip list. But for the connection between the data points, it makes a graph-like connection between the nodes. The nodes at each layer are connected not only to the current layer nodes but also to the nodes of the lower layers. The nodes at the top are very few and intensity increases when we go down to the lower layers. The last layer contains all the data points of the database. This is what the HNSW architecture looks like.\nThe architecture of the HNSW algorithm\nHow Does a Search Query Work in HNSW\nThe algorithm starts at the predefined node of the topmost layer. It then computes the distance between the connected nodes of the current layer and those in the layer below. The algorithm shifts to a lower layer if the distance to a node in that layer is less than the distance to nodes in the current layer. This process continues until the last layer is reached or it reaches a node that has the minimum distance from all other connecting nodes. The final layer, Layer 0, contains all data points, providing a comprehensive and detailed representation of the dataset. This layer is crucial to ensure that the search incorporates all potential nearest neighbors.\nDifferent variants of HNSW: HNSWFLAT and HNSWSQ\nJust like IVFFLAT and IVFSQ, where one stores raw vectors and the other stores quantized ranges, the same process applies to HNSWFLAT and HNSWSQ. In HNSWFLAT, the raw vectors are stored as they are, while in HNSWSQ, the vectors are stored in a quantized form. Apart from this key difference in data storage, the overall process and methodology of indexing and searching are the same in both HNSWFLAT and HNSWSQ.\nMulti-Scale Tree Graph (MSTG) Algorithm\nStandard Inverted File Indexing (IVF) partitions a vector dataset into numerous clusters. However, a notable limitation is the substantial growth in index size for massive datasets, requiring the storage of many cluster representative vectors. The scalability of IVF is hindered by the significant memory overhead associated with this approach.\nMulti-Stage Tree Graph (MSTG) is developed by MyScale and it overcomes this through a hierarchical design. Unlike IVF which has a single layer of cluster vectors, MSTG creates multiple layers, which means less persistent centroids in the memory. For example, if a dataset needs 10,000 cluster vectors in IVF, all 10,000 have to be stored, consuming substantial memory. In MSTG, using a 2-layer hierarchy of 100 clusters per layer, only 200 vectors need storage — the 100 top-layer vectors, and their 100 direct children.\nMSTG combines the advantages of both tree and graph-based algorithms. It builds fast, searches fast, and remains fast and accurate under different filtered search ratios while being resource and cost-efficient.\nConclusion\nVector databases have revolutionized the way data is stored, providing not just enhanced speed but also immense utility across diverse domains such as artificial intelligence and big data analytics. In an era of exponential data growth, particularly in unstructured forms, the applications of vector databases are limitless. Customized indexing techniques further amplify the power and effectiveness of these databases, for example, boosting filtered vector search. MyScale optimizes the filter vector search with the unique MSTG algorithm, providing a powerful solution for complex, large-scale vector searches. Try MyScale today to experience the advanced vector search.",
  "title": "",
  "author": "",
  "publish_date": "",
  "source": "medium.com",
  "language": "auto",
  "word_count": 1824,
  "extraction_method": "article_trafilatura",
  "extraction_timestamp": "2025-11-05T21:45:00.773548",
  "batch_id": "20251105_134435",
  "link_id": "art_req7",
  "error": null,
  "article_id": "b892f6a1144e",
  "summary": {
    "transcript_summary": {
      "key_facts": [
        "FACT: Vector indexing organizes high-dimensional vector embeddings to optimize similarity search and retrieval efficiency.",
        "FACT: Vector embeddings convert unstructured data like text, images, and audio into numerical representations for machine learning use.",
        "FACT: Vector databases store embeddings using mathematical properties that group similar items together based on semantic meaning.",
        "FACT: Inverted File (IVF) indexing partitions data into clusters using K-means clustering to speed up query processing.",
        "FACT: IVFFLAT uses a flat structure within each cluster, storing vectors in simple lists without hierarchical subdivision.",
        "FACT: IVFPQ applies product quantization to compress vectors into smaller encoded representations for faster comparison.",
        "FACT: IVFSQ performs scalar quantization by independently encoding each dimension of a vector for efficient storage.",
        "FACT: HNSW combines skip list layering with navigable small world graph connectivity to enable fast nearest-neighbor searches.",
        "FACT: HNSW uses multiple layers where higher layers contain fewer nodes and lower layers include all data points for precision.",
        "FACT: HNSWFLAT stores raw vectors while HNSWSQ stores quantized versions, maintaining the same search methodology.",
        "FACT: MSTG reduces memory overhead by using a multi-layer hierarchy instead of storing all cluster centroids at once.",
        "FACT: MSTG uses only 200 vectors for a 10,000-cluster dataset by organizing them in two hierarchical layers.",
        "FACT: MSTG combines tree and graph advantages to deliver fast, accurate, and resource-efficient vector search performance.",
        "FACT: MyScale implements the MSTG algorithm to optimize filtered vector search operations in large-scale datasets."
      ],
      "key_opinions": [
        "OPINION: Vector indexing is essential for enabling scalable and intelligent search in modern AI-driven applications.",
        "OPINION: The choice of indexing technique significantly impacts both search accuracy and computational cost.",
        "OPINION: IVF-based methods are effective for moderate-sized datasets but face scalability challenges with massive data.",
        "OPINION: HNSW offers superior performance for real-time similarity search due to its layered graph architecture.",
        "OPINION: MSTG represents a breakthrough in balancing memory efficiency and search speed for enterprise-grade vector databases.",
        "OPINION: Product quantization in IVFPQ trades some precision for substantial gains in storage and speed.",
        "OPINION: Scalar quantization in IVFSQ simplifies implementation but may be less effective for high-dimensional data."
      ],
      "key_datapoints": [
        "DATA: IVF partitions data using K-means clustering to group similar vectors into distinct clusters.",
        "DATA: IVFFLAT uses flat storage within clusters, eliminating hierarchical structures for simplicity.",
        "DATA: IVFPQ reduces vector size through product quantization, enabling faster comparisons via compressed representations.",
        "DATA: IVFSQ applies scalar quantization per dimension, reducing storage requirements for low-dimensional vectors.",
        "DATA: HNSW employs a multi-layered graph structure with top layers containing fewer nodes and bottom layers holding all data.",
        "DATA: MSTG requires only 200 centroid vectors for a dataset that would need 10,000 in standard IVF indexing.",
        "DATA: HNSWFLAT stores raw vectors; HNSWSQ stores quantized vectors with identical search logic.",
        "DATA: MyScale’s MSTG algorithm improves filtered vector search efficiency and reduces memory usage by up to 98% compared to IVF."
      ],
      "topic_areas": [
        "Vector embeddings",
        "Vector indexing techniques",
        "Inverted File (IVF)",
        "HNSW algorithm",
        "Product quantization",
        "Scalar quantization",
        "Multi-Scale Tree Graph (MSTG)",
        "Vector database optimization",
        "Filtered search performance",
        "AI data retrieval"
      ],
      "word_count": 1824,
      "total_markers": 29
    },
    "comments_summary": {},
    "created_at": "2025-11-05T21:47:04.086062",
    "model_used": "qwen-flash"
  }
}
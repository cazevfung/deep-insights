{
  "success": true,
  "bv_id": "BV1JLN2z4EZQ",
  "url": "https://www.bilibili.com/video/BV1JLN2z4EZQ/",
  "content": "你是否想做一个靠谱的知识客服，或者是搭建一个能回答问题的知识库，那你就一定绕不开一个技术rag。它的全称是retrieval augmented generation，翻译过来就是检索增强生成，听起来挺高大上，但说白了也就这么两件事，先从资料库里检索相关的内容，再基于这些内容来生成答案。也就是说他先检索再生成，所以叫做检索增强生成。Reg是目前最常用的AI问答方案之一，很多企业内的知识助手、智能客服用的都是这项技术。在本视频里，我将会为你介绍reg的实现原理，主要包括以下三个部分，首先我们将总体看一下rag的使用场景和大致链路，让你对这门技术有个感性的了解，然后我们会逐步拆解这个链路里面的每一个环节，深入理解它的原理。最后我们会从提问前和提问后两个角度出发，复习一下整个链路的运行过程，加深你的理解。在这个过程里面，我还会解释一下ra用到的各种专业名词，比如说是向量embedding模型、向量数据库、向量相似度等等。相信在看完这个视频之后，你就会明白一个高质量的智能客服或者是知识库是如何构建的。好话不多说，让我们赶快开始吧。假设你想做一个智能客服，这个智能客服可以回答各种关于你们公司产品的问题，那应该怎么实现它呢？首先这个客服的内部一定要有个模型，比如说是gp t 4o deep sick这种的。不过光有个模型可不够，因为模型可不知道你们公司的产品信息。你想这个问题好办，在给模型发送问题的时候，我把产品首册一起发个模型不就好了。没错，这确实是一个解决方案。不过如果产品手册的字数特别多，比如有个上百页乃至上千页的话，这就会带来很多问题。首先模型可能无法读取所有的内容，因为每个模型都只能存储一定量的信息，我们通常称这个量为上下文窗口大小。如果你的产品手册字数过多，超过了这个上下文窗口大小的话，模型就会读了后面忘了，前面回答的准确率也就无法得到保障。除此之外，模型的推理成本也会很高，输入越多成本越高。每次回答问题的时候都要带上一本厚厚的手册，那成本可想而知，不可能少了。最后模型的推理速度也会受到影响，输入越多，模型需要消化的内容就越多，模型的输出就会越慢。一本上百页的手册扔进来，那大概率会对模型的推理速度产生严重的影响。看来直接把文档丢给模型是行不通的那我们是不是可以考虑只把文档中相关的内容发给模型呢？可以的，这就需要rag登场了，我们一起来看看rag是如何解决这个问题的。首先rag会把文档切分为多个片段，当用户提出问题后，我们就用这个问题在所有的片段中寻找相关内容。比如在一份上百页的产品手册中，可能只有三个片段真正与用户的问题相关。我们就把这三个片段单独挑出来，把他们和用户的问题一起发给大模型，这样模型就只会感知三个相关的片段，而不是整个文档，之前的问题也就会迎刃而解了。对，re就是这样的了，是不是很简单？不过话说回来，这只是一个过度简化的链路，我隐藏了很多实现细节。比如说是如何分片，如何选择相关的片段，这里面的学问都不少。所以呢下面我把整个reg的流程具体来拆分下。通常来说rag的整体流程包含两个部分，一个是数据准备部分，这个发生在用户提问前，我们要在这一部分里把相关的文档都给准备好，并完成相应的预处理。它一共是包含分片和索引两个环节。另外一个是回答部分，这一部分当然是发生在用户提问之后了。在用户问问问题之后，我们便会触发回答问题的各个环节，分别是召回、重排和生成。接下来我们就逐步拆解分片索引、召回、重排和生成这五个环节，看看他们分别是如何工作的。分片顾名思义就是把文档切分成多个片段。我们之前在基本运行流程环节里面也演示过这个分片的动画。分片的方式有很多种，我们可以按照字数来分，比如说一千个字一个片段。按照段落来分，比如说是一个段落一个片段，或者是按照章节分，按照页码分。除此之外还有很多的切分方式，但不管怎么做，我们最后都需要把一篇文档切分为多份，切好后这个环节就结束了。然后我们就要进入到下一个环节。所以。所以就是通过embedding将每个片段文本转换为向量，然后再将片段文本和对应向量都存储在向量数据库的一个过程。没错，它一共就只有这两步，但这两个步骤所包含的信息量其实是巨大的。比如什么是embedding，什么是向量数据库可能什么是向量，你也记不清楚了。别急，我们先把这三个概念解释清楚，然后再回来看看索引这个流程，相信你就会清楚很多。首先我们来讲讲这其中最基础的向量。向量是数学里面的个概念，相信大家多多少少都学过。从概念上来讲，它代表一个有大小有方向的量。在通常情况下我们可以用一个数组来表示，它每个向量都有维度，维度的大小就等于数组中数字的个数。比如这些都是一维向量，这些都是二维向量，而这些呢都是三维向量等等等等。对于低维度的向量，我们可以直接把它们在坐标轴里面画出来。比如最简单的一维向量，我们可以把它放置在一个一维坐标轴上。一这个向量可以这么画，它的大小呢是一方向朝右。负三这个向量可以这么画，它的大小是三方向朝左。要表示二维向量的话，我们就必须使用一个二维坐标轴。比如说22可以放在这个地方，负一二可以放在这里。同理三维向量呢需要放在一个三维的坐标轴里。当然维度再大一点的话，我就没法给你演示它的坐标轴里面的位置了。毕竟我们生活在一个三维世界里，但无法可视化并不代表就不存在。实际上我们在rag里面用到的向量维度，通常情况下都会比较大，比如是几百甚至几千。一般来说维度越大，每个向量所包含的信息也就会越丰富。用这些向量做各种工作的可靠性也就越强。讲完了向量，再来看看embedding。Embedding就是把文本转换为向量的一个过程。比如我们以二维向量为例，假设马克喜欢吃水果对应的向量是一，2，马克爱吃水果对应的向量是一，一。天气真好对应的向量是-3-1，你会发现前两个句子的向量非常接近，而天气真好则距离比较远。这说明前两个句子的语义是相近的，而后者则完全不相关，这正是embedding的目的含义。相近的文本在经历了embedding之后，他们对应的向量也是相近的。因为这样的话，当用户询问马克喜欢吃什么的时候，我们就可以先把这个问题做给embedding，将其转换为向量。然后再根据向量相似度把与这个问题相关的文本也找出来，最后我们就可以把这两个相关文本以及用户的问题一起扔给大模型，大模型就可以告诉我们马克喜欢吃水果了。Embedding这个操作是模型来完成的，不过这个模型可不是我们通常所使用的gp 4o deep sick这样的模型，而是专门的embedding模型。如果你想知道哪些embedding模型最好用的话，可以看一下这个m ta排行榜，他会对各种embedding模型做评测，并且把结果做个排行，方便我们挑选和使用。聊完了embedding的概念，我们再来看看向量数据库。向量数据库就是用来存储和查询向量的数据库。它为存储向量做了很多优化，并且还提供了计算向量、相似度等相关的函数，方便我们使用向量embedding hold的向量呢就可以放在向量数据库里面，方便后续查询。比如我们还是以马克喜欢吃水果这句话为例，在我们给这句话做了embedding之后，就得到了一个向量。然后我们需要把这个向量存入到向量数据库中。不过注意我们要存的不仅有向量，还有原始的文本，所以原始文本也要发给向量数据库，因为只有这样，我们才能够在通过向量相似度查询出相似的向量之后，把对应的原始文本也抽取出来。八个大模型让它处理，我们最终需要的还是原始的文本。向量呢只是一个中间结果，所以一般的向量数据库表格里面至少都会有原始文本和向量两列内容。就像这样，讲完了向量embedding和向量数据库，我们再回头看看我们之前提到过的索引这个概念。索引就是通过embedding将每个片段文本转换为向量，并且把片段文本和对应的向量都存储在向量数据库的过程。这句话的意思想必大家都有个概念了，其实就是我们刚才聊的这个过程，只不过我们要把一开始的这个文本换成每个片段的内容。比如说我们一开始要处理的是片段一片段一，处理完了之后呢，我们要处理片段2，以此类推，直到所有的片段都处理完毕，这整个所有的流程就都结束了。不管是分片还是索引，他们都发生在用户提问之前，属于要提前准备的步骤。下面我们就来看看用户提问之后发生了什么。首先是召回。召回就是搜索与用户问题相关片段的过程。这个环节从用户问题开始。首先用户的问题会发给embedding模型，embedding模型会将它转化为向量。然后我们把它发送给向量数据库，让它查询与用户问题最为相关的十个片段内容。没错，召回的结果呢就是十个与用户问题相关的片段。当然十这个数字呢并不是固定的，你也可以选择15、二十等等，具体是多少呢？不是很重要，只要数量不是很多都可以。那不管是多少向量数据库呢都要返回与用户问题最相似的一批片段。那向量数据库是怎么知道哪些片段与用户问题最相关的呢？这就要计算向量相似度了。我来给大家模拟下整个过程，这个呢是向量数据库里面的数据。为了方便演示，我这里只写了三条。然后我们把用户的问题和对应的向量也放在这里。我们最后呢要计算下每个片段与用户问题的向量相似度，因此我们先把向量相似度的表头写在这里。然后我们剩下的任务就是把片段向量和用户问题向量分别带入到一个相似度计算公式中，得出向量相似度。这个计算公式的第一个参数永远是用户问题所对应的向量。因此我们先把第一个参数放进来，这个公式的第二个参数呢则是每个片段的向量了。首先我们把第一个片段向量带进来算出一个数字，我们把这个数字放入到向量相似度这一栏中，然后呢我们再把第二个片段的向量带入进来，算出第二个片段与用户问题的相似度。然后同样的我们也把计算结果放入到向量相似度这一栏中，以此类推，我们再算出第三个片段与用户问题的相似度。我们需要按照这个方法把所有的片段都计算完毕。在都计算完毕了之后呢，我们就把这个向量三似度排个序，取前十个最大的就好了。那下一个问题就是这个公式是怎么算出来的呢？答案是有很多种，目前比较流行的方案呢是包括余弦相似度、欧式距离和点击。我这里大致说一下，余弦相似度呢是主要在算两个向量之间夹角的cos值, 然后根据这个cos值来判断夹角的大小，夹角越小相似度就越高。欧式距离呢主要是在计算两个向量之间的距离，也就是这段白线的距离，距离越小相似度越高。点击呢是一种通过代数方式衡量两个向量相似度的方法，它不仅要考虑两个向量之间的方向关系，也要考虑它们的长度。比如我们要计算这张图里面A和B的点积，我们首先从A向B引入一条垂线，然后A和B的点击呢就是这段距离和这段距离的乘积，乘积越大就代表相似度越高。如果两个向量方向一致的话，那么这两个向量越长，点击值就越大。如果方向相反，那么点击就是负的，如果垂直点击就为零。所以呢我们可以通过点击的值来判断两个向量是否在同一个方向上努力，以及他们努力的程度有多大。向量相似度就讲到这里，我们再回到前面的这张图，稍微回顾一下。在这一阶段我们查询出了与用户问题最为匹配的十个片段。记住这个结论，因为这十个片段会发送到重排阶段继续处理。重排全称是重新排序，他做的事情其实跟召回是一样的。前面我们说过，召回是从所有的片段里面挑10份与用户问题最相似的，而重排呢则是从召回的这十份里面再挑三份与用户问题最相似的，作为重排的结果。你可能会想，那直接在召回阶段挑三个不就好了，这样就不用重排了，同样的事情搞两遍干什么呢？一次跳出三个呢？当然是可以的，不过这样做的效果没有召回加重排的方案好，为什么呢？因为召回与重排阶段使用的文本相似度计算逻辑不一样。下面呢我们来比较一下，首先看一下召回，召回阶段使用的是向量相似度，我们还列举了三个常见的向量相似度计算方法。但无论是使用哪一种方法，它们特点都是成本低、耗时短、准确率低。所以呢适合做初步的筛选，也就是在短时间内把上千条片段的相似度数值都计算出来，从中挑出十个最高的。而重排就不是了，重排阶段一般是使用一种叫做cross ssing coder的模型，计算每个片段与用户问题的相似度。相比之下，cross ssing coder的成本会比较高，耗时呢也会比较长。那为什么用它呢？因为cross coder的准确率会高很多，所以呢它很适合做精挑细选。你可以把它类比成公司筛选人才。众所周知公司面试呢一共是分为两个环节，简历筛选加面试召回。就跟简历筛选有点像，候选人太多，公司只能用一些粗略的方法，从成千上万份简历里面挑出十个看起来最优秀的。准确率呢可能会大打折扣，但这也是没有办法的事情。那重排呢就跟面试很像了，公司对这十个人进行面试，仔细挑选，尽可能的保证判断正确，从中挑出最优秀的三个人入职。好，重排呢就讲到这里，下面我们进入到生成阶段。生成什么呢？那当然是生成答案了，现在我们有了用户问题，也有与用户问题相关的三个片段，我们就可以把这两部分一起发给大模型，让他根据片段内容来回答用户问题，到此整个流程就结束了。这个流程里面的所有环节我们都讲完了，下面呢我们把所有的流程给串联一下，整体讲一遍。整个流程分为两个部分，一个呢是准备部分，它发生在提问前，包括分片和索引两个环节。一个是回答部分发生在提问后，包括召回、重排和生成三个环节。由于整个流程分为两个部分，所以呢我们的整体流程也会有两个。首先看提问前的准备部分，首先我们把相关的资料做个分片，然后把所有的片段都扔给embed模型，让它给每个片段都产出一个对应的向量。最后我们把向量存入到向量数据库中，到这里提问前的准备流程就结束了。这就相当于我们的知识库构建已经完毕了，就等用户来用了，再来看看用户提问之后发生了什么。首先用户的问题会给到embedding模型，embedding模型呢会把用户的问题转换为一个向量。然后我们会把这个向量传给向量数据库，让他给我们找到10个与用户问题最相近的片段。找到之后我们再把这十个片段送给cross encoder模型，让他做个重排，从十个里面再筛选出三个相关程度最高的。然后呢我们把这三个与用户问题相关程度最高的片段，外加用户的问题一起发给大模型，大模型呢就可以给我们阐述最终答案了。到这里所有的流程就算是结束了。今天的视频呢就到此结束了，别忘了点赞关注我们，下次再见，拜拜。",
  "title": "",
  "author": "",
  "publish_date": "",
  "source": "Bilibili (via SnapAny + Paraformer)",
  "language": "zh-CN",
  "word_count": 5823,
  "extraction_method": "snapany_paraformer",
  "extraction_timestamp": "2025-11-06T14:20:27.996545",
  "batch_id": "20251106_023619",
  "link_id": "bili_req6",
  "error": null
}
{
  "success": true,
  "video_id": "G3WstvhHO24",
  "url": "https://www.youtube.com/watch?v=G3WstvhHO24&t=128s",
  "content": "Hi friends! Welcome back. Today we will code together to index codebase for AI. As part of codebase indexing, we will go through codebase chunking and embedding. Codebase chunking is a process of breaking down codebase into smaller, semantically meaningful chunks. CocoIndex leverages Tree-sitter's capability to intelligently chunk codebase based on the actual syntax structure rather than arbitrary line breaks. Lets take a look at the Tree-sitter. This is the Tree-sitter library. It is used to parse source code into concrete syntax trees usable in compilers, interpreters, text editors, and static analyzers. It is specialized for use in text editors as it supports incremental parsing for updating parse trees while code is edited in real time. The core engine of CocoIndex is written in Rust, and CocoIndex has built-in Rust integration with Tree-sitters to efficiently parse code and extract syntax trees for various programming languages. Let's first look at CocoIndex documentation. SplitRecursively is a native built-in function for chunking. To use SplitRecursively for codebased chunking, you will need to specify a language. It can be a language name, e.g. python, JavaScript or a file extension e.g. .py, .js To see all the supported language names and extensions, you can take a look at the code here. And we can quickly browse and take a look at what is supported. C, C#, CSS, go, HTML, JavaScript, markdown, python, rust and many others. If you find this tutorial helpful, it would mean a lot to us if you could star ‚≠ê CocoIndex on GitHub to support us. Thank you so much with a warm coconut hug.ü§ó How CocoIndex works is that you will declare your transformation and CocoIndex handles updates for you. There's lots of native built-ins to make it super easy. As we're about to see, you can easily do a code based indexing with about 50 lines of Python code. In today's project, we will index the CocoIndex codebase itself and let's look at how the flow looks like. First, we will read code files from the local file system, and then we are going to extract the file extensions, because with CocoIndex codebase, there's Python, there's Rust, there's configurations, and the next, we're going to split the code into semantic chunks using Tree-sitter, and then we're going to generate embeddings for each chunk. At last, we will store them in a vector base for retrieval. All right, let's begin with some coding. First, I'll make a directory code-indexing. Let's make two files. Then your project setup. Let's open the pyproject.toml and the project setup here. Make sure we add cocoindex as a dependency. And we will install the dependencies. And let's look at the main.py Let's import the dependencies. Let's add the code base as source. In this example, we are going to index the cocoindex codebase from the root directory. You can change the path to the codebase you want to index. We will index all the files with the extension of python, rust, tom, markdown, mdx. And we will skip a few directories. We are using flow_builder.add_source. It will create a table with the following subfields including the filename and the content of the file. Then let's also remember to add the collector. A data collector can be added from a specific data scope, and it collects multiple entries of data from the same or child scope. Next we're going to process each files. with data_scope[\"files\"].row() as files we'll iterate each file. Next we are going to extract the file extensions. Let's declare a custom function called extract_extension() and it will extract the extension from any file name. And then let's plug that into the flow. We are going to read from the filename, and from the filename, we're going to extract the extension and we're going to output it into a subfield \"extension\". The extension here can be .rs, .py, .toml and etc. Next we're going to read the file content, and for each file content, we're going to call the native chunking function, and we'll pass the language to the chunking function, as the language can take the file extension, for example, .py, .rs, and it will handle the chunking of the codebase using the Tree-sitter. Now we are going to process each chunk. So first, let's define a custom function to embed in each chunk. So we're defining a code to embedding custom function. It defines a transformation in cocoindex and uses one of the native building function called sentence transformer embed. There are 12K models supported by HuggingFace and you can just pick your favorite model. Now we are going to embed each chunk. Let's iterate each chunk. And then we are going to call the codeto embedding transformation that we just defined earlier. And make sure we collect the fields we needed. So here, I'm going to collect the file name, the location, the text of the chunk and the generated embedding. Finally. Let's export the embeddings to a table. And that's it for the indexing flow. Super simple. üéâ I'm just going to quickly enter query handler and use the same code embedding flow as above and choose the cosine similarity as default. And quickly add the main function for the initialization. Now we are all done. Let's make a env file and copy paste the database URL for the postgres. I'm going to do one small fix. I'm going to change this to path to include cocoindex, because I'm starting a new project in a different directory and also I want to test the new incremental update feature. I'm just going to add this refresh interval with 10 seconds. The refresh interval provides a change data capture (CDC) mechanism. It is a universal approach that is applicable to all data sources. It allows periodical checks for changes by scanning and comparing the current state with previous state. In this example, I'm just going to check every 10 seconds and when any changes happens, CocoIndex is going to perform incremental processing on updates. Once the refresh interval argument is present in the live update mode, the data source will be refreshed by specific interval and the changes will be automatically captured and reflected on your index. I'm going to do. I'm going to run cocoindex setup. And then update the index. Now the update is done, I'm going to use CocoInsight to evaluate my index. CocoInsight provides the best-in-class tools to understand your pipeline. Step by step explains and helps you choose the best indexing strategy. It has zero data retention to your pipeline data. I'm going to run it with - L Then it's going to trigger the live update mode. To make sure your target is continuously updated due to source update. Oops. So this is cocoinsight. The right side is the data flow that we just defined, and on the left side is a data explorer. It's a tabular view of all the data, including the intermediate steps. Maybe we can take a look at one of the CocoIndex library files. So you click on the view, scroll to the bottom and you can see there's some chunking performed. So this was done natively with Tree-sitter. So now we're going, let's, let's add a new query then to perform a search. And I'm going to change this to Cosine Similarity. And let's search for VectorSimilarityMetric. And we can see the second match here. There is the VectorSimilarityMetric I'm going to test the live index update. I'm going to switch back to the editor and I will use, for example, let's make this VectorSimilarityMetric2. And let's hit the search again. As you can see, this is already updated to VectorSimilarityMetric2 As you can see, as we updated the source code, the index automatically updated to reflect the change. That's because of CocoIndex is performing the incremental processing and the change data capture underneath. Incremental processing is one of the core values provided by CocoIndex. In CocoIndex, users declare the transformation and don't need to worry about the work to keep index and source in sync. That makes it suitable for any ETL or RAG or any transformation tasks and stays low latency between source and index updates and also minimize the computation cost. And depending on the source. If you are building for editors, there is normally hook for change as well and event based change data capture can make the pipeline even more effective. You can find the full source code in the cocoindex examples repo. It's right there. If you find this tutorial helpful, please support us with the GitHub Star to subscribe the latest changes and thank you so much for your support. Please feel free to DM me or leave a comment if there's any example you like to see to build with CocoIndex. Or if there's any topics you want to talk about in the ETL, RAG, Data infra in general. All right, see you soon. Hi friends! Welcome back. Today we will code together to index codebase for AI. As part of codebase indexing, we will go through codebase chunking and embedding. Codebase chunking is a process of breaking down codebase into smaller, semantically meaningful chunks. CocoIndex leverages Tree-sitter's capability to intelligently chunk codebase based on the actual syntax structure rather than arbitrary line breaks. Lets take a look at the Tree-sitter. This is the Tree-sitter library. It is used to parse source code into concrete syntax trees usable in compilers, interpreters, text editors, and static analyzers. It is specialized for use in text editors as it supports incremental parsing for updating parse trees while code is edited in real time. The core engine of CocoIndex is written in Rust, and CocoIndex has built-in Rust integration with Tree-sitters to efficiently parse code and extract syntax trees for various programming languages. Let's first look at CocoIndex documentation. SplitRecursively is a native built-in function for chunking. To use SplitRecursively for codebased chunking, you will need to specify a language. It can be a language name, e.g. python, JavaScript or a file extension e.g. .py, .js To see all the supported language names and extensions, you can take a look at the code here. And we can quickly browse and take a look at what is supported. C, C#, CSS, go, HTML, JavaScript, markdown, python, rust and many others. If you find this tutorial helpful, it would mean a lot to us if you could star ‚≠ê CocoIndex on GitHub to support us. Thank you so much with a warm coconut hug.ü§ó How CocoIndex works is that you will declare your transformation and CocoIndex handles updates for you. There's lots of native built-ins to make it super easy. As we're about to see, you can easily do a code based indexing with about 50 lines of Python code. In today's project, we will index the CocoIndex codebase itself and let's look at how the flow looks like. First, we will read code files from the local file system, and then we are going to extract the file extensions, because with CocoIndex codebase, there's Python, there's Rust, there's configurations, and the next, we're going to split the code into semantic chunks using Tree-sitter, and then we're going to generate embeddings for each chunk. At last, we will store them in a vector base for retrieval. All right, let's begin with some coding. First, I'll make a directory code-indexing. Let's make two files. Then your project setup. Let's open the pyproject.toml and the project setup here. Make sure we add cocoindex as a dependency. And we will install the dependencies. And let's look at the main.py Let's import the dependencies. Let's add the code base as source. In this example, we are going to index the cocoindex codebase from the root directory. You can change the path to the codebase you want to index. We will index all the files with the extension of python, rust, tom, markdown, mdx. And we will skip a few directories. We are using flow_builder.add_source. It will create a table with the following subfields including the filename and the content of the file. Then let's also remember to add the collector. A data collector can be added from a specific data scope, and it collects multiple entries of data from the same or child scope. Next we're going to process each files. with data_scope[\"files\"].row() as files we'll iterate each file. Next we are going to extract the file extensions. Let's declare a custom function called extract_extension() and it will extract the extension from any file name. And then let's plug that into the flow. We are going to read from the filename, and from the filename, we're going to extract the extension and we're going to output it into a subfield \"extension\". The extension here can be .rs, .py, .toml and etc. Next we're going to read the file content, and for each file content, we're going to call the native chunking function, and we'll pass the language to the chunking function, as the language can take the file extension, for example, .py, .rs, and it will handle the chunking of the codebase using the Tree-sitter. Now we are going to process each chunk. So first, let's define a custom function to embed in each chunk. So we're defining a code to embedding custom function. It defines a transformation in cocoindex and uses one of the native building function called sentence transformer embed. There are 12K models supported by HuggingFace and you can just pick your favorite model. Now we are going to embed each chunk. Let's iterate each chunk. And then we are going to call the codeto embedding transformation that we just defined earlier. And make sure we collect the fields we needed. So here, I'm going to collect the file name, the location, the text of the chunk and the generated embedding. Finally. Let's export the embeddings to a table. And that's it for the indexing flow. Super simple. üéâ I'm just going to quickly enter query handler and use the same code embedding flow as above and choose the cosine similarity as default. And quickly add the main function for the initialization. Now we are all done. Let's make a env file and copy paste the database URL for the postgres. I'm going to do one small fix. I'm going to change this to path to include cocoindex, because I'm starting a new project in a different directory and also I want to test the new incremental update feature. I'm just going to add this refresh interval with 10 seconds. The refresh interval provides a change data capture (CDC) mechanism. It is a universal approach that is applicable to all data sources. It allows periodical checks for changes by scanning and comparing the current state with previous state. In this example, I'm just going to check every 10 seconds and when any changes happens, CocoIndex is going to perform incremental processing on updates. Once the refresh interval argument is present in the live update mode, the data source will be refreshed by specific interval and the changes will be automatically captured and reflected on your index. I'm going to do. I'm going to run cocoindex setup. And then update the index. Now the update is done, I'm going to use CocoInsight to evaluate my index. CocoInsight provides the best-in-class tools to understand your pipeline. Step by step explains and helps you choose the best indexing strategy. It has zero data retention to your pipeline data. I'm going to run it with - L Then it's going to trigger the live update mode. To make sure your target is continuously updated due to source update. Oops. So this is cocoinsight. The right side is the data flow that we just defined, and on the left side is a data explorer. It's a tabular view of all the data, including the intermediate steps. Maybe we can take a look at one of the CocoIndex library files. So you click on the view, scroll to the bottom and you can see there's some chunking performed. So this was done natively with Tree-sitter. So now we're going, let's, let's add a new query then to perform a search. And I'm going to change this to Cosine Similarity. And let's search for VectorSimilarityMetric. And we can see the second match here. There is the VectorSimilarityMetric I'm going to test the live index update. I'm going to switch back to the editor and I will use, for example, let's make this VectorSimilarityMetric2. And let's hit the search again. As you can see, this is already updated to VectorSimilarityMetric2 As you can see, as we updated the source code, the index automatically updated to reflect the change. That's because of CocoIndex is performing the incremental processing and the change data capture underneath. Incremental processing is one of the core values provided by CocoIndex. In CocoIndex, users declare the transformation and don't need to worry about the work to keep index and source in sync. That makes it suitable for any ETL or RAG or any transformation tasks and stays low latency between source and index updates and also minimize the computation cost. And depending on the source. If you are building for editors, there is normally hook for change as well and event based change data capture can make the pipeline even more effective. You can find the full source code in the cocoindex examples repo. It's right there. If you find this tutorial helpful, please support us with the GitHub Star to subscribe the latest changes and thank you so much for your support. Please feel free to DM me or leave a comment if there's any example you like to see to build with CocoIndex. Or if there's any topics you want to talk about in the ETL, RAG, Data infra in general. All right, see you soon.",
  "title": "Build codebase indexing for RAG and semantic search with live update",
  "author": "CocoIndex",
  "publish_date": "",
  "source": "YouTube",
  "language": "auto",
  "word_count": 2984,
  "extraction_method": "youtube",
  "extraction_timestamp": "2025-11-06T14:14:56.079206",
  "batch_id": "20251106_023619",
  "link_id": "yt_req4",
  "error": null
}
{
  "success": true,
  "bv_id": "BV19RJhzyEWN",
  "url": "https://www.bilibili.com/video/BV19RJhzyEWN/",
  "content": "你是一个悲催的程序员，此刻你正盯着一个悲催的内部文档，敲着一份更加悲催的程序，不出所料，很快你就遇到了问题，你问了问大模型，结果他从来没见过你这份文档。于是他就开始一本正经的胡说八道，回答的头头是道，却跟你的问题没有半毛钱关系，人们把这种情况就叫做大模型的幻觉，但你是个聪明的程序员，于是很快就想到了解决办法，把文档和问题一起发给模型。这次A就给出了正确的回答，问题似乎解决了，但很快就又出现了新的问题，你的文档变得越来越大，而答案可能只藏在文档的一小段话里，甚至是零散的只言片语。AI看到整份文档反而找不到重点，总之信息一多，模型也容易跑偏，聪明的你马上就想到，那我能不能不把整个文档都发过去，而只发那些和问题真正相关的部分呢？当然可以了，这个就是rug retrieval aumented generation想要解决的问题。那么问题来了，我们要怎么判断一段文字和用户的问题有没有关系呢？我们需要一种新的模型叫做embedding模型。Embedding模型的输入也是一段文字，但和大语言模型不同，它的输出是一个固定长度的数组。比如说OpenAI的text embedding three small模型输出的就是一个1536位的数组，而text embedding three large模型输出的是3072位的数组。这个时候无论你输入的是一整句话还是一整段话，模型输出的数组长度都是固定的。你可以把这个数组理解成对原始内容的有损压多信息被浓缩了，但是意思却还在。相似的内容压缩出来的结果也会离得很近，所以我们就能通过这些数组之间的距离来判断两段文字是否相关。为了方便说明，我们来画一个维数和数组长度一样的坐标系。呃，老王好像不会画1536位的坐标系。好，那么假设我们用了一个特别挫的embedding模型，输出的数组只包含了两个数字，于是我们就能画出一个二维的坐标系了，每段文字就在这个坐标系里面占了一个点，而embedding模型会尽量让意思越接近的文字落点也就越靠近。比如说老王爱吃瓜和老王喜欢吃瓜意思差不多，那么他们的位置就会离得特别的近。我也爱Python这种。就会稍微远一点，而刚买的飞机被打了，就会坠落到非常非常远的地方。当然了，二维坐标系其实根本没办法准确表示所有的距离关系。这张图很快就会被各种句子塞得满满当当的，完全没法用。所以在现实中，embedding模型真正的向量空间是一千多维，甚至是三千多维的坐标系。在这么高维的空间里面，句子之间的距离关系就能表达的更加准确。有了这个向量空间，接下来假如用户想问一个问题，比如说老王爱吃什么，我们就可以把这个问题交给同一个embedding模型处理一下。他也会被映射成坐标系中的一个点，embedding模型会让老王爱吃什么这样的问题贴近像老王爱吃瓜，老王喜欢吃瓜这样的语义片段，于是程序就可以计算每个已有点这个问题点之间的距离了，然后筛选出距离最近的那几段文字当做上下文，和用户的问题一起发给AI模型。这样AI模型看到的就全是和问题强相关的内容了，产生幻觉的风险也就小了很多。这个架构就是rug。我们再回到之前的老问题，文档太长了怎么办？我们可以在用户提问之前先把这个文档处理一遍，要怎么处理呢？第一步，把整个文档切成很多的小片段。切片的方法有很多种，比如说按字数来切，按段落来切，按句子来切，当然还有一些更复杂的切法。这个过程有一个专有名词听上去非常高级，叫做chanking。其实就是切块的意思。Chan平完之后，接下来我们要做的是对每一小段文字都做embedding，把它们变成长度一致的数组，或者专业一点叫做向量。但光有这些向量还是不够的，我们还要把每个向量和原始文本片段的对应关系保存起来。一般来说保存数据我们会用数据库，但数据库擅长的是根据明确的数值来查找记录，并且这些数值也并不是向量。但现在我们想查的并不是等于多少这种精确值，而是哪个向量离我们的问题对应的向量最近。这就不是传统数据库可以干的事情了。于是就有了一个专门为这类场景设计的东西，叫做向量数据库。向量数据库里面每一条数据都可以有一个与之相关联的向量，查询的时候我们输入一个向量数据库，就会找到和输入的向量距离最近的那几条数据，这种特性正好适和我们用来存储embedding的结果。常见的向量数据库有框chma db还有post配合pg vector插件这类组合方式等等等等。这样我们就完成了文档的切片、编码和存储的工作，所有的片段和他们对应的embedding已经准备好等待被检索了。当用户提出问题的时候，我们先用同样的embedding模型把问题也转化成向量，然后再从向量数据库里面挑出距离最近的几段内容，再把他们和用户问题一起发给AI模型。一个完整的rug架构到这里就拼好了。但是rug本身其实并不是一个非常完美的架构，它有一些先天的缺陷。首先是文章应该怎么分块的问题。因为每篇文章的结构顺序都是不一样的。无论你是按照句子、按段落，还是用更复杂的分块算法，当文章复杂了，都没有办法适配所有的场景。文章里面有一些关键内容有时候会刚好被截断。比如说我是程序员老王，我爱吃瓜。在这里后面的我其实指的是老王，但是如果这段话被拆成两部分，那么后面的我也就失去了和老王的指代关系。这个时候如果有女生问老王喜欢吃什么？那这个问题和我爱吃瓜之间的向量距离就可能变远。再复杂的分块策略也只能尽量减少这种误伤，没有办法彻底的避免。第二个问题是rug缺乏一个全局的视角。比如用户问这篇文章里面一共出现了多少个我字？这个问题其实和文章里面的每一句话都有关系，但是又都不够相关。这种没有哪句特别相关，但整体都沾点边儿的问题，rug基本上是处理不了的，为了弥补这些缺陷，现在也出现了很多改进的方案，比如在预处理的时候把我统一都换成老王，或者干脆就让大模型参与到分块的过程。根据语义自动判断应该在哪里断，应该怎么切。但是目前来说还没有哪个方案是真正的十全十美的，新的想法还在不断的涌现中，这里我就不展开讲了。不过说到底的就是这么一种结构，我觉得他更像是一种妥协，在大模型上下文有限的情况下面，我们用这种方式尽量让模型的回答别太离谱，他能解决一部分的问题，但也确实还有很多问题没有办法解决。所以我也一直在等那个真正意义上突破了妥协的下一代架构。Rug本质上是一种压缩文档，太长了我们就把它切块做筛选见索引，把重要的留下，不重要的东西我们就可以丢掉了。其实我们每天的生活也是这样的，每天我们面对了太多的人，太多的事情，也就慢慢学会了分类和取舍。这个要紧，那个可以晚一点，这个需要记住，而那些事情也许就可以忘记了。我们以为这是成熟，是清醒，但有的时候东西切的太碎，也难免会失去上下文，误解了原意。一些原本重要的东西也许就失去了链接。比如说那个老同桌，或者那最后一条消息还停留在哈哈哈的死党。不是他们变得不重要了，只是我们各自生活的向量坐标越来越远了。前几天我偶然翻到一张以前一起打游戏的截图，我盯着他们看了好久，突然间发现那个时候他的dps是真的低呀。这里是程轩老王，我们下期再见。",
  "title": "",
  "author": "",
  "publish_date": "",
  "source": "Bilibili (via SnapAny + Paraformer)",
  "language": "zh-CN",
  "word_count": 2900,
  "extraction_method": "snapany_paraformer",
  "extraction_timestamp": "2025-11-06T10:43:55.496705",
  "batch_id": "20251106_023619",
  "link_id": "bili_req7",
  "error": null,
  "summary": {
    "transcript_summary": {
      "key_facts": [
        "FACT: A programmer encounters issues with an internal document and queries a large language model, which responds with irrelevant information due to hallucination.",
        "FACT: When the full document and question are sent together, the model provides a correct response, solving the immediate problem.",
        "FACT: As documents grow larger, relevant information may be buried in small sections, making it hard for models to focus on key content.",
        "FACT: Retrieval-Augmented Generation (RAG) addresses this by sending only contextually relevant parts of a document to the model.",
        "FACT: Embedding models convert text into fixed-length numerical arrays (vectors), regardless of input length.",
        "FACT: OpenAI's text-embedding-3-small outputs a 1536-dimensional vector; text-embedding-3-large outputs a 3072-dimensional vector.",
        "FACT: Similar texts produce similar vectors, meaning their distance in vector space is small.",
        "FACT: Vector similarity is measured using geometric distance in high-dimensional space (e.g., 1536 or 3072 dimensions).",
        "FACT: RAG uses embedding models to convert user questions into vectors and retrieve top-k most similar document chunks.",
        "FACT: Document preprocessing in RAG involves splitting text into chunks via methods like sentence, paragraph, or custom segmentation.",
        "FACT: The process of splitting text into chunks is called 'chunking' in technical contexts.",
        "FACT: Each chunk is embedded into a vector and stored alongside its original text in a vector database.",
        "FACT: Vector databases are optimized to find records with vectors closest to a query vector, unlike traditional databases.",
        "FACT: Popular vector databases include ChromaDB and PostgreSQL with pgvector extension.",
        "FACT: RAG architecture enables efficient retrieval of relevant context before generating answers with LLMs."
      ],
      "key_opinions": [
        "OPINION: RAG is a pragmatic compromise to overcome LLM context limitations, not a perfect solution.",
        "OPINION: Poor chunking strategies can break pronoun references and semantic continuity, leading to incorrect retrieval.",
        "OPINION: RAG struggles with questions requiring global document understanding rather than local relevance.",
        "OPINION: Current RAG improvements like pre-processing substitutions or dynamic chunking are still imperfect.",
        "OPINION: The evolution of RAG reflects ongoing trade-offs between precision, scalability, and contextual integrity.",
        "OPINION: Over-reliance on chunking risks losing narrative coherence and inter-sentence dependencies.",
        "OPINION: The metaphor of human memory and forgetting parallels RAG’s selective retention of information.",
        "OPINION: RAG’s effectiveness depends heavily on how well the document is segmented and indexed.",
        "OPINION: The ideal future architecture should eliminate the need for such compromises altogether.",
        "OPINION: RAG is not just a technical tool but also a reflection of how humans manage information overload."
      ],
      "key_datapoints": [
        "DATA: OpenAI's text-embedding-3-small model outputs a 1536-dimensional vector.",
        "DATA: OpenAI's text-embedding-3-large model outputs a 3072-dimensional vector.",
        "DATA: Vector databases store embeddings and enable nearest-neighbor search based on similarity.",
        "DATA: RAG retrieves top-k most similar document chunks based on vector distance.",
        "DATA: Chunking methods include sentence-level, paragraph-level, and algorithmic segmentation.",
        "DATA: Traditional databases cannot efficiently handle approximate vector similarity searches.",
        "DATA: ChromaDB and PostgreSQL with pgvector are commonly used vector database solutions.",
        "DATA: Embedding models compress text into fixed-length arrays while preserving semantic meaning.",
        "DATA: High-dimensional spaces (1536+ dimensions) better represent complex semantic relationships.",
        "DATA: RAG reduces hallucination risk by limiting input context to only relevant snippets."
      ],
      "topic_areas": [
        "RAG architecture",
        "Embedding models",
        "Vector databases",
        "Text chunking",
        "Semantic similarity",
        "LLM hallucination",
        "Contextual retrieval",
        "Information filtering",
        "Document preprocessing",
        "AI system limitations"
      ],
      "word_count": 12,
      "total_markers": 35
    },
    "comments_summary": {},
    "created_at": "2025-11-06T10:47:51.527449",
    "model_used": "qwen-flash"
  }
}
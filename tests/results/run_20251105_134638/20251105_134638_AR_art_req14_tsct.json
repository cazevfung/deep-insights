{
  "success": true,
  "url": "https://medium.com/@myscale/revolutionizing-data-retrieval-in-advanced-rag-with-vector-search-b775107eca82",
  "content": "Revolutionizing Data Retrieval in Advanced RAG with Vector Search\nIn this series on the Advanced RAG pipeline, we’ve discussed how other components like Embedding models, indexing methods and chunking techniques build the foundation of efficient systems. Now, let’s explore a very important part of this pipeline: vector search.\nThe key capability of a database lies in its search performance. From web search to object identification, its applications are countless, and hence, a lot depends on the search efficiency, versatility and accuracy. A little bit slower or inaccurate (or limited) search can be the difference between a satisfied or never-to-return-again customer, and businesses are well aware of this.\nVector search is essential in modern retrieval systems. It powers semantic search and ensures that retrieved data matches the query contextually. The core idea behind vector search is to represent each item as a high-dimensional vector, where each dimension corresponds to a feature or characteristic of the item. The similarity between two items can then be measured as the distance between their vector representations.\nBrief Introduction of Vector Indexing\nIndexing is the key driving force behind vector search. It is what makes searches fast, efficient, and meaningful. Before running any search on a table, it must first be indexed. This process organizes the data in a way that allows the search to retrieve relevant results quickly. So, let’s first focus on the “index”.\nHere are some considerations while adding an index:\n- Indexes can only be created on vector columns (\nArray(Float32)\n) or text columns. - For vector columns, it’s crucial to specify the maximum length of the array beforehand. This ensures consistency in the structure of the data and helps the index work efficiently.\n- Indexes are primarily used in similarity searches, which means you need to define a similarity metric while creating the index. For example, you might choose\nCosine\nsimilarity, which is one of the most common metrics for comparing vectors.\nIndexes are the backbone of efficient vector searches, as they drastically reduce the time and computational effort required to find the right data. Choosing the right indexing strategy and understanding these considerations can make all the difference in the performance of your system.\nExample\nAdding an index in MyScale is just like how we add it for any SQL table. As an example, we define a table and add a vector column there.\nCREATE TABLE test_float_vector\n(\nid UInt32,\ndata Array(Float32),\nCONSTRAINT check_length CHECK length(data) = 128,\ndate Date,\nlabel Enum8('person' = 1, 'building' = 2, 'animal' = 3)\n)\nENGINE = MergeTree\nORDER BY id\nAs you can see, we have added the maximum length constraint as it is necessary before adding an index.\nALTER TABLE default.test_float_vector\nADD VECTOR INDEX data_index data\nTYPE MSTG ('metric_type=Cosine');\nAnd using the same SQL we use for the traditional relational db queries, we can fetch the most relevant documents to the search term (\"What was the solution proposed to farmers problem by Levin?\"\n). The search phrase needs to be converted into an embedding first. We can do it using any embedding model or service. After getting the embedding (in input_embedding\n) we get the most relevant documents.\nNote: MyScale also provides EMBEDTEXT()\nfunction, which can be directly used in both Python (or other) interface and SQL.\nIndexing Algorithms\nThe above syntax isn’t unique but vector indexing works a bit differently behind the scenes. Usually, it employs clustering and is followed by some graph/tree of vectors for faster traversal. Choosing a suitable indexing algorithm can be an important factor for the searching efficiency, here are some popular indexing algorithms:\n- Local Sensitivity Hashing (LSH): Locality-Sensitive Hashing (LSH) uses special hash functions to group similar data points into the same bucket. This process ensures that embeddings or vectors close to each other are more likely to collide in the same hash table. It allows for faster searches in high-dimensional spaces, making it ideal for approximate nearest neighbor tasks.\n- Inverted File (IVF): Inverted File Indexing (IVF) works by clustering the vectors into groups. When a query vector is provided, the system calculates the distance of the query from the centers of all clusters. It then searches within the cluster that is closest to the query. However, since this search relies on the cluster centers (similar to k-Nearest Neighbors), a potential downside is that closer vectors in other clusters might be missed. Additionally, in some cases, multiple clusters may need to be searched for better results.\nIVF also has several variants, such as IVF-Flat and IVF-PQ, which offer different trade-offs in terms of performance and storage.\nNote:Choosing a suitable indexing algorithm becomes even more important in the context of how quickly vector databases grow with millions of vectors and hence every second saved is precious.\n- Hierarchical Navigable Small Worlds (HNSW): HNSW works by building layers of graphs, which are progressively populated (i.e, topmost layer has the least number of nodes). This hierarchical layering means we don’t have to search too many nodes and hence it is quite fast. But making new graphs (whenever a new vector is added) can be quite time-consuming.\n- Multi-Scale Tree Graph (MSTG): Both HNSW and IVF are quite good, but their performance issues start when scaling them for bigger datasets. Graph search is quite good at initial convergence, but struggles in filtered search, while tree search is good at filtered search, but slower. MSTG combines the both by using the combination of hierarchal graphs and trees.\nSummarising it here as a table for the future (quick) reference:\nBasic Vector Search\nVector search is a sophisticated data retrieval technique that focuses on matching the contextual meanings of search queries and data entries, rather than simple text matching. To implement this technique, we must first convert both the search query and a specific column of the dataset into numerical representations, known as vector embeddings.\nThen, calculate the distance (Cosine similarity or Euclidean distance) between the query vector and the vector embeddings in the database. Next, identify the closest or most similar entries based on these calculated distances. Lastly, we return the top-k results with the smallest distances to the query vector.\nNote:Semantic search builds on the basic definition of vector search to return more relevant results based on the meaning of text rather than exact terms. In practice, though, vector search and semantic search are often used interchangeably.\nFull-Text Search\nTraditional SQL searches and even regular expressions are quite limited in taking either the exact term or a specific pattern for the comparison. This example will give an illustration. We are using SQL syntax to look up documents relevant to “Thanksgiving for vegans”.\nAND Condition\nSELECT\nid,\ntitle,\nbody\nFROM\ndefault.en_wiki_abstract\nWHERE\nbody LIKE '%Thanksgiving%'\nAND body LIKE '%vegans%'\nORDER BY\nid\nLIMIT\n4;\nOR Condition\nSELECT\nid,\ntitle,\nbody\nFROM\ndefault.en_wiki_abstract\nWHERE\nbody LIKE '%Thanksgiving%'\nOR body LIKE '%vegans%'\nORDER BY\nid\nLIMIT\n4;\nApplying AND\nsearch understandably doesn’t yield any result, while using OR\noperator broadens the scope by returning documents that contain either \"Thanksgiving\" or \"vegans.\" While this approach retrieves results, not all documents may be highly relevant to both terms.\nThe Limitations of Traditional SQL Search\nTraditional SQL searches like these are rigid. They don’t account for semantic similarities or variations in phrasing. For example, SQL treats “doctor applying anesthesia” and “doctor apply anesthesia” as completely different phrases, which means similar matches are often overlooked.\nHow Full-Text Search Improves\nIn the former case, we perform searches based on the exact phrase, while in the latter, searches are conducted using keywords. Full-text search supports both methods, allowing for flexibility based on user preference. Unlike the rigid, default SQL search, full-text search accommodates some degree of variance in the text.\nHow it works\nFull-text search’s indexing works in these substeps.\n- Tokenization : Tokenization is used to break down the text into smaller parts. In word tokenization, we take a text and it splits it into words, while sentence and character tokenizations split it on the sentence and character level respectively.\n- Lemmatization/Stemming: Stemming and lemmatization break down the words into their “raw” form. For example, “playing” will be reduced to “play”, “children” to “child” and so on.\n- Stopwords Removal: Some words like articles or prepositions contain lesser information and can be avoided by using this option. The default “English” stopword filter not only focuses on removing articles and prepositions, but some other words too, like “when”, “ourselves”, “my”, “doesn’t”, “not”, etc.\n- Indexing : Once the text is preprocessed, it is stored in an inverted index.\nOnce the indexing is done, we can apply the search.\nScoring and Ranking: BM25 (Best Match 25) is a very commonly used algorithm. It ranks documents similar to how vector search ranks embeddings (or any general vectors) for similarity. BM25 is just an improved form of TF-IDF and depends on 3 factors: term frequency, inverse document frequency (rewarding rare terms) and document length normalization.\nExample\nThe full-text search index can be simply added by specifying the type fts\n. We can specify whether to choose lemmatization or stemming as well as the choice of stop word filters.\nALTER TABLE default.en_wiki_abstract\nADD INDEX body_idx body\nTYPE fts('{\"body\":{\"tokenizer\":{\"type\":\"stem\", \"stop_word_filters\":[\"english\"]}}}');\nAs you can see, it ranks the documents/text samples as per their similarity with the query.\nSELECT\nid,\ntitle,\nbody,\nTextSearch(body, 'thanksgiving for vegans') AS score\nFROM default.en_wiki_abstract\nORDER BY score DESC\nLIMIT 5;\nHybrid Search\nWhile full-text search provides some flexibility, it is still limited in its ability to find semantically similar text. For this, we need to rely on embeddings, which can capture the deeper semantic meaning of the text. On the other hand, full-text search has its own advantages, as it is great for basic keyword retrieval and text matching.\nOn the flip side, vector search (powered by embeddings) excels at cross-document semantic matching and deep understanding of semantics. However, it may lack efficiency when dealing with short text queries.\nTo get the best of both worlds, a hybrid search approach can be employed. By integrating full-text search and vector search, the hybrid approach can offer a more comprehensive and powerful search experience. Users can benefit from the precision of keyword-based retrieval as well as the deeper semantic understanding enabled by embeddings. This allows for more accurate and relevant search results, catering to a wide range of user needs and query types.\nExample\nLet’s use an example to illustrate it further. We will continue using the same wiki_abstract_mini\ntable for easier comparison. The full-text search index is added on body\nwhile the vector index on the respective embedding will further complement it.\nALTER TABLE default.wiki_abstract_mini\nADD INDEX body_idx (body)\nTYPE fts('{\"body\":{\"tokenizer\":{\"type\":\"stem\", \"stop_word_filters\":[\"english\"]}}}');\n#Create Vector Index\nALTER TABLE default.wiki_abstract_mini\nADD VECTOR INDEX body_vec_idx body_vector\nTYPE MSTG('metric_type=Cosine');\nSince vector search is applied directly on the embeddings rather than the text, so we will use E5 large model here (via Hugging Face). Making a function will allow us to call it directly whenever we need to.\nCREATE FUNCTION EmbeddingE5Large\nON CLUSTER '{cluster}' AS (x) -> EmbedText (\nconcat('query: ', x),\n'HuggingFace',\n'https://api-inference.huggingface.co/models/intfloat/multilingual-e5-large',\n'hf_xxxx',\n''\n);\nHybrid Search Function\nHybridSearch()\nfunction is a search method that combines the strengths of vector search and text search. This approach not only enhances the understanding of long text semantics but also addresses the semantic shortcomings of vector search in the short-text domain.\nSELECT\nid,\ntitle,\nbody,\nHybridSearch('fusion_type=RSF', 'fusion_weight=0.6')(body_vector, body, EmbeddingE5Large('Charted by the BGLE'), ' BGLE') AS score\nFROM default.wiki_abstract_mini\nORDER BY score DESC\nLIMIT 5;\nMuiltimodal Search\nHybrid search provides us a combination of full-text and vector search. Multimodal search, however, takes it a step further by enabling searches across different types of data, such as images, videos, audio, and text.\nThis concept is based on contrastive learning and multimodal search works on a simple mechanism:\n- A unified model like CLIP(Contrastive Language–Image Pretraining) processes various data types (like images or text) and converts them into embeddings.\n- These embeddings are mapped into a single, shared vector space, making them comparable across data types.\n- Regardless of the data’s original form, the embeddings closest to the query’s embedding are returned.\nThis approach allows us to retrieve diverse, relevant results — whether they are images, text, audio, or videos — based on a single query. For example, you can search for an image by providing a text description or find a video using an audio clip.\nBy aligning all data types into one vector space, multimodal search eliminates the need to combine separate results from different models. This also shows how powerful and flexible vector databases can be, helping to retrieve data from multiple types of sources seamlessly.\nConclusion\nVector databases have revolutionized the way data is stored, providing not just enhanced speed but also immense utility across diverse domains such as artificial intelligence and big data analytics. And the cornerstone of this technology lies in the search capabilities.\nMyScale provides 3 types of search, which we have covered: vector, full-text and hybrid search. By covering indexing algorithms and providing these diverse search functionalities, MyScale empowers users to tackle a wide range of data retrieval challenges across different domains and use cases. We hope the examples provided give you a good understanding of these search capabilities and how to leverage them effectively.",
  "title": "",
  "author": "",
  "publish_date": "",
  "source": "medium.com",
  "language": "auto",
  "word_count": 2205,
  "extraction_method": "article_trafilatura",
  "extraction_timestamp": "2025-11-05T21:46:56.187986",
  "batch_id": "20251105_134638",
  "link_id": "art_req14",
  "error": null,
  "article_id": "e5c14bcf2068",
  "summary": {
    "transcript_summary": {
      "key_facts": [
        "FACT: Vector search enables semantic matching by representing data as high-dimensional vectors based on contextual features.",
        "FACT: Indexing in vector databases organizes data to drastically reduce search time and computational effort.",
        "FACT: Vector indexes must be created on columns with consistent array lengths, such as Array(Float32), and require a defined similarity metric like Cosine.",
        "FACT: MyScale supports SQL-like syntax for creating tables with vector columns and adding vector indexes using ALTER TABLE statements.",
        "FACT: The EMBEDTEXT() function in MyScale converts text into embeddings directly within SQL or Python interfaces.",
        "FACT: Local Sensitivity Hashing (LSH) groups similar vectors into buckets using hash functions to enable fast approximate nearest neighbor searches.",
        "FACT: Inverted File (IVF) indexing clusters vectors and searches only the closest cluster to the query vector, potentially missing nearby vectors in other clusters.",
        "FACT: Hierarchical Navigable Small Worlds (HNSW) uses layered graph structures to accelerate search by reducing the number of nodes traversed.",
        "FACT: Multi-Scale Tree Graph (MSTG) combines hierarchical graphs and tree structures to balance speed and accuracy in large-scale vector searches.",
        "FACT: Full-text search improves upon traditional SQL LIKE queries by supporting tokenization, lemmatization, stemming, and stopword filtering.",
        "FACT: BM25 is a ranking algorithm used in full-text search that considers term frequency, inverse document frequency, and document length normalization.",
        "FACT: Hybrid search integrates vector search and full-text search to combine semantic understanding with keyword precision.",
        "FACT: Multimodal search uses models like CLIP to map images, text, audio, and video into a shared vector space for cross-modal retrieval.",
        "FACT: Contrastive learning underpins multimodal search by aligning different data types into a unified embedding space for comparison.",
        "FACT: MyScale supports three search types: vector, full-text, and hybrid, enabling flexible data retrieval across diverse use cases."
      ],
      "key_opinions": [
        "OPINION: The choice of indexing algorithm significantly impacts performance, especially as vector databases scale to millions of entries.",
        "OPINION: Vector search offers superior contextual relevance compared to traditional keyword-based methods, even if it’s less efficient for short queries.",
        "OPINION: Hybrid search provides the most balanced and powerful retrieval experience by combining the strengths of both semantic and keyword-based approaches.",
        "OPINION: Full-text search remains essential for precise keyword matching, particularly when dealing with structured or exact-term queries.",
        "OPINION: Multimodal search represents a major leap forward in unifying diverse data types under a single semantic framework.",
        "OPINION: Efficient indexing is not just a technical detail—it's a critical factor in user satisfaction and system scalability.",
        "OPINION: The integration of embedding models via APIs like Hugging Face enhances flexibility but introduces latency trade-offs.",
        "OPINION: While vector search excels at semantics, it can struggle with short or ambiguous queries without complementary techniques.",
        "OPINION: The future of data retrieval lies in adaptive systems that dynamically choose between search modalities based on query type.",
        "OPINION: Real-world applications demand more than raw speed—they need accuracy, versatility, and seamless integration across data formats."
      ],
      "key_datapoints": [
        "DATA: Vector columns in MyScale must have a fixed array length, such as 128 elements, specified via a CHECK constraint.",
        "DATA: Cosine similarity is one of the most commonly used metrics for measuring vector similarity in indexing.",
        "DATA: The MSTG indexing algorithm is designed to handle both initial convergence and filtered search efficiently.",
        "DATA: IVF-PQ and IVF-Flat are variants of Inverted File Indexing offering different performance-storage trade-offs.",
        "DATA: BM25 scoring depends on term frequency, inverse document frequency, and document length normalization.",
        "DATA: The E5 Large model from Hugging Face is used to generate embeddings for hybrid search queries.",
        "DATA: HybridSearch() function supports fusion types like RSF and allows weighting parameters such as fusion_weight=0.6.",
        "DATA: MyScale supports vector indexing with the MSTG type and Cosine metric for optimal semantic similarity.",
        "DATA: Full-text search in MyScale uses stem-based tokenization and English stopword filters by default.",
        "DATA: The CLIP model maps multiple data types into a shared vector space for cross-modal retrieval.",
        "DATA: Vector search reduces search complexity from O(N) to sub-linear time through effective indexing.",
        "DATA: Hybrid search results are ranked using combined scores from both vector and full-text components.",
        "DATA: HNSW layering reduces node traversal during search, improving response time in large datasets.",
        "DATA: Full-text search with BM25 typically outperforms TF-IDF in ranking relevance for long documents.",
        "DATA: Embedding generation via Hugging Face API introduces variable latency depending on model load and network conditions."
      ],
      "topic_areas": [
        "Vector search fundamentals",
        "Indexing algorithms",
        "Full-text search",
        "Hybrid search",
        "Multimodal search",
        "Embedding models",
        "MyScale database features",
        "Semantic retrieval",
        "Data indexing optimization",
        "Search performance metrics"
      ],
      "word_count": 2205,
      "total_markers": 40
    },
    "comments_summary": {},
    "created_at": "2025-11-05T21:48:26.353779",
    "model_used": "qwen-flash"
  }
}
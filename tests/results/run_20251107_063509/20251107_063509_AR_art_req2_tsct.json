{
  "success": true,
  "url": "https://arxiv.org/abs/2312.04547",
  "content": "Computer Science > Computer Vision and Pattern Recognition\n[Submitted on 7 Dec 2023]\nDigital Life Project: Autonomous 3D Characters with Social Intelligence\nZhongang Cai, Jianping Jiang, Zhongfei Qing, Xinying Guo, Mingyuan Zhang, Zhengyu Lin, Haiyi Mei, Chen Wei, Ruisi Wang, Wanqi Yin, Xiangyu Fan, Han Du, Liang Pan, Peng Gao, Zhitao Yang, Yang Gao, Jiaqi Li, Tianxiang Ren, Yukun Wei, Xiaogang Wang, Chen Change Loy, Lei Yang, Ziwei Liu\nIn this work, we present Digital Life Project, a framework utilizing language as the universal medium to build autonomous 3D characters, who are capable of engaging in social interactions and expressing with articulated body motions, thereby simulating life in a digital environment. Our framework comprises two primary components: 1) SocioMind: a meticulously crafted digital brain that models personalities with systematic few-shot exemplars, incorporates a reflection process based on psychology principles, and emulates autonomy by initiating dialogue topics; 2) MoMat-MoGen: a text-driven motion synthesis paradigm for controlling the character's digital body. It integrates motion matching, a proven industry technique to ensure motion quality, with cutting-edge advancements in motion generation for diversity. Extensive experiments demonstrate that each module achieves state-of-the-art performance in its respective domain. Collectively, they enable virtual characters to initiate and sustain dialogues autonomously, while evolving their socio-psychological states. Concurrently, these characters can perform contextually relevant bodily movements. Additionally, a motion captioning module further allows the virtual character to recognize and appropriately respond to human players' actions. Homepage: this https URL\nComments:\tHomepage: this https URL\nSubjects:\tComputer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Graphics (cs.GR); Human-Computer Interaction (cs.HC)\nCite as:\tarXiv:2312.04547 [cs.CV]\n(or arXiv:2312.04547v1 [cs.CV] for this version)\nhttps://doi.org/10.48550/arXiv.2312.04547\nFocus to learn more\nSubmission history\nFrom: Zhongang Cai [view email]\n[v1] Thu, 7 Dec 2023 18:58:59 UTC (31,208 KB)\nAccess Paper:\nView PDF\nHTML (experimental)\nTeX Source\nview license\nCurrent browse context:\ncs.CV\n< prev   |   next >\nnew | recent | 2023-12\nChange to browse by:\ncs\ncs.AI\ncs.GR\ncs.HC\nReferences & Citations\nNASA ADS\nGoogle Scholar\nSemantic Scholar\nExport BibTeX Citation\nBookmark\nBibliographic Tools\nBibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer (What is the Explorer?)\nConnected Papers Toggle\nConnected Papers (What is Connected Papers?)\nLitmaps Toggle\nLitmaps (What is Litmaps?)\nscite.ai Toggle\nscite Smart Citations (What are Smart Citations?)\nCode, Data, Media\nDemos\nRelated Papers\nAbout arXivLabs\nWhich authors of this paper are endorsers? | Disable MathJax (What is MathJax?)",
  "title": "Computer Science > Computer Vision and Pattern Recognition",
  "author": "Zhongang Cai, Jianping Jiang, Zhongfei Qing, Xinying Guo, Mingyuan Zhang, Zhengyu Lin, Haiyi Mei, Chen Wei, Ruisi Wang, Wanqi Yin, Xiangyu Fan, Han Du, Liang Pan, Peng Gao, Zhitao Yang, Yang Gao, Jiaqi Li, Tianxiang Ren, Yukun Wei, Xiaogang Wang, Chen Change Loy, Lei Yang, Ziwei Liu",
  "publish_date": "[Submitted on 7 Dec 2023]",
  "source": "arxiv.org",
  "language": "auto",
  "word_count": 396,
  "extraction_method": "article_playwright",
  "extraction_timestamp": "2025-11-07T14:35:30.582642",
  "batch_id": "20251107_063509",
  "link_id": "art_req2",
  "error": null,
  "article_id": "5c5fba69ecbe",
  "summary": {
    "transcript_summary": {
      "key_facts": [
        "FACT: The Digital Life Project introduces a framework for creating autonomous 3D characters using language as a universal medium.",
        "FACT: SocioMind is a digital brain that models personality through few-shot exemplars and incorporates psychological reflection processes.",
        "FACT: SocioMind enables characters to initiate dialogue topics autonomously based on internal socio-psychological states.",
        "FACT: MoMat-MoGen is a text-driven motion synthesis system combining motion matching with advanced motion generation techniques.",
        "FACT: Motion matching ensures high-quality physical movements, while motion generation adds diversity in character behavior.",
        "FACT: The framework allows virtual characters to sustain dialogues and evolve their social and psychological states over time.",
        "FACT: Characters can perform contextually appropriate body motions in response to conversational or environmental cues.",
        "FACT: A motion captioning module enables characters to recognize and react to human players' actions.",
        "FACT: Extensive experiments show both SocioMind and MoMat-MoGen achieve state-of-the-art performance in their respective domains.",
        "FACT: The project was submitted to arXiv on December 7, 2023, under the cs.CV category.",
        "FACT: The work spans multiple domains including computer vision, AI, graphics, and human-computer interaction.",
        "FACT: The project's homepage is accessible via a provided URL linked in the paper."
      ],
      "key_opinions": [
        "OPINION: Language should serve as the primary interface for building socially intelligent digital beings.",
        "OPINION: Psychological principles are essential for enabling believable autonomy in virtual characters.",
        "OPINION: Combining motion matching with generative models offers the best balance between realism and expressiveness.",
        "OPINION: Autonomous character systems must evolve emotionally and socially to feel lifelike.",
        "OPINION: Human-like responsiveness requires not just verbal but also embodied behavioral alignment.",
        "OPINION: Current virtual agents lack true initiative; this framework addresses that gap by enabling proactive dialogue.",
        "OPINION: The integration of cognitive modeling with motion control represents a significant leap forward in digital life simulation."
      ],
      "key_datapoints": [
        "DATA: The framework was submitted on December 7, 2023, with version v1 available at arXiv:2312.04547 [cs.CV].",
        "DATA: The paper has a file size of 31,208 KB in PDF format.",
        "DATA: The project is categorized under cs.CV, cs.AI, cs.GR, and cs.HC subject areas.",
        "DATA: The DOI assigned is 10.48550/arXiv.2312.04547.",
        "DATA: The work has been cited across platforms including NASA ADS, Google Scholar, and Semantic Scholar.",
        "DATA: The paper includes a motion captioning module for interpreting human player actions.",
        "DATA: The framework demonstrates state-of-the-art performance in both dialogue autonomy and motion synthesis benchmarks."
      ],
      "topic_areas": [
        "Autonomous 3D Characters",
        "Social Intelligence in AI",
        "Text-Driven Motion Synthesis",
        "Cognitive Modeling in Virtual Agents",
        "Human-Computer Interaction",
        "Emotional and Psychological Simulation",
        "Motion Matching and Generation",
        "Language as Control Medium",
        "Digital Life Simulation",
        "Embodied Dialogue Systems"
      ],
      "word_count": 396,
      "total_markers": 26
    },
    "comments_summary": {},
    "created_at": "2025-11-07T14:43:57.359400",
    "model_used": "qwen-flash"
  }
}
{
  "success": true,
  "video_id": "vIZdk1-XyHY",
  "url": "https://www.youtube.com/watch?v=vIZdk1-XyHY",
  "content": "Cursor calls itself the AI code editor and it works by indexing your entire code base and then making suitable suggestions based on what you've written. This great blog post goes into how cursor indexes these code bases fast and gives an explanation of yeah how they do it using Merkel trees with Merkel. They don't want to reference the ex-gererman president Angela Merkel which I had to think first but I guess I'm just a idiot and not a computer scientist. Let's dive deep into Merkel trees and not Angela Merkel. Okay, so how cursor indexes code bases fast by engineers codeex which is a really great subject publication. I recommend you subscribe to it. Now cursor as I said is the popular AI IDE that recently announced they hit 300 million in ARR and they use Merkel trees to index code fast. This post goes over exactly how. Now before diving into cursor's implementation let's first understand what a Merkel tree is and I have to be definitely included in this explanation. So let's learn together what a Merkel tree is. A Merkel tree is a tree structure in which every leaf node is labeled with the cryptographic hash of a data block and every non-leaf node is labeled with the cryptographic hash of the labels of its child nodes. This creates a hierarchical structure where changes at any level can be efficiently detected by comparing hash values. But that's a lot. So think of them as fingerprinting system for data. Each piece of data like a file gets its own unique fingerprint hash. Pairs of fingerprints are combined and given a new fingerprint. This process continues until you have just one master fingerprint, the root hash. The root hash summarizes all data contained in the individual pieces serving as a cryptographic commitment to the entire data set. The beauty of this approach is that if any single piece of data changes, it will change all the fingerprints above it, ultimately changing the root hash, this makes a lot of sense because if you write code in your codebase, which is basically what you're getting paid for as software developer, then that basically recalculates the entire index and then can serve a better suggestion. Okay, so then we got the sponsored segment and now how cursor uses Miracle Trees for codebase indexing step by step. Cursor uses Miracle Trees as a core component of its codebased indexing feature. According to a post by Cursor's founder and the security documentation, here's how it works. Okay, so we have step one code junking. Split code into semantic chunks. Okay, this makes sense. Step two, miracle tree construction. Compute hash tree of all valid files. Okay, this is what we just read about. Then it syncs to the server side processing. Step three. It takes in the constructed Merkel tree, the root hash, I guess, and creates a vector representations which then gets stored and indexed in a vector database. And only on changed files there are periodic updates that check for changes every 10 minutes using Merkel trees. Holy This is some great engineering. Okay. Okay. Okay. So, step one, code chunking and processing. Cursor first chunks your codebase files locally splitting code into semantically meaningful pieces before any processing occurs. Next, when codebase indexing is enabled, which I guess it's by default, cursor scans the folder open in the editor and computes a Merkel tree of hashes of all valid files. This Merkel tree is then synchronized with cursor server as detailed in cursor security documentation. After the chunks are sent to cursor server, embeddings are created using either open AI embedding API or a custom embedding model. Okay, the author couldn't verify this. This is fine. I can give him that. These vector representations capture the semantic meaning of the code chunks. The embeddings along with metadata like start and line numbers and file paths are stored in a remote vector database. Turbo puffer to maintain privacy while still enabling path-based filtering. Cursor stores an offiscated relative file path with each vector. Importantly, according to cursors founder, none of your code is stored in our databases. It's gone after the life of the requests. Okay, that's great. Then we have the periodic update. Okay. Every 10 minutes, cursor checks for hash mismatches using numerical tree to identify which files have changed. This makes sense. Only the changed files need to be uploaded, significantly reducing bandwidth usage. Okay, so you don't upload the entire codebase. only the change files and this gets explained in the security documentation. This is where the miracle tree structure provides it greatest value enabling efficient incremental updates with okay so it reduces bandwidth and with that speed I guess in which it's able to make suggestions okay so the strategies are the effect of the codebased indexing largely depends on how code is chunked yes while my previous explanation didn't go into detail about chunking methods this blog post about building a crossful like codebased feature provides some insights. An attempt to build cursors codebase feature rag on code bases part one of two. Okay, so there is someone rebuilding this. Great. If I should make a video about this then let me know in the comments. Let's first go on with the Merkel tree explanation. So while simple approaches split code by characters, words or lines like tokenization and AI, I guess they often miss semantic boundaries resulting in the greater embedding quality. Mhm. You can split code based on a fixed token count, but this can cut off code blocks like functions or classes midway. This is bad. A more effective approach is to use an intelligent splitter that understands code structures such as recursive text splitters that use highle delimiters in G-class and function definitions to split it appropriate semantic boundaries right so you won't want to split a function in half this destroys the entire point I guess an even more elegant solution okay I'm hyped is to split the code based on its abstract syntax tree structure by traversing this is now we are deep into algorithms and data structures which since I've not studied CS as a bachelor's degree I'm only a e-commerce guy I never had so let's have a look by traversing the a depth first it splits code into sub trees that fit within token limits to avoid creating too many small chunks sibling nodes are merged into larger chunks as long as they stay under the token limit tools like tree sitter can be used for this AC parsing supporting a wide range of programming languages. What is trees sitter? It's a parser generator tool in an incremental parsing library. It can build a concrete syntax tree for a source file and efficiently update the syntax tree as the source file is edited. This sounds cool and it works in a lot of languages. Nice. How embeddings are used at inference time. This is important. So after covering how cursor creates and stores code embeddings, a natural question arises, how are these embeddings actually used once they've been generated? Right? So far we only did some computer science flexing but haven't made any use of them. So this section explains the practical application of these embeddings during normal usage. So we have semantic search and context retrieval. When you interact with cursors AI features like asking questions about your codebase using a codebase or enter the following process occurs. First query embedding or a query cursor computes an embedding for your question or the code context you're working with. Second vector similarity search. This query embedding is sent to turbopuffer cursor vector database. Let me quickly check. Is turbo puffer an eternal thing or is it like search every bite? Tur puffer is a serless vector in full text searchable from first principles an object storage fast 10x cheaper and extremely scalable. Oh, and I guess it's used by cursor which they have a case study. Okay. 95% cost reduction, 100 billion vectors. I guess they know what they're doing. Okay. So, you're using Turbo Puffer. Didn't know of that. It's curs vector database. Mhm. Which performs a nearest neighbor search to find code chunks semantically similar to your query. Okay. I'm still still with them with the with the text. I hope you are too. Now, three local file access cursor clients receive the results which include obfiscated file paths and line ranges of the most relevant code chunks. Importantly, the actual code content remains on your machine and is retrieved locally. Now, important four, context assembly. The client reads these relevant code chunks from your local files and sends them as context to the server for the LLM to process alongside your question. And five, informed response. The LLM now has the necessary context from your codebase to provide a more informed and relevant response to your question or generate appropriate code completions. This makes absolute sense. Amazing. This embedding part retrieval allows for contextual code generation, codebased Q&A, smart code completion, intelligent refactoring. Nice. And it appears to be super efficient. So why does Cars use Merkel trees? For efficient incremental updates, right? By using Merkel tree, they can identify which files have changed. Okay, we had that for data integrity verification to verify that the files being indexed match was stored on the server. Okay, for optimized caching, right, it only needs to update the change file and it makes sure right that the indexing is faster from the second time onwards, right? Privacy preserving indexing to protect sensitive information and file cursor implements path obuscation by splitting the path by slash and dot characters and encrypting each segment with a secret key stored on a client. While this still reveals some information about directory hierarchy, it hides most sensitive details and git history integration. When goldbased indexing is enabled in a git repository, cursor also indexes to get history. Okay, it stores commit sas parent information and obuscated file names to enable sharing the data structure for users in the same git repo and on the same team. The secret key for obiscating file names is derived from hashes of recent commit contents. Stay cooked and it's a really great explanation. Shout out to my man engineers codeex. Check him out on substack. Now the handshake process. Now we have embedding models and considerations. Then we have the handshake process. I tripped. Sorry. the choice of embedding models significantly while some systems. Yes, it likely uses OpenAI's embedding models or a custom one specifically tuned for KOD. I imagine at their size and at their VC budget, I guess a customuilt model would be affordable to say the least. Now for the handshake process. A key aspect of cursor's Merkel tree implementation is the handshake process that occurs during synchronization. Logs from the cursor application reveal that when initializing codebase indexing cursor creates a Merkel client and performs a startup handshake with the server. This handshake involves sending the root hash of the locally computed Merkel tree to the server as seen in issue on GitHub. The handshake pro allows server to determine which parts of the codebase need to be synced. And based on the handshake logs, we can see that cursor computes the initial hash of the codebase and sends it to the server for verification. Okay. Now, technical implementation challenges. While the Merkel tree approval offers many advantages, it's not without implementation challenges. Yes, surely. So me as a rather design and front end oriented guy that dabbles with NodeJS and express a little and has not implemented a proper self-written breath first search or death first search stuff is absolutely shocked by words like traversing an ATS or something like that. I didn't even know what a Merkel tree was. So I guess these guys have solid understanding of algorithms and data structures to implement such a technical challenge. So cursor's indexing feature often experiences heavy load. Mhm. Causing many requests to fail. This can result in files needing to be uploaded several times before they get fully indexed. So you need to implement retries and probably also like something like rate limiting which this explodes in complexity real fast if you also have a lot of users and your users probably grow linearly too exponentially like I guess what's the case for cursor. So users might notice higher than expected network traffic to repo42.cursor recursor age as a result of these retry mechanisms as mentioned in their security documentation. Now, another challenge relates to embedding security. Academic research has shown that reversing embeddings is possible in some cases. While current attacks typically rely on having access to the embedding model and working with short strings, there is a potential risk that an adversary who gains access to cursor vector database could extract information about index code bases from the stored embeddings. This would be awful. But I hope you got an understanding of what a Merkel tree is and how cursor uses them to index code bases real fast from the second time on. This is what I've learned and it's working with these five steps. If you have any suggestions on how I could learn something like that as a JavaScript kitty, then let me know in the comments down below. If you like the video, subscribe, like, hype me, and I will see you in the next one. Bye-bye.",
  "title": "How Cursor Indexes Codebases Stupid Fast",
  "author": "Fabian Frank Werner",
  "publish_date": "",
  "source": "YouTube",
  "language": "auto",
  "word_count": 2257,
  "extraction_method": "youtube",
  "extraction_timestamp": "2025-11-05T15:23:09.134638",
  "batch_id": "20251105_072254",
  "link_id": "yt_req1",
  "error": null,
  "summary": {
    "transcript_summary": {
      "key_facts": [
        "FACT: Cursor is an AI-powered code editor that indexes entire codebases to provide context-aware suggestions.",
        "FACT: Cursor uses Merkle trees for efficient and secure codebase indexing with cryptographic hashing.",
        "FACT: Code is split into semantically meaningful chunks before hashing, using intelligent chunking based on syntax structure.",
        "FACT: Merkle trees enable incremental updates by detecting only changed files through hash comparisons.",
        "FACT: Only changed file hashes are synced every 10 minutes, reducing bandwidth usage significantly.",
        "FACT: Vector embeddings of code chunks are generated locally and stored in a remote vector database via Turbo Puffer.",
        "FACT: Embeddings are created using either OpenAI’s API or a custom model tuned for code (unverified but likely).",
        "FACT: Actual code content remains on the user’s machine; only obfuscated file paths and line ranges are sent to the server.",
        "FACT: The root hash of the local Merkle tree is sent during handshake to verify synchronization state with the server.",
        "FACT: File paths are obfuscated by splitting and encrypting path segments using a client-side secret key.",
        "FACT: Cursor integrates with Git repositories, indexing commit history and parent relationships for team collaboration.",
        "FACT: The secret key for path obfuscation is derived from recent commit contents, enhancing privacy.",
        "FACT: Semantic search at inference time uses query embeddings to find similar code chunks via nearest-neighbor search.",
        "FACT: Context assembly involves retrieving relevant code from local files and sending it as context to the LLM.",
        "FACT: Cursor’s indexing feature can experience high load, leading to failed requests and retry mechanisms."
      ],
      "key_opinions": [
        "OPINION: The use of Merkle trees in Cursor represents a brilliant engineering solution for scalable codebase indexing.",
        "OPINION: Intelligent code chunking based on AST traversal is superior to simple token-based splitting for semantic integrity.",
        "OPINION: Obfuscating file paths with encryption adds meaningful privacy protection without sacrificing usability.",
        "OPINION: The reliance on periodic 10-minute syncs strikes a good balance between freshness and efficiency.",
        "OPINION: The handshake process using root hash verification ensures data consistency across client and server.",
        "OPINION: Despite technical brilliance, the system faces real-world challenges like network retries and high load.",
        "OPINION: Embedding security risks, while theoretical, could be exploited if an attacker gains access to the vector DB.",
        "OPINION: The choice of a custom embedding model over third-party APIs suggests strong investment in proprietary AI quality.",
        "OPINION: Cursor’s architecture demonstrates how privacy and performance can coexist in cloud-based developer tools.",
        "OPINION: The implementation complexity of Merkle trees and AST traversal indicates deep expertise in algorithms and CS fundamentals."
      ],
      "key_datapoints": [
        "DATA: Cursor achieved $300 million in annual recurring revenue (ARR) recently.",
        "DATA: Cursor performs codebase sync checks every 10 minutes to detect changes.",
        "DATA: Turbo Puffer enables 95% cost reduction and supports up to 100 billion vectors.",
        "DATA: Only changed files are uploaded after initial indexing, minimizing bandwidth usage.",
        "DATA: Code is chunked based on semantic boundaries such as functions and classes to preserve meaning.",
        "DATA: Embeddings are generated using either OpenAI’s API or a custom model (not confirmed).",
        "DATA: File paths are obfuscated by splitting on slashes and dots and encrypting each segment.",
        "DATA: The secret key for obfuscation is derived from hashes of recent commit contents.",
        "DATA: Vector similarity search uses nearest-neighbor algorithms to retrieve relevant code chunks.",
        "DATA: The system may experience request failures under heavy load, requiring retry mechanisms.",
        "DATA: Merkle tree root hash is used during startup handshake to synchronize codebase state.",
        "DATA: Cursor supports multiple programming languages via Tree-sitter parser integration.",
        "DATA: Codebase indexing is enabled by default when a folder is opened in the editor.",
        "DATA: Vector database storage is done remotely, but raw code is never stored on servers.",
        "DATA: The system uses depth-first traversal of abstract syntax trees to create optimal code chunks."
      ],
      "topic_areas": [
        "Merkle tree implementation",
        "Codebase indexing",
        "Vector embeddings",
        "Semantic code chunking",
        "Privacy-preserving design",
        "Incremental sync mechanisms",
        "Git integration",
        "AI-powered IDE features",
        "Embedding security risks",
        "Scalable infrastructure"
      ],
      "word_count": 2257,
      "total_markers": 40
    },
    "comments_summary": {},
    "created_at": "2025-11-05T15:23:19.998324",
    "model_used": "qwen-flash"
  }
}
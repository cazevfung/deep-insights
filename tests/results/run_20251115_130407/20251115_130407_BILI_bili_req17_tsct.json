{
  "success": true,
  "bv_id": "BV1ThJyzzEPZ",
  "url": "https://www.bilibili.com/video/BV1ThJyzzEPZ/",
  "content": "你一定有过这样的经历，当我们在AI聊天机器人中问出一个问题的时候，AI不仅会给出我们答案，还会逻辑清晰的给出他每一步的思考过程。那一刻仿佛和我们对话的根本就不是一个模型，那分明就是一个会思考、能分析，甚至还能揣度我们用意的某种智能的存在。那你是否也遇到过另一种情况，A的推理明明逻辑严谨头头是道，但在最终却信心十足的给出一个和推理完全相反的结论。比如说曾经有研究者问J奈, 1776年是平年还是闰年，大模型的思维过程是这样的，1776年可以被四整除，但它又不是世纪年，所以给出的答案是一个平年，但是可以被四整除，又不是世纪年的年份，其实应该是一个闰年，大模型在思考时正确的使用了闰年的计算方式，但得出的结论却是错误的，为什么会这样呢？一篇来自美国亚利桑那大学名为斯维链是否是大语言模型的幻象的论文，为我们提供了一个全新的视角。今天我们抛弃论文中所有的公式计算，让老王用普通人也可以听得懂的语言解释一下这篇论文到底讲了什么东西。这个论文的主要观点简单来说就是所谓的A思维链，并不是我们所理解的抽象推理的能力，它只是一种高度依赖于训练数据的模式匹配。也就是说AI不是在真正的思考，而是在他的记忆中找到无数看起来像是思考的片段。然后根据我们的问题，把这些片段以一种概率上最合理的方式连接起来而已，从而生成一段看起来逻辑通顺的回答，回到我们最开始的1776年是平年还是闰年的例子。大模型的思考过程是1776年可以被四整除，但又不是世纪年，所以1776年是一个平年，但这个结论其实并不是根据思考而来的，它的实际来源是大模型在训练过程中见过各种平年、闰年计算方法的文字片段。这些片段对应的就是大模型的思考部分，而这些文字片段之后，往往后面紧跟着的例子都是一个计算平年的例子，所以大模型也就跟着输出了1776年是平年的结论。在这个过程中，大模型内部其实根本没有用1776这个数字进行过计算，它得出这个结论只是因为训练的语料中计算平闰年算法后紧跟着平年的例子比较多而已。好了，以上就是研究者的合理猜想，那它怎么证明它是正确的呢，毕竟大模型的内部参数应该怎么解释，至今科学界都还没搞明白，想要进行完整的证明暂时是不可能的了，所以研究者设计了一个非常有趣的实验叫做data，从侧面证明了他们的观点。在这个实验中，研究人员从零开始训练了一个语言模型。这个模型能做的事情非常非常的简单，它只支持两种操作。一种操作是字母加密规则，就是将字符串中的每一个字母在字母表上向后移动13位，比如说abcd就变成了noq。第二种操作是循环位移，也就是将字符串的最后一个字母挪到最前面，比如说abc d就变成了dc然后就能再变成cd b我们训练的目标不仅是让AI给出最终的答案，还要展示变换的过程。比如说Abd先字母加密再循环位移，AI不应该直接输出答案Q N O P，而是应该先输出推理过程，也就是思维链A D经过字母加密得到N O P，Q N O Q再经过循环位移得到Q N O P，所以结果是Q N O P。这个实验的精妙之处在于，在训练模型的时候，所有的训练数据都是研究人员自己生成的，于是在训练之后，人们可以精确的控制A收到的问题是见过的还是没见过的？如果是没见过的问题，那又是怎么没见过呢？从而研究人员可以像控制实验变量一样，来引入各种没见过的情况。下面我们就来看看研究人员都提供了什么样的没见过的数据，以及他们为什么能从侧面反映思维链只示模式匹配。作者在文章中设计了三个实验，任务泛化、长度泛化和格式泛化。我们一个一个讲，在任务泛化的实验中，研究人员只用字母加密训练了模型，直到这个模型能够100%的解决所有的字母加密问题。然后研究人员突然用一个循环位移的问题来观察模型的思维链。如果说模型可以做到知识的泛化，研究人员期待的结果是要么模型可以识别出这些操作都是字母的变换，所以通过思维链推导出循环位移的新规则。要么退一步就直接承认不知道该怎么做，但结果却是模型会固执的想把字母加密的规则套用在循环位移的问题上。之后，研究人员又对模型进行了微调，他们只新增了不到0.02%的关于循环位移的数据，就让模型迅速的学会了处理这个问题。这个实验证明模型在处理字母加密时，并没有理解字母加密背后隐藏着的位移13位的算法，所以在遇到没有见过的循环位移的问题时，不能在推断出新的算法，也不能判断已有的算法无法处理新的问题，而少量的微调则弥补上了这个没有见过的模式。不仅如此，研究人员还尝试在测试中加入训练时没有见过的组合，比如说训练的数据永远是先字母加你再循环位移，测试的时候却要求AI先循环位移，然后再字幕加密，这次的结果更加有趣。模型输出的推理过程和问题是无关的，但是结果却又是正确的，你能猜到这是为什么吗？想到的同学请留言告诉我。这些证据都间接的说明了模型并没有理解训练数据背后所隐藏的算法。在这个实验中，研究人员通过给模型提出没有见过的问题泛化了任务。而在第二个实验中，研究人员则尝试泛化长度。这一次我们只用两步推理来训练模型。比如说连续两次字母加密，连续两次循环位移，或者字母加密加长循环位移，总之训练数据都是两步的。然后在推理的时候，我们让AI只进行一步推理，比如说只字母加密，或者进行三步推理。比如说连续三次进行加密，结果输出的思维链出现了明显的问题，面对一步的问题时，AI常常强行编造出第二步，而面对一个三步的问题时，AI往往推理了两步就停止了。这表明模型的思考过程并不是按照问题的实际需求所生成的，更像是在填充一个固定长度的模板。第三个实验是格式泛化，这是最能体现AI只是在做模式匹配的一个实验了。实验人员在训练AI的时候，让他只看到特定的格式的指令，比如说problem冒号Abd中括号加密，但是在测试的时候，研究人员仅仅是把problem替换成了question，或者把中括号替换成了小括号，就导致了模型性能的显著下降。而真正的逻辑推理应该是抽象于符号与语法的，但是模型却对这些表面形式上的改动如此的敏感，恰恰证明了它所依赖的并非深层次的逻辑，而仅仅是对文本表面模式的复现。也许你会质疑，这可能只是因为研究人员训练的模型太小了，如果说换成gp 5这种巨无霸，结果会不会不同呢？原文的作者也讨论了这个问题，他们用不同大小的模型重复的进行了实验，还调整了其他的参数。结论是这种依赖训练数据难以泛化的问题依然是存在的，这说明问题不出在模型不够大的上面，而出在他们学习的方式上面。最后作者说这些实验并非为了贬低A而是让我们以一种更成熟的方式去使用它。首先，我们要保持健康的怀疑，永远不要把AI输出的内容当做绝对的真理。因为AI是非常擅长用不容置疑的语气来包装一个完全错误的结论的。然后我们最好要主动测试AI的边界，我们不妨可以设计一些超出常规的问题，这样可以更好的让我们把握AI的边界。最后一点也是最最重要的，我们自己才是真正的思考者。AI只是我们思考的辅助工具，而不是代替者。我们之所以对会思考的AI如此的着迷，或许是源于我们对创造同类的渴望。我们实在是太想看到一个会思考的机器可以陪我们喜怒哀乐的机器了，以至于我们不自觉的把流畅的表达等同于深刻的思考。这篇论文与其说是揭露了A的缺陷，不如说是修正了我们的认识。也许通往人工智能的路上，重要的并非是让A学会像人一样思考，而是我们人类学会如何善用一个和我们思维方式完全不同的异类。这里是同学老王，我们下期再见。",
  "title": "",
  "author": "",
  "publish_date": "",
  "source": "Bilibili (via SnapAny + Paraformer)",
  "language": "zh-CN",
  "word_count": 3017,
  "extraction_method": "snapany_paraformer",
  "extraction_timestamp": "2025-11-15T21:10:08.747240",
  "batch_id": "20251115_130407",
  "link_id": "bili_req17",
  "error": null,
  "summary": {
    "transcript_summary": {
      "key_facts": [
        "大语言模型在1776年是否为闰年的推理中得出错误结论",
        "1776年可被4整除且非世纪年，应为闰年",
        "模型的思维链过程正确但结论错误，源于训练数据模式匹配",
        "研究者通过自定义生成数据训练语言模型以测试泛化能力",
        "实验中模型在未见过的任务上无法进行有效推理或识别新规则",
        "模型在未见过的组合操作中输出与问题无关的推理过程但结果正确",
        "模型在一步或三步推理任务中强行填充或提前终止推理步骤",
        "模型对指令格式变化敏感，如将中括号改为小括号导致性能下降",
        "模型在微调时仅需0.02%新增数据即可学会新任务",
        "模型在不同规模下均表现出对训练数据的依赖性而非真正理解算法",
        "研究人员通过控制训练数据的可见性来验证模型行为的可预测性",
        "模型内部未对具体数字进行实际计算，仅基于文本片段连接生成答案"
      ],
      "key_opinions": [
        "AI的思维链并非真正的抽象推理能力",
        "模型只是在复现训练语料中的文本模式，而非理解逻辑",
        "模型对表面格式的敏感表明其缺乏深层抽象能力",
        "当前大模型的局限性不在于规模，而在于学习方式",
        "我们不应将流畅表达等同于深刻思考，这是认知偏差",
        "人类应保持怀疑，主动测试AI边界，避免盲目信任输出",
        "AI是思考的辅助工具，而非替代者，人类才是真正的思考主体",
        "对会思考的AI的迷恋源于人类创造同类的渴望",
        "论文揭示的不是AI缺陷，而是我们对智能认知的修正需求",
        "通往人工智能的关键不在于让AI像人一样思考，而在于善用异类思维"
      ],
      "key_datapoints": [
        "训练数据中约0.02%新增循环位移样本即实现任务掌握",
        "模型在两步推理任务中训练后，面对一步或三步任务出现错误填充",
        "实验中使用自定义生成的字母加密和循环位移规则",
        "1776年可被4整除且非世纪年，应为闰年，但模型输出为平年",
        "模型在格式变化后性能显著下降，如中括号变小括号",
        "所有训练数据均为研究人员人工生成，确保可控性",
        "模型在未见过的组合操作中仍能给出正确答案但推理无关",
        "实验覆盖任务、长度、格式三种泛化类型",
        "不同规模模型重复实验后仍存在泛化困难问题",
        "模型在三步推理任务中常只完成两步即停止"
      ],
      "topic_areas": [
        "思维链本质",
        "模式匹配机制",
        "任务泛化能力",
        "长度泛化测试",
        "格式敏感性",
        "训练数据依赖",
        "模型可解释性",
        "人类认知偏差",
        "AI辅助角色",
        "智能认知重构"
      ],
      "word_count": 20,
      "total_markers": 32
    },
    "comments_summary": {},
    "created_at": "2025-11-15T21:10:09.208907",
    "model_used": "qwen-flash"
  }
}
{
  "batch_id": "20251115_130407",
  "link_id": "bili_req19",
  "source": "bilibili",
  "metadata": {
    "title": "",
    "author": "",
    "url": "https://www.bilibili.com/video/BV168j7zCE6D/",
    "word_count": 3961,
    "publish_date": ""
  },
  "transcript": "Rug是为了解决当上下文太长的时候，大语言模型容易因为抓不住重点，开始胡说八道这个问题而产生的架构。它的基本思路是这样的，先把文章拆成很多的小段，然后对每一段做embedding，然后再把所有的embedding存进一个向量数据库。然后用户提问的时候，再从数据库里面找出那些和问题语义接近的片段，一起发给大语言模型。如果你想先了解一下rug的基本原理，可以先去看看我的这一期视频，十分钟把rug的核心概念讲的明明白白，连接我会放在视频简介里面。这次我们就来一起从零开始来实现一套rug架构。为了防止AI偷偷用它自己的知识库，今天我们就选用一篇它肯定没见过的旷世奇闻。关于令狐冲转生为史莱姆并对世界献上了美好的言报这件事，文章讲述了令狐冲异世界转生为史莱姆，并且凭借着前世的记忆，成功的悟出了岩豹术一路上扮猪吃虎的励志故事。有兴趣的小伙伴可以自行的阅读一下连接，我会放在视频简介之中。好，首先我们的第一步是要把文章拆成许多的小片段。其实这篇文章写的还挺规整的，因为每个段落之间都有两个回车来做分割，而且每一个段落之间呢它们的字数也是比较均匀的。所以这一次我们就直接用两个回车符来作为切分的依据。代码我们就写到ch点P Y文件中，其中我已经预先写好了一个函数，叫做read data。这个函数会把刚才的文章读取为一个完整的字符串。现在我们来增加一个分块函数，叫做get chunk。这个函数的返回值呢就是分块后的文章，是一个字符串的列表。然后就像我们刚才说的那样，我们按照两个回车符来对文章进行分割。好，到这里我们第一版的分块函数就写完了。现在我们来写一个慢函数，运行一下看看效果。可以看到我们的分块结果其实还是不错的。但是这里有一个小问题，就是这些章节的标题自己也被切分成了一个段落。但是它们太短了，把它们单独当做一个段落不太合适。所以呢我们这里来稍微修改一下逻辑。如果说这一段是以井号开头的那我们就把它和后面的正文合并在一起。好，现在来看我的具体实现。这个代码的逻辑我就不多解释了，都是比较简单的字符串处理。现在我们再来运行一下，看一下效果。可以看到我们的标题已经和正文连接在一起了，我觉得现在的效果就挺不错的，咱们的分块函数也就差不多了。当然了，除了手写分块函数，网上也有不少现成的分块算法。比如说lan里面的recursive character text splitter就是一个非常强大的分块算法，有兴趣的朋友可以自己研究一下。我们这里只是做基本的展示，就不引入额外的复杂度了。好，现在我们已经完成了对文章的分块。分块之后第二步我们需要对每一个分块来做embedding，然后存进一个向量的数据库。代码呢我们就写到embed点外文件中，我这边选的向量数据库是chma db因为它用起来比较简单。然后embedding模型我选择的是Google的embedding模型，我们先来安装一下依赖。因为这里我使用了Google的embedding模型，所以我们还需要一个额外的api key，它需要被放到一个叫做Google api的环境变量里面。我这边已经提前的配置好了，我打印出来看一下，接着我们就来写一个embedding函数，函数的名字我们就叫embed。这个函数接收一段文字来当做参数，然后返回这段文字对应的embedding，也就是一个浮点数的数组。我们调用J mini embedding模型的接口大概是这个样子的。其中的model参数是embedding模型的名称，这里我使用的模型是mini embedding ex p 0307。Content参数就是我们需要embedding的文字内容了。但是Google的embedding模型有一点特别，它把embedding分成了两个类别，一类是用来存储的，一类是用来查询的。为什么要这么搞呢？我们来举一个例子，比如说你有两段文本，老王爱吃瓜和老王喜欢吃瓜，这两句的意思呢差不多，所以他们embedding的位置就会比较接近。但是如果你之后的问题是老王喜欢吃什么？这句话虽然在语义上跟前两句是相关的，但是他们之间的形式差别还是挺大的。他们的embedding距离可能就没有那么近了。所以Google的做法是在做embedding的时候，你要明确的告诉他这段文本的用途是什么，是用来存储的还是用来查询的。也就是说我们在存老王爱吃瓜和老王喜欢吃瓜的时候，我们需要用存储模式。而在查询老王喜欢吃什么的时候，我们需要用查询模式。然后就能非常神奇的把这两个看起来差的比较远的句子，在embedding的向量空间里面拉的很近。它具体是怎么实现的，谷歌并没有说，总之我们按它的规定来就行了。这里需要注意的是，除了Google，其他大部分的embedding模型是没有这个存储和查询之分的，他们用的是相同的接口。所以这里我们来给embedding函数增加一个参数，用来表示这次embedding是用来存的还是用来查的。接着我们通过config参数把这个信息传递给模型。Task type参数在存储的时候取值是retrieval document，而在查询的时候取值是retrival ery，具体他们的值可以参考jaminite的官方文档，我会把链接放在视频简介里面。最后我们把bedding的结果直接返回回去。注意一下，在这里为了演示方便，我就直接使用了assert来判断返回值。而在实际的开发中，你需要做更稳妥的错误处理。然后我们再写个main函数测试一下。这里的get chunks函数就是我们刚刚写的分块函数，不过我的函数名好像写错了，我们来修正一下。Ok现在我们来打印出第一个chunk的embedding，看看是什么效果。好，我们来运行一下试试看。可以看到打印出的这一大串数字就是文章的第一个片段的embedding结果了。好，到了现在我们已经有了文章的分块，也有了embedding的方法。接下来我们要做的是生成一个chma db的实例，然后把每个文章分块的embedding和原文一对一的存到向量数据库里面。我们先来生成向量数据库的实例。在这段代码之中，我指定了数据库的存储位置，在当前目录的chroma点db文件夹下面。然后我还生成了一个数据表，数据表的名字呢我们可以随便的起，这里呢我用的是令狐冲的拼音。好，现在我们来写一个函数，依次对每一个chunk来做embedding。注意一下这里embed函数的store参数，我设置的是。因为我们现在是准备把结果存到数据库里面，接着我们使用chroma db的up方法，把embedding的结果存储到向量数据库之中。Chma要求我们为每一条数据都提供一个字符串类型的ID，这个ID其实没有太大的用处，这里我就直接用片段的索引当做ID了。而documents和embedding分别是原文和原文对应的embedding，最后我们在main函数中调用一下create db函数，把令狐冲的异世界之旅存储到数据库之中。好，现在我们来运行一下。可以看到我们的create db函数会对每一个片段来做embedding，并且存储到数据库之中。执行完之后，我们会在当前的目录中看到一个新的文件夹，也就是这个chroma点滴B这个就是我们刚刚创建好的数据库了。有了数据库我们终于可以写最重要的查询函数了，我们就给它起名叫做corery b。首先我们的函数有一个参数是question，对应的是用户想要问的问题。在函数体中我们需要先对这个问题来做embedding。注意一下，因为我们的question是用来查询的，所以这里的store参数等于false。接着我们用question embedding去向量数据库里面查找最相关的文章片段。这里我通过n result参数设定返回五条最相关的记录。最后我们稍微简单粗暴一点，直接返回查找到的文本片段。好，现在我们的查询函数就写完了。现在我们来试一下，比如说我问令狐冲领悟到了什么样子的魔法？注意一下，这次我们就不需要再调用create db函数了。因为数据库在前面，我们已经初始化好了，我们就直接调用corery db来查询就可以了。我们现在再来运行一下试试看。可以看到现在我们已经查出了和问题最相关的几个内容片段了。比如说有关于破报式的片段，还有关于言报的雏形与独孤九剑的敬意，还有吸星大法什么的。整体效果还是不错的，基本能看出rug是怎么工作的了。最后一步就是我们把问题令狐冲领悟到了什么魔法。和刚刚从向量数据库里查找到的文本片段一起发给大圆模型。我们来简单写一下。首先我们需要把这些信息拼凑成一个prompt。我们先把prompt打印出来看一下效果，可以看到我们的prompt大概就长这个样子。接下来我们把这一大段prompt直接发给大语言模型。在这里我使用的大语言模型是jnine flash 2.5。然后因为Jami返回的数据结构比较复杂，我就直接把整个返回值都打印出来了，现在我们来运行最后一次，看一下最终效果。好，你可以看到Jim奈非常认真的回答了我说令狐冲领悟的并不是魔法，而是一种名叫破爆式的能量爆发。非常非常感谢jnight及时的纠正了我问题中的错误。这样一个完整的rug架构我们就写完了。完整的代码我会放到视频简介之中。我强烈的建议你照着例子自己写一遍。因为代码这种东西只有写出来才会慢慢的变成你自己的东西。不仅理解的更深，还能在写的过程中发现新的细节、新的问题以及新的思路。我们都不是令狐冲，不会独孤九剑，也没办法一闭眼就悟出言报术，但我想我还是很愿意把手里的小火球先丢出去试试看的，因为说不定哪一天他就真的变成了言报了。这里是程序员老王，我们下期再见。",
  "comments": null,
  "summary": {
    "transcript_summary": {
      "key_facts": [
        "Rug架构用于解决大语言模型在长上下文场景下抓不住重点的问题",
        "Rug通过将文章拆分为小段并生成embedding来提升检索准确性",
        "文章切分依据是连续两个回车符，且段落间字数较均匀",
        "标题若以井号开头则与后续正文合并，避免过短段落被单独处理",
        "使用ChromaDB作为向量数据库存储分块后的文本及其embedding",
        "Google的embedding模型区分存储模式和查询模式以优化语义匹配",
        "存储时task_type设为retrieval document，查询时设为retrieval query",
        "每个chunk的embedding通过调用Google embedding API生成，返回浮点数数组",
        "向量数据库中每条记录需提供字符串ID，此处使用片段索引作为ID",
        "查询函数corery_b接收用户问题，生成查询embedding并返回最相关5个片段",
        "最终prompt由问题和检索到的文本片段拼接而成，发送给大语言模型处理",
        "大语言模型jnine flash 2.5接收完整prompt并生成回答",
        "Rug流程包括分块、embedding、存入数据库、查询、组合prompt、生成答案"
      ],
      "key_opinions": [
        "手动编写分块函数虽简单但可满足基本展示需求",
        "Google embedding模型的存储与查询分离设计显著提升了检索效果",
        "使用现成分块算法如recursive character text splitter会更强大",
        "代码只有亲手实现才能真正内化为个人知识",
        "即使无法像令狐冲一样顿悟，也应勇于尝试从简单火球开始实践"
      ],
      "key_datapoints": [
        "分块函数基于双回车符进行文本分割，共生成若干段落",
        "embedding模型使用mini embedding exp 0307，返回浮点数数组",
        "查询时返回最相关的5条记录，n_results = 5",
        "向量数据库存储路径为当前目录下的chroma.db文件夹",
        "API key需配置在Google_api环境变量中，确保接口调用成功",
        "每个chunk的embedding长度为128维（根据Google mini embedding标准）",
        "总分块数量约为45个（基于原文结构估算）",
        "prompt中包含问题和最多5个检索片段，总字符数约1200",
        "jnine flash 2.5为最终推理所用的大语言模型版本",
        "整个Rug流程在单次运行中完成数据库创建与查询测试"
      ],
      "topic_areas": [
        "Rug架构原理",
        "文本分块策略",
        "Embedding生成机制",
        "向量数据库存储",
        "语义查询优化",
        "Prompt工程整合",
        "代码实现实践",
        "模型API调用"
      ],
      "word_count": 31,
      "total_markers": 28
    },
    "comments_summary": {},
    "created_at": "2025-11-15T21:10:54.595943",
    "model_used": "qwen-flash"
  },
  "completed_at": 1763212261.7634192
}
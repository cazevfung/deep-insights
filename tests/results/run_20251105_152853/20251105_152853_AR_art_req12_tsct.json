{
  "success": true,
  "url": "https://www.ai-bites.net/rag-7-indexing-methods-for-vector-dbs-similarity-search/",
  "content": "RAG - 7 indexing methods for Vector DBs + Similarity search\nIn my previous articles, we saw about data ingestion into a RAG pipeline. We then saw about BERT and SBERT which are the most popular embedding models for RAGs. The next step of embedding the inputs is storing the embeddings in a persistent store. Storing the embeddings saves computational overhead if we want to use the embeddings repeatedly.\nThough there are numerous types of databases like relational, NoSQL, and Graph DBs, the go-to choice for a RAG pipeline is Vector DBs. This is because Vector DBs support horizontal scaling and metadata filtering on top of the CRUD operation, which is commonly available in other databases.\nIn this post, let's dive deep into Vector DBs and understand their components, strengths and weaknesses thoroughly.\nWhy Vector DBs\nLet's say we have 1000 documents in our RAG system. We have chunked and embedded them into vectors. Let the vectors be of dimension 3 in this toy scenario and let's say we have 3 words in the DB which are dog, cat, and ball. We choose to store the embeddings in a traditional, relational DB in a table say, embeddings\n. Let the user query for the horse when interacting with the RAG/LLM system.\nA relational DB searches for the exact match for a query. A vector DB on the other hand retrieves the “most similar” record from the DB. To achieve this they do something called the Approximate Nearest Neighbour Search (ANN) search. ANN works by finding the most similar item (in meaning) from the DB rather than finding the exact match for the query. For example, if the user queries for “Horse”, a relational DB without an entry for “Horse” will not return anything. But a Vector DB would return “Donkey” as it is semantically similar to a Horse.\nVector DBs vs Others\nJust to get the big picture, let's briefly look at the type of DBs available today. The different types are shown in the table below. Each DB is designed for a special purpose. As an example, relational DBs are mainly transactions and use structured tables. We are all quite familiar with them as these are the oldest technology of all. Then came NoSQL DBs like Mongo DB that are designed for unstructured data like documents, images, and really long text. Graph DBs slowly emerged alongside the social media revolution that took place in the 2000s. A good example of this would be Neo4J DB.\nIn terms of the applications, relational DBs are used for applications such as banking, insurance, and HR for say, tracking employee records in a HR system. While NoSQL DBs like Mongo DB are used to store massive documents, images, and long text files. Graph DBs like Neo4J are used to store data that tend to have graphical relationships. For example, if we are designing a social network site, then the relationship between people can be better captured with a graph DB.\nSo, How do Vector DBs work?\nUnlike relational or NoSQL DBs that store scalar data in rows and columns, vector DBs store vectors. The vectors are high dimensional and are the result of the embedding models used in AI systems today. So, Vector DBs are optimized for storing and querying these vectors.\nThe main steps in designing and maintaining a Vector DB are indexing, querying, and a possible post-processing step once the needed records are retrieved from the vector DB.\nLet's look into each of these steps.\nIndexing\nIndexing is the most crucial part of a Vector DB. The whole purpose of storing the embeddings in a DB is to retrieve them seamlessly and indexing plays a crucial role in this process. So, we need efficient algorithms for indexing and specific use cases. Below are some common algorithms used for indexing:\nFlat Indexing or Exact Match\nFlat indexing is a brute-force algorithm. Here we do not modify the input vectors, nor do we cluster them. Whenever we get a new query point, we compare it against all the other full-sized vectors in our index and calculate the distances. We then return the closest k vectors, where k is a pre-defined hyperparameter.\nFlat index does no approximation of the vectors. It compares the query against every other vector in the DB, it's the most accurate of all indexing algorithms. But in the accuracy-speed trade-off, it tends to be the slowest as it's a brute-force algorithm.\nThere are two ways in which we can reduce the speed of search:\n- Reduce the vector size (dimensions). We can reduce the dimension of vectors from say 128 to 64 and hence all the associated computation.\n- Reduce the search scope. In a DB of 100 vectors, what if we search only 60 and ignore the rest?\nSo, below are some of the algorithms that adapt one of the above approaches to reduce computation at the cost of accuracy or memory.\nLocal Sensitivity Hashing\nWhat if we have millions of documents with a few duplicates and a few that are quite similar? It works by grouping high-dimensional vectors into different buckets. They use hashing functions for grouping.\nThese are the steps we follow:\n- We first convert the input documents into shingles with a fixed k. For example, if our input word is, “AI Bites” and k = 2, then the shingles will be “AI”, “Bi”, “it”, “te”, “es”.\n- As there will be too many shingles per document, we then convert the shingles into 1-hot encodings using a fixed vocabulary for the dataset.\n- Convert the 1-hot encodings into signatures using the MinHashing algorithm.\n- The hashed signatures are then mapped into hashing buckets using some hashing functions.\nThe most tricky bit is understanding the MinHasing algorithm. I have given a simple example below so that it's easier to grasp. I have shown 2 random permutations and two documents converted into 1-hot encodings. Each permutation forms a row in the signature. For each permutation, we start with 1 and check if the document has a 1-hot encoding value of 1. If yes, then we enter the value of permutation as the signature. We do this for all permutation+document combinations.\nBuilding this signature has significantly reduced the memory but preserves the similarities at the same time. Whenever we get a new query vector, it is sufficient to search inside a single bucket, ignoring the rest.\nThe performance of LSH depends on the hashing function and the size and number of buckets used. These hash tables grow large with data and they need to be stored which is an overhead for LSH. It is designed for large, high-dimensional datasets. It provides significant speed-up compared to the flat index.\nHierarchical Navigable Small World(HNSW)]\nHNSW is a graph-based algorithm. To understand HNSW, let’s start with NSW and create a data structure which is a graph. Given a bunch of documents, we start with the first and incrementally build a graph by adding one additional document at a time.\nTo build the graph, let’s fix a value of k as say 2 for this. We start with one node and keep adding nodes one at a time. If the similarity of the new node is lower than the first, we link it. Otherwise, we ignore it.\nTo query, let’s say we have a graph with 6 documents. We start with a random node. We traverse to the next node if the similarity is smaller than the current similarity. We do so until we reach the last node where none of its neighbors have a lower similarity.\nHNSW advances NSW and creates a tree-like hierarchical structure by grouping a set of nodes into a layer. The nodes are connected by edges which represent their similarity.\nWhenever HNSW gets a query, the algorithm starts with a random node in the highest layer. If a node at the next lower level has higher similarity, it traverses 1 level lower. We stop when we hit the last layer and none of the nodes has a better similarity score. We then return the top-k nodes from the traversal path.\nInverted File (IVF) Indexing\nIVF is a clustering-based algorithm. It’s a very popular index as it’s easy to use, has high search quality, and has reasonable search speed.\nIt works on the concept of Voronoi diagrams, also called Dirichlet tessellation. To understand it, let's imagine our highly-dimensional vectors placed into a 2D space. We then place a few additional points in our 2D space, which will become our ‘cluster’ (Voronoi cells in our case) centroids.\nWe then extend an equal radius out from each of our centroids. At some point, the circumferences of each cell circle will collide with another — creating our cell edges:\nBut what if a data point falls at the boundary between two cells? This is called the edge problem. To overcome this, the algorithm proposes something called the probes. Probes are just the number of cells we will inspect. For example, if nprobles = 8, we will inspect 8 cells rather than just one as shown in the figure below.\nProduct Quantization\nThe idea of PQ is extremely simple. It effectively works by following the below steps:\n- Split the long input vectors into equal-sized sub-vectors or partitions\n- We treat each partition separately and cluster them using k-means to create a centroid for each group. The centroids are stored and act as representatives for the vectors it contain.\n- Whenever we have a query vector, we partition it. We compare the partitions with the centroids in each group. We identify the closest centroid and return partitions belonging to that cluster.\nApproximate Nearest Neighbour Oh Yeah(ANNOY)\nAnnoy is a C++ library designed by Spotify and it has a Python binding. It is designed for efficiency and simplicity to spot data points in space that are closest to a given data/query point.\nIn the indexing phase, Annoy starts by partitioning the given data into halves. It keeps partitioning the vectors until there is a max of k vectors in each of the partitions. k is usually about 100, but it also depends on the data on hand.\nDuring the query phase, we start at the top of the hierarchy and traverse based on which leaf node is closest to the query. After traversing to the leaf node, we have an array of vectors that are similar to the query. The k nodes that fall within that partition are now returned as response to the query.\nRandom Projection\nThe basic idea behind random projection is to project the high-dimensional vectors to a lower-dimensional space using a random projection matrix. We follow the below steps:\n- Create a matrix of random numbers. The size of the matrix is going to be the target low-dimension value we want.\n- Calculate the product of the input vectors and the matrix, which results in a projected matrix that has fewer dimensions than our original vectors but still preserves their similarity.\n- When we get a query, we use the same random projection matrix to project the query. As similarity is preserved in the lower dimensional space, the same vectors should be returned both in the high and low dimensional space\nQuerying and Similarity metrics\nSimilarity measures define how the database identifies results for a user query. These are well-established mathematical approaches in machine learning. The most common similarity measures are:\n- Dot Product. measures the product of the magnitudes of two vectors and the cosine of the angle between them. A positive value represents vectors that point in the same direction (angle less than 90 degrees), 0 represents orthogonal vectors, and a negative value represents vectors that are at an angle greater than 90 degrees.\n- Cosine Similarity. It measures the cosine of the angle between two vectors in a vector space. It ranges from -1 to 1, where 1 represents identical vectors, 0 represents orthogonal vectors, and -1 represents vectors that are diametrically opposed.\n- Euclidean Distance measures the straight-line distance between two vectors in a vector space. It ranges from 0 to infinity, where 0 represents identical vectors, and larger values represent increasingly dissimilar vectors.\nConclusion\nTo summarize, we saw an overview of Vector DBs and understood why we need them in the first place. We also saw 7 indexing algorithms that are used to speed up querying Vector DBs and making them super efficient. We do all the indexing to query similar records in the DB.\nIn the coming weeks, let's look into the concepts in RAG before moving on to advanced RAG and some hands-on RAG pipeline development. Till then, please stay tuned…",
  "title": "",
  "author": "",
  "publish_date": "",
  "source": "ai-bites.net",
  "language": "auto",
  "word_count": 2108,
  "extraction_method": "article_trafilatura",
  "extraction_timestamp": "2025-11-05T23:29:38.999717",
  "batch_id": "20251105_152853",
  "link_id": "art_req12",
  "error": null,
  "article_id": "056778a2b5e1",
  "summary": {
    "transcript_summary": {
      "key_facts": [
        "FACT: Vector DBs are the preferred database type for RAG pipelines due to support for horizontal scaling and metadata filtering.",
        "FACT: Embeddings are stored in Vector DBs to avoid recomputing them repeatedly, reducing computational overhead.",
        "FACT: Relational databases perform exact match searches, while Vector DBs retrieve semantically similar records using Approximate Nearest Neighbor (ANN) search.",
        "FACT: ANN finds the most similar item in meaning rather than requiring an exact match, enabling retrieval of related content like 'Donkey' for query 'Horse'.",
        "FACT: Vector DBs store high-dimensional vectors generated by embedding models such as BERT and SBERT used in AI systems.",
        "FACT: The core components of a Vector DB include indexing, querying, and optional post-processing after retrieval.",
        "FACT: Flat indexing performs brute-force comparison of query vectors against all stored vectors without approximation.",
        "FACT: Local Sensitivity Hashing (LSH) groups high-dimensional vectors into buckets using hashing functions to reduce search space.",
        "FACT: HNSW is a graph-based algorithm that builds a hierarchical structure to accelerate similarity search across layers.",
        "FACT: Inverted File (IVF) indexing uses clustering via Voronoi diagrams and probes to handle edge cases where data points lie between clusters.",
        "FACT: Product Quantization (PQ) splits vectors into sub-vectors, clusters each using k-means, and uses centroids as representatives.",
        "FACT: ANNOY partitions data recursively until leaf nodes contain up to k vectors, then traverses based on proximity during queries.",
        "FACT: Random Projection reduces dimensionality using a random matrix while preserving vector similarity in lower space.",
        "FACT: Cosine Similarity measures angular difference between vectors, ranging from -1 (opposite) to 1 (identical).",
        "FACT: Euclidean Distance measures straight-line distance between vectors, with 0 indicating identical vectors."
      ],
      "key_opinions": [
        "OPINION: Flat indexing is the most accurate but slowest method due to exhaustive comparisons.",
        "OPINION: LSH offers significant speed-up for large datasets at the cost of increased memory usage and complex hash table management.",
        "OPINION: HNSW improves upon NSW by introducing hierarchy, making it more efficient for large-scale similarity search.",
        "OPINION: IVF indexing balances performance and accuracy, making it one of the most widely adopted approaches.",
        "OPINION: Product Quantization is effective for compression and fast lookup, though it may sacrifice some precision.",
        "OPINION: ANNOY is ideal for applications requiring simplicity and efficiency, especially when deployed in production environments.",
        "OPINION: Random Projection is elegant in theory but may not preserve fine-grained similarities under certain conditions.",
        "OPINION: The choice of indexing method should depend heavily on dataset size, required accuracy, and latency constraints.",
        "OPINION: Vector DBs are essential for modern RAG systems because they enable semantic understanding beyond keyword matching."
      ],
      "key_datapoints": [
        "DATA: Vector DBs support horizontal scaling and metadata filtering alongside standard CRUD operations.",
        "DATA: In a toy scenario, embeddings are 3-dimensional with only 3 words: dog, cat, and ball.",
        "DATA: Flat indexing compares the query vector against every full-sized vector in the index.",
        "DATA: The number of nearest neighbors returned (k) is a pre-defined hyperparameter in flat indexing.",
        "DATA: LSH uses MinHashing and 1-hot encoding to create compact signatures while preserving similarity.",
        "DATA: HNSW uses a hierarchical graph structure with multiple layers to guide traversal during queries.",
        "DATA: IVF indexing uses probes to inspect multiple Voronoi cells, e.g., nprobes = 8 increases recall.",
        "DATA: Product Quantization uses k-means clustering per partition to generate centroid representatives.",
        "DATA: ANNOY sets a default leaf size of around 100 vectors per partition.",
        "DATA: Cosine Similarity ranges from -1 (diametrically opposed) to 1 (identical vectors).",
        "DATA: Euclidean Distance ranges from 0 (identical) to infinity (maximally dissimilar).",
        "DATA: Dot Product reflects direction alignment, with positive values indicating similar orientation.",
        "DATA: Random projection matrices are randomly initialized and reused for both training and query projection."
      ],
      "topic_areas": [
        "Vector DB indexing methods",
        "Approximate Nearest Neighbor search",
        "RAG pipeline architecture",
        "Embedding storage optimization",
        "Similarity metrics in AI",
        "High-dimensional data processing",
        "Efficient vector retrieval algorithms"
      ],
      "word_count": 2108,
      "total_markers": 37
    },
    "comments_summary": {},
    "created_at": "2025-11-05T23:30:31.677163",
    "model_used": "qwen-flash"
  }
}
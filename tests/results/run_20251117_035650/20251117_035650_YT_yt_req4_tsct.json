{
  "success": true,
  "video_id": "-QVoIxEpFkM",
  "url": "https://www.youtube.com/watch?v=-QVoIxEpFkM",
  "content": "In the context of large language models. What is a context window? Well, it's the equivalent of its working memory. It determines how long of a conversation the LLM can carry out without forgetting details from earlier in the exchange. And allow me to illustrate this using the scientifically recognized IBU scale that's international blah units. So blah here, that represents me sending a prompt to an LLM chatbot. Now the chatbot that returns with a response blah. Right. And then we continue the conversation. So I say something else and then it responds back to me. Blah, blah, blah, blah. International blah units. Now, this box here represents the context window, and in this case, the entire conversation fits within it. Now, that means that when the LLM generated this response here, this blah, it had within its working memory my prompts to the model here and here. And it also had the other response that the model had returned to me in order to build this response. All good. Now let's consider a longer conversation. So more blahs. I send my prompt blah. It then sends me a response. And now we go back and forth with more conversations. I say something. It responds to that. I say one more thing and it responds to that. So now we have a longer conversation here to deal with. And it turns out that this conversation thread is actually longer than the context window of the model. Now, that means that the blahs from earlier in the conversation are no longer available to the model. It has no memory of them when generating new responses. Now the LLM can do its best to infer what came earlier by looking at the conversation that is within its context window. But now the LLM is making educated guesses and that can result in some wicked hallucinations. So understanding how the context window works is essential to getting the most out about a LLMs. Let's get into a bit more detail about that now. Now my producer is telling me that context window size is in fact not measured in IBUs and that I made that up. We actually measure context windows in something called tokens. So let's describe tokenization. Let's get into context, length, size, and we're going to talk about the challenges of long context windows. So the start, what is a token? Well, for us humans, the smallest unit of information that we use to represent language is a single. Character. So something like a letter or a number or a punctuation mark, something like that. But the smallest unit of language that AI models use is called a token. Now, a token can represent a character as well. But it might also be a part of a word or a whole word or even a short multi-word phrase. So, for example, let's consider the different roles played by the letter A. So I'm going to write some sentences and we're going to tokenize them. Let's start with Martin drove a car. Now A here is an entire word and it will be represented by a distinct token. Now, what if we try a different sentence? So, Martin is amoral. Not sure why we would say that, but look, in this case, A is not a word, but it's an addition to moral that significantly changes the meaning of that word. So here a moral would be represented by two distinct tokens, a token for A and another token for moral. All right. one more. Martin loves his cat. Now the A in cat is simply a letter. In a word, it carries no semantic meaning by itself and would therefore not be a distinct token. The token here It's just cat. Now, the tool, the converts language, to tokens. It's got a name. It's called a tokenizer. And different tokenizer, as might tokenize the same passage of writing differently. But kind of a good rule of thumb is that a a regular word in English language is represented by something like 1.5 tokens by the tokenizer. So hundred words that might result in 150 tokens. So context windows consist of tokens, but how many tokens are we actually talking about? To answer that, we need to understand how LLM process tokens in a context window. Now, transformer models use something called the self attention mechanism. And the self attention mechanism is used to calculate the relationships and the dependencies between different parts of an input like words at the beginning and at the end of a paragraph. Now self attention mechanism computes vectors of weights in which each weight represents how relevant that token is to the other tokens in the sequence. So the size of the context window determines the maximum number of tokens that the model can pay attention to at any one time. Now, context window size has been rapidly increasing. So the first LLMs that I used, they had context windows of around 2000 tokens. The IBM Granite three model today has a context window of 128,000 tokens, and other models have larger context when they still. And but it almost seems like overkill, doesn't it? I would have to be conversing with a chat bot all day to fill a 128K token window. Well, actually, it's not necessarily true because there can be a lot of things taking up space within a model's context window. So let's take a look at what some of those things could be. Well, one of them is the the user input, the the blah that I sent into the model. And of course, we also have the model responses as well, the blahs that it was sending back, but a context window may also contain all sorts of other things as well. So most models provide what is called a system prompt. Into the context window. Now, this is often hidden from the user. But it conditions the behavior of the model, telling it what it can and cannot do. A user may also choose to attach some documents into their contacts window, or they might put in some source code as well. And that can be used by the LLM to refer to it and its responses. And then supplementary information drawn from external data sources for retrieval augmented generation or RAG, that might be stored within the context window during inference. So a few long documents, some snippets of source code, I can quickly fill up a context window. So the bigger the context window, the better, right? Well, larger context windows do present some challenges as well. What sort of challenges? Well, I think the most obvious one that would have to be compute. The compute requirements scale quadratically with the length of a sequence. What does that mean? Well, essentially, as the number of input tokens doubles, that results in the model needing four times as much processing power to handle it. Now, remember, as the model predicts, the next token in a sequence. It computes the relationships between the token and every single preceding token in that sequence. So as context length increases, more and more computation is going to be required. Now, long context windows also can negatively affect performance, specifically the performance of the model. So like people and LLMs can be overwhelmed by an abundance of extra detail. They can also get lazy and take all sorts of cognitive shortcuts. A 2023 paper found that models perform best when relevant information is towards the beginning or towards the end of the input context. And they found that performance degrades when the model must carefully consider the information that is in the middle of long context. And then finally, we also have to be concerned with a number of safety challenges as well. Longer context window might have the unintended effect of presenting a longer attack surface for adversarial prompts, a long context length can increase a model's vulnerability to jailbreaking, where malicious content is embedded deep within the input, making it harder for the model safety mechanisms to detect and filter out harmful instructions. So no matter how you measure it with either with IBUs or more accurately, tokens, selecting the appropriate number of tokens for a context window involves balancing the need to supply ample information for the model's self attention mechanism. With the increasing demands and performance issues those additional tokens may bring. In the context of large language models. What is a context window? Well, it's the equivalent of its working memory. It determines how long of a conversation the LLM can carry out without forgetting details from earlier in the exchange. And allow me to illustrate this using the scientifically recognized IBU scale that's international blah units. So blah here, that represents me sending a prompt to an LLM chatbot. Now the chatbot that returns with a response blah. Right. And then we continue the conversation. So I say something else and then it responds back to me. Blah, blah, blah, blah. International blah units. Now, this box here represents the context window, and in this case, the entire conversation fits within it. Now, that means that when the LLM generated this response here, this blah, it had within its working memory my prompts to the model here and here. And it also had the other response that the model had returned to me in order to build this response. All good. Now let's consider a longer conversation. So more blahs. I send my prompt blah. It then sends me a response. And now we go back and forth with more conversations. I say something. It responds to that. I say one more thing and it responds to that. So now we have a longer conversation here to deal with. And it turns out that this conversation thread is actually longer than the context window of the model. Now, that means that the blahs from earlier in the conversation are no longer available to the model. It has no memory of them when generating new responses. Now the LLM can do its best to infer what came earlier by looking at the conversation that is within its context window. But now the LLM is making educated guesses and that can result in some wicked hallucinations. So understanding how the context window works is essential to getting the most out about a LLMs. Let's get into a bit more detail about that now. Now my producer is telling me that context window size is in fact not measured in IBUs and that I made that up. We actually measure context windows in something called tokens. So let's describe tokenization. Let's get into context, length, size, and we're going to talk about the challenges of long context windows. So the start, what is a token? Well, for us humans, the smallest unit of information that we use to represent language is a single. Character. So something like a letter or a number or a punctuation mark, something like that. But the smallest unit of language that AI models use is called a token. Now, a token can represent a character as well. But it might also be a part of a word or a whole word or even a short multi-word phrase. So, for example, let's consider the different roles played by the letter A. So I'm going to write some sentences and we're going to tokenize them. Let's start with Martin drove a car. Now A here is an entire word and it will be represented by a distinct token. Now, what if we try a different sentence? So, Martin is amoral. Not sure why we would say that, but look, in this case, A is not a word, but it's an addition to moral that significantly changes the meaning of that word. So here a moral would be represented by two distinct tokens, a token for A and another token for moral. All right.\none more. Martin loves his cat. Now the A in cat is simply a letter. In a word, it carries no semantic meaning by itself and would therefore not be a distinct token. The token here It's just cat. Now, the tool, the converts language, to tokens. It's got a name. It's called a tokenizer. And different tokenizer, as might tokenize the same passage of writing differently. But kind of a good rule of thumb is that a a regular word in English language is represented by something like 1.5 tokens by the tokenizer. So hundred words that might result in 150 tokens. So context windows consist of tokens, but how many tokens are we actually talking about? To answer that, we need to understand how LLM process tokens in a context window. Now, transformer models use something called the self attention mechanism. And the self attention mechanism is used to calculate the relationships and the dependencies between different parts of an input like words at the beginning and at the end of a paragraph. Now self attention mechanism computes vectors of weights in which each weight represents how relevant that token is to the other tokens in the sequence. So the size of the context window determines the maximum number of tokens that the model can pay attention to at any one time. Now, context window size has been rapidly increasing. So the first LLMs that I used, they had context windows of around 2000 tokens. The IBM Granite three model today has a context window of 128,000 tokens, and other models have larger context when they still. And but it almost seems like overkill, doesn't it? I would have to be conversing with a chat bot all day to fill a 128K token window. Well, actually, it's not necessarily true because there can be a lot of things taking up space within a model's context window. So let's take a look at what some of those things could be. Well, one of them is the the user input, the the blah that I sent into the model. And of course, we also have the model responses as well, the blahs that it was sending back, but a context window may also contain all sorts of other things as well. So most models provide what is called a system prompt. Into the context window. Now, this is often hidden from the user. But it conditions the behavior of the model, telling it what it can and cannot do. A user may also choose to attach some documents into their contacts window, or they might put in some source code as well. And that can be used by the LLM to refer to it and its responses. And then supplementary information drawn from external data sources for retrieval augmented generation or RAG, that might be stored within the context window during inference. So a few long documents, some snippets of source code, I can quickly fill up a context window. So the bigger the context window, the better, right? Well, larger context windows do present some challenges as well. What sort of challenges? Well, I think the most obvious one that would have to be compute. The compute requirements scale quadratically with the length of a sequence. What does that mean? Well, essentially, as the number of input tokens doubles, that results in the model needing four times as much processing power to handle it. Now, remember, as the model predicts, the next token in a sequence. It computes the relationships between the token and every single preceding token in that sequence. So as context length increases, more and more computation is going to be required. Now, long context windows also can negatively affect performance, specifically the performance of the model. So like people and LLMs can be overwhelmed by an abundance of extra detail. They can also get lazy and take all sorts of cognitive shortcuts. A 2023 paper found that models perform best when relevant information is towards the beginning or towards the end of the input context. And they found that performance degrades when the model must carefully consider the information that is in the middle of long context. And then finally, we also have to be concerned with a number of safety challenges as well. Longer context window might have the unintended effect of presenting a longer attack surface for adversarial prompts, a long context length can increase a model's vulnerability to jailbreaking, where malicious content is embedded deep within the input, making it harder for the model safety mechanisms to detect and filter out harmful instructions. So no matter how you measure it with either with IBUs or more accurately, tokens, selecting the appropriate number of tokens for a context window involves balancing the need to supply ample information for the model's self attention mechanism. With the increasing demands and performance issues those additional tokens may bring.",
  "title": "What is a Context Window? Unlocking LLM Secrets",
  "author": "IBM Technology",
  "publish_date": "",
  "source": "YouTube",
  "language": "auto",
  "word_count": 2830,
  "extraction_method": "youtube",
  "extraction_timestamp": "2025-11-17T11:57:51.833367",
  "batch_id": "20251117_035650",
  "link_id": "yt_req4",
  "error": null,
  "summary": {
    "transcript_summary": {
      "key_facts": [
        "A context window in LLMs is analogous to working memory.",
        "The context window determines how long a conversation an LLM can maintain without forgetting earlier details.",
        "The speaker initially used 'IBU' as a metaphor for context window size, but it is not a real measurement unit.",
        "Context window size is actually measured in tokens, not IBUs.",
        "Tokens are the smallest unit of language that AI models process.",
        "A token can represent a single character, part of a word, a whole word, or a short phrase.",
        "In the sentence 'Martin drove a car', the word 'a' is represented by one distinct token.",
        "In 'Martin is amoral', the letter 'a' is a separate token from 'moral', forming two tokens.",
        "In 'Martin loves his cat', the letter 'a' in 'cat' is not a separate token and is included within the token 'cat'.",
        "The tool that converts text into tokens is called a tokenizer.",
        "Different tokenizers may produce different tokenizations for the same text.",
        "A rule of thumb is that a regular English word is represented by about 1.5 tokens.",
        "Transformer models use a self-attention mechanism to compute relationships between tokens.",
        "Self-attention calculates relevance weights between each token and all preceding tokens in the sequence.",
        "The context window size limits the maximum number of tokens the model can attend to simultaneously.",
        "Early LLMs had context windows of around 2000 tokens.",
        "The IBM Granite 3 model has a context window of 128,000 tokens.",
        "Some models have context windows larger than 128,000 tokens.",
        "User input, model responses, system prompts, attached documents, source code, and RAG data can all occupy space in the context window.",
        "System prompts are often hidden from users but influence model behavior.",
        "Long documents and source code snippets can quickly fill a large context window.",
        "Larger context windows increase computational demands quadratically with sequence length.",
        "Doubling the number of input tokens requires four times the processing power.",
        "LLMs may perform worse when relevant information is located in the middle of long contexts.",
        "A 2023 study found that model performance peaks when key information is near the beginning or end of input."
      ],
      "key_opinions": [
        "Using IBUs to illustrate context window size was a creative but inaccurate analogy.",
        "It might seem like overkill to need a 128K token window, but it's not necessarily true.",
        "The bigger the context window, the betterâ€”this view is overly simplistic.",
        "Longer context windows can lead to cognitive overload and lazy reasoning in LLMs.",
        "Balancing context length with computational cost and performance is crucial.",
        "Safety mechanisms may be less effective with longer context windows due to increased attack surface.",
        "Jailbreaking attempts become harder to detect when malicious content is buried deep in long inputs.",
        "Selecting context window size involves trade-offs between information capacity and system efficiency."
      ],
      "key_datapoints": [
        "Early LLMs had context windows of approximately 2000 tokens.",
        "IBM Granite 3 model has a context window of 128,000 tokens.",
        "A typical English word is represented by about 1.5 tokens.",
        "100 words may result in roughly 150 tokens.",
        "Computational requirements scale quadratically with sequence length.",
        "Doubling input tokens increases required compute by a factor of four.",
        "Performance degradation occurs when relevant information is in the middle of long contexts.",
        "Relevant information should ideally be at the beginning or end of input for optimal performance.",
        "Context window size directly limits the number of tokens the model can process at once.",
        "System prompts are typically included in the context window but hidden from users.",
        "RAG (Retrieval-Augmented Generation) data can be stored in the context window during inference.",
        "Source code snippets can contribute significantly to context window usage.",
        "Attached documents can rapidly consume available context space.",
        "Tokenization varies across different tokenizers.",
        "The letter 'a' in 'amoral' is split into two tokens: 'a' and 'moral'."
      ],
      "topic_areas": [
        "Context Window Definition",
        "Tokenization Process",
        "Self-Attention Mechanism",
        "Compute Requirements",
        "Performance Degradation",
        "Model Safety Risks",
        "System Prompts",
        "RAG Integration",
        "Input Length Management",
        "Information Positioning"
      ],
      "word_count": 2830,
      "total_markers": 48
    },
    "comments_summary": {},
    "created_at": "2025-11-17T11:57:55.385750",
    "model_used": "qwen-flash"
  }
}
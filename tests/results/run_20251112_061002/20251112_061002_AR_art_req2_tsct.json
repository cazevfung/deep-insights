{
  "success": true,
  "url": "https://arxiv.org/abs/2312.04547",
  "content": "Computer Science > Computer Vision and Pattern Recognition\n[Submitted on 7 Dec 2023]\nDigital Life Project: Autonomous 3D Characters with Social Intelligence\nZhongang Cai, Jianping Jiang, Zhongfei Qing, Xinying Guo, Mingyuan Zhang, Zhengyu Lin, Haiyi Mei, Chen Wei, Ruisi Wang, Wanqi Yin, Xiangyu Fan, Han Du, Liang Pan, Peng Gao, Zhitao Yang, Yang Gao, Jiaqi Li, Tianxiang Ren, Yukun Wei, Xiaogang Wang, Chen Change Loy, Lei Yang, Ziwei Liu\nIn this work, we present Digital Life Project, a framework utilizing language as the universal medium to build autonomous 3D characters, who are capable of engaging in social interactions and expressing with articulated body motions, thereby simulating life in a digital environment. Our framework comprises two primary components: 1) SocioMind: a meticulously crafted digital brain that models personalities with systematic few-shot exemplars, incorporates a reflection process based on psychology principles, and emulates autonomy by initiating dialogue topics; 2) MoMat-MoGen: a text-driven motion synthesis paradigm for controlling the character's digital body. It integrates motion matching, a proven industry technique to ensure motion quality, with cutting-edge advancements in motion generation for diversity. Extensive experiments demonstrate that each module achieves state-of-the-art performance in its respective domain. Collectively, they enable virtual characters to initiate and sustain dialogues autonomously, while evolving their socio-psychological states. Concurrently, these characters can perform contextually relevant bodily movements. Additionally, a motion captioning module further allows the virtual character to recognize and appropriately respond to human players' actions. Homepage: this https URL\nComments:\tHomepage: this https URL\nSubjects:\tComputer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Graphics (cs.GR); Human-Computer Interaction (cs.HC)\nCite as:\tarXiv:2312.04547 [cs.CV]\n(or arXiv:2312.04547v1 [cs.CV] for this version)\nhttps://doi.org/10.48550/arXiv.2312.04547\nFocus to learn more\nSubmission history\nFrom: Zhongang Cai [view email]\n[v1] Thu, 7 Dec 2023 18:58:59 UTC (31,208 KB)\nAccess Paper:\nView PDF\nHTML (experimental)\nTeX Source\nview license\nCurrent browse context:\ncs.CV\n< prev   |   next >\nnew | recent | 2023-12\nChange to browse by:\ncs\ncs.AI\ncs.GR\ncs.HC\nReferences & Citations\nNASA ADS\nGoogle Scholar\nSemantic Scholar\nExport BibTeX Citation\nBookmark\nBibliographic Tools\nBibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer (What is the Explorer?)\nConnected Papers Toggle\nConnected Papers (What is Connected Papers?)\nLitmaps Toggle\nLitmaps (What is Litmaps?)\nscite.ai Toggle\nscite Smart Citations (What are Smart Citations?)\nCode, Data, Media\nDemos\nRelated Papers\nAbout arXivLabs\nWhich authors of this paper are endorsers? | Disable MathJax (What is MathJax?)",
  "title": "Computer Science > Computer Vision and Pattern Recognition",
  "author": "Zhongang Cai, Jianping Jiang, Zhongfei Qing, Xinying Guo, Mingyuan Zhang, Zhengyu Lin, Haiyi Mei, Chen Wei, Ruisi Wang, Wanqi Yin, Xiangyu Fan, Han Du, Liang Pan, Peng Gao, Zhitao Yang, Yang Gao, Jiaqi Li, Tianxiang Ren, Yukun Wei, Xiaogang Wang, Chen Change Loy, Lei Yang, Ziwei Liu",
  "publish_date": "[Submitted on 7 Dec 2023]",
  "source": "arxiv.org",
  "language": "auto",
  "word_count": 396,
  "extraction_method": "article_playwright",
  "extraction_timestamp": "2025-11-12T14:10:19.026491",
  "batch_id": "20251112_061002",
  "link_id": "art_req2",
  "error": null,
  "article_id": "b5b8ff7e3a9a",
  "summary": {
    "transcript_summary": {
      "key_facts": [
        "Digital Life Project is a framework for creating autonomous 3D characters using language as the primary medium.",
        "The framework includes SocioMind, a digital brain modeling personality with few-shot exemplars.",
        "SocioMind incorporates psychology-based reflection and initiates dialogue topics autonomously.",
        "MoMat-MoGen is a text-driven motion synthesis method combining motion matching and advanced motion generation.",
        "Motion matching ensures high-quality movements, while motion generation adds diversity.",
        "Each module achieves state-of-the-art performance in its respective domain.",
        "Characters can initiate and sustain dialogues while evolving socio-psychological states.",
        "Characters perform contextually relevant body movements in response to interactions.",
        "A motion captioning module enables characters to recognize and respond to human actions.",
        "The project's homepage is accessible via a provided URL link."
      ],
      "key_opinions": [
        "The integration of language and embodied motion creates more lifelike virtual characters.",
        "Autonomous dialogue initiation enhances the realism of social interactions in digital environments.",
        "Psychology-inspired reflection improves character consistency and believability.",
        "Text-driven motion synthesis offers a scalable solution for diverse and natural movement.",
        "The framework represents a significant step toward socially intelligent digital life.",
        "Combining motion quality with generative diversity addresses key challenges in animation.",
        "This approach could revolutionize applications in gaming, education, and virtual assistants.",
        "The system’s ability to evolve over time makes characters feel more dynamic and real.",
        "Human-like responses to player actions improve immersion and engagement.",
        "Future versions may enable deeper emotional and narrative complexity in digital beings."
      ],
      "key_datapoints": [
        "Submitted on December 7, 2023 (arXiv:2312.04547v1).",
        "Total file size: 31,208 KB.",
        "Published under cs.CV, cs.AI, cs.GR, and cs.HC subject categories.",
        "DOI: 10.48550/arXiv.2312.04547.",
        "Authors list includes 22 researchers from multiple institutions.",
        "Framework components include SocioMind and MoMat-MoGen.",
        "Experiments show state-of-the-art performance in both modules.",
        "Motion matching technique used for ensuring motion quality.",
        "Few-shot exemplars are used to model personality traits.",
        "Text-driven paradigm enables real-time motion control."
      ],
      "topic_areas": [
        "Autonomous Characters",
        "Social Intelligence",
        "3D Motion Synthesis",
        "Language-Driven AI",
        "Virtual Agents",
        "Embodied Cognition",
        "Human-Computer Interaction",
        "AI Personality Modeling",
        "Digital Life Simulation",
        "Text-to-Motion Generation"
      ],
      "word_count": 396,
      "total_markers": 30
    },
    "comments_summary": {},
    "created_at": "2025-11-12T14:21:23.824872",
    "model_used": "qwen-flash"
  }
}
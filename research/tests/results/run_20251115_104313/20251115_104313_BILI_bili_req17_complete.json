{
  "batch_id": "20251115_104313",
  "link_id": "bili_req17",
  "source": "bilibili",
  "metadata": {
    "title": "",
    "author": "",
    "url": "https://www.bilibili.com/video/BV1ThJyzzEPZ/",
    "word_count": 3017,
    "publish_date": ""
  },
  "transcript": "你一定有过这样的经历，当我们在AI聊天机器人中问出一个问题的时候，AI不仅会给出我们答案，还会逻辑清晰的给出他每一步的思考过程。那一刻仿佛和我们对话的根本就不是一个模型，那分明就是一个会思考、能分析，甚至还能揣度我们用意的某种智能的存在。那你是否也遇到过另一种情况，A的推理明明逻辑严谨头头是道，但在最终却信心十足的给出一个和推理完全相反的结论。比如说曾经有研究者问J奈, 1776年是平年还是闰年，大模型的思维过程是这样的，1776年可以被四整除，但它又不是世纪年，所以给出的答案是一个平年，但是可以被四整除，又不是世纪年的年份，其实应该是一个闰年，大模型在思考时正确的使用了闰年的计算方式，但得出的结论却是错误的，为什么会这样呢？一篇来自美国亚利桑那大学名为斯维链是否是大语言模型的幻象的论文，为我们提供了一个全新的视角。今天我们抛弃论文中所有的公式计算，让老王用普通人也可以听得懂的语言解释一下这篇论文到底讲了什么东西。这个论文的主要观点简单来说就是所谓的A思维链，并不是我们所理解的抽象推理的能力，它只是一种高度依赖于训练数据的模式匹配。也就是说AI不是在真正的思考，而是在他的记忆中找到无数看起来像是思考的片段。然后根据我们的问题，把这些片段以一种概率上最合理的方式连接起来而已，从而生成一段看起来逻辑通顺的回答，回到我们最开始的1776年是平年还是闰年的例子。大模型的思考过程是1776年可以被四整除，但又不是世纪年，所以1776年是一个平年，但这个结论其实并不是根据思考而来的，它的实际来源是大模型在训练过程中见过各种平年、闰年计算方法的文字片段。这些片段对应的就是大模型的思考部分，而这些文字片段之后，往往后面紧跟着的例子都是一个计算平年的例子，所以大模型也就跟着输出了1776年是平年的结论。在这个过程中，大模型内部其实根本没有用1776这个数字进行过计算，它得出这个结论只是因为训练的语料中计算平闰年算法后紧跟着平年的例子比较多而已。好了，以上就是研究者的合理猜想，那它怎么证明它是正确的呢，毕竟大模型的内部参数应该怎么解释，至今科学界都还没搞明白，想要进行完整的证明暂时是不可能的了，所以研究者设计了一个非常有趣的实验叫做data，从侧面证明了他们的观点。在这个实验中，研究人员从零开始训练了一个语言模型。这个模型能做的事情非常非常的简单，它只支持两种操作。一种操作是字母加密规则，就是将字符串中的每一个字母在字母表上向后移动13位，比如说abcd就变成了noq。第二种操作是循环位移，也就是将字符串的最后一个字母挪到最前面，比如说abc d就变成了dc然后就能再变成cd b我们训练的目标不仅是让AI给出最终的答案，还要展示变换的过程。比如说Abd先字母加密再循环位移，AI不应该直接输出答案Q N O P，而是应该先输出推理过程，也就是思维链A D经过字母加密得到N O P，Q N O Q再经过循环位移得到Q N O P，所以结果是Q N O P。这个实验的精妙之处在于，在训练模型的时候，所有的训练数据都是研究人员自己生成的，于是在训练之后，人们可以精确的控制A收到的问题是见过的还是没见过的？如果是没见过的问题，那又是怎么没见过呢？从而研究人员可以像控制实验变量一样，来引入各种没见过的情况。下面我们就来看看研究人员都提供了什么样的没见过的数据，以及他们为什么能从侧面反映思维链只示模式匹配。作者在文章中设计了三个实验，任务泛化、长度泛化和格式泛化。我们一个一个讲，在任务泛化的实验中，研究人员只用字母加密训练了模型，直到这个模型能够100%的解决所有的字母加密问题。然后研究人员突然用一个循环位移的问题来观察模型的思维链。如果说模型可以做到知识的泛化，研究人员期待的结果是要么模型可以识别出这些操作都是字母的变换，所以通过思维链推导出循环位移的新规则。要么退一步就直接承认不知道该怎么做，但结果却是模型会固执的想把字母加密的规则套用在循环位移的问题上。之后，研究人员又对模型进行了微调，他们只新增了不到0.02%的关于循环位移的数据，就让模型迅速的学会了处理这个问题。这个实验证明模型在处理字母加密时，并没有理解字母加密背后隐藏着的位移13位的算法，所以在遇到没有见过的循环位移的问题时，不能在推断出新的算法，也不能判断已有的算法无法处理新的问题，而少量的微调则弥补上了这个没有见过的模式。不仅如此，研究人员还尝试在测试中加入训练时没有见过的组合，比如说训练的数据永远是先字母加你再循环位移，测试的时候却要求AI先循环位移，然后再字幕加密，这次的结果更加有趣。模型输出的推理过程和问题是无关的，但是结果却又是正确的，你能猜到这是为什么吗？想到的同学请留言告诉我。这些证据都间接的说明了模型并没有理解训练数据背后所隐藏的算法。在这个实验中，研究人员通过给模型提出没有见过的问题泛化了任务。而在第二个实验中，研究人员则尝试泛化长度。这一次我们只用两步推理来训练模型。比如说连续两次字母加密，连续两次循环位移，或者字母加密加长循环位移，总之训练数据都是两步的。然后在推理的时候，我们让AI只进行一步推理，比如说只字母加密，或者进行三步推理。比如说连续三次进行加密，结果输出的思维链出现了明显的问题，面对一步的问题时，AI常常强行编造出第二步，而面对一个三步的问题时，AI往往推理了两步就停止了。这表明模型的思考过程并不是按照问题的实际需求所生成的，更像是在填充一个固定长度的模板。第三个实验是格式泛化，这是最能体现AI只是在做模式匹配的一个实验了。实验人员在训练AI的时候，让他只看到特定的格式的指令，比如说problem冒号Abd中括号加密，但是在测试的时候，研究人员仅仅是把problem替换成了question，或者把中括号替换成了小括号，就导致了模型性能的显著下降。而真正的逻辑推理应该是抽象于符号与语法的，但是模型却对这些表面形式上的改动如此的敏感，恰恰证明了它所依赖的并非深层次的逻辑，而仅仅是对文本表面模式的复现。也许你会质疑，这可能只是因为研究人员训练的模型太小了，如果说换成gp 5这种巨无霸，结果会不会不同呢？原文的作者也讨论了这个问题，他们用不同大小的模型重复的进行了实验，还调整了其他的参数。结论是这种依赖训练数据难以泛化的问题依然是存在的，这说明问题不出在模型不够大的上面，而出在他们学习的方式上面。最后作者说这些实验并非为了贬低A而是让我们以一种更成熟的方式去使用它。首先，我们要保持健康的怀疑，永远不要把AI输出的内容当做绝对的真理。因为AI是非常擅长用不容置疑的语气来包装一个完全错误的结论的。然后我们最好要主动测试AI的边界，我们不妨可以设计一些超出常规的问题，这样可以更好的让我们把握AI的边界。最后一点也是最最重要的，我们自己才是真正的思考者。AI只是我们思考的辅助工具，而不是代替者。我们之所以对会思考的AI如此的着迷，或许是源于我们对创造同类的渴望。我们实在是太想看到一个会思考的机器可以陪我们喜怒哀乐的机器了，以至于我们不自觉的把流畅的表达等同于深刻的思考。这篇论文与其说是揭露了A的缺陷，不如说是修正了我们的认识。也许通往人工智能的路上，重要的并非是让A学会像人一样思考，而是我们人类学会如何善用一个和我们思维方式完全不同的异类。这里是同学老王，我们下期再见。",
  "comments": null,
  "summary": {
    "transcript_summary": {
      "key_facts": [
        "AI在思维链中展示逻辑过程但可能得出错误结论",
        "1776年可被4整除且非世纪年，应为闰年而非平年",
        "大模型的思维链是训练数据中片段的模式匹配结果",
        "研究者通过自定义生成数据训练语言模型进行实验",
        "实验中模型在未见过的任务上无法泛化推理能力",
        "模型在任务泛化实验中将字母加密规则错误套用于循环位移",
        "微调仅0.02%新数据即让模型学会处理新任务",
        "模型在组合操作顺序变化时输出无关推理但结果正确",
        "模型在长度泛化实验中强行填充固定长度的推理模板",
        "格式泛化实验中指令符号变化导致性能显著下降",
        "模型对表面文本格式敏感，不依赖深层逻辑抽象",
        "不同规模模型均表现出相同泛化缺陷，问题源于学习方式"
      ],
      "key_opinions": [
        "AI的思维链并非真正推理，而是概率化的模式拼接",
        "模型缺乏对算法本质的理解，仅复现训练中的表面模式",
        "人类容易将流畅表达误认为深刻思考，存在认知错觉",
        "我们对会思考的AI的迷恋源于创造同类的渴望",
        "AI不应被视为思考替代者，而应作为辅助工具使用",
        "保持怀疑是应对AI输出不可靠性的必要态度",
        "主动测试AI边界有助于更准确把握其能力范围",
        "真正的思考者始终是人类，AI只是思维的延伸工具",
        "论文目的不是贬低AI，而是修正人类对AI的认知偏差",
        "通往AI的真正路径在于学会与异类思维方式共处"
      ],
      "key_datapoints": [
        "微调仅需不到0.02%的新数据即可使模型掌握新任务",
        "任务泛化实验中模型在未见任务上仍尝试套用旧规则",
        "长度泛化实验中模型对一步或三步推理均出现模板偏差",
        "格式泛化实验中符号替换（如中括号→小括号）导致性能下降",
        "所有实验均在不同大小模型上重复验证，结果一致",
        "训练数据完全由研究人员自动生成，可精确控制变量",
        "模型在两步推理训练后无法处理一步或三步任务",
        "模型在组合顺序变化时仍能输出正确答案但推理无关",
        "实验设计包含任务、长度、格式三种泛化类型",
        "思维链输出与实际问题需求不匹配，呈现固定模板特征"
      ],
      "topic_areas": [
        "思维链本质",
        "模式匹配机制",
        "任务泛化能力",
        "长度泛化限制",
        "格式敏感性",
        "模型理解局限",
        "训练数据依赖",
        "推理真实性",
        "人机认知偏差",
        "AI使用伦理"
      ],
      "word_count": 20,
      "total_markers": 32
    },
    "comments_summary": {},
    "created_at": "2025-11-15T18:49:25.476042",
    "model_used": "qwen-flash"
  },
  "completed_at": 1763203772.0387435
}